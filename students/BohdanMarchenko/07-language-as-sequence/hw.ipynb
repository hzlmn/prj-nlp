{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: leipzig-corpus/eng-eu_web_2015_10K-sentences.txt\n",
      "Extracted 3951 sentences\n",
      "[[['01.08.2011', False], ['Public', False], ['call', False], ['for', False], ['standard', False], ['projects', False], ['No.', False], ['02/2009', False], ['.', False]], [['â€¢', False], ['05.00', False], ['Energy', False], ['Rating', False], ['Introduce', False], ['energy', False], ['rating', False], ['models', False], ['for', False], ['selected', False], ['technologies', False], ['based', False], ['on', False], ['high', False], ['resolution', False], ['databases', False], ['in', False], ['the', False], ['Photovoltaic', False], ['Geographic', False], ['Information', False], ['System', False], ['(PVGIS),', False], ['also', False], ['in', False], ['collaboration', False], ['with', False], ['ENEA', True], ['06.00', False], ['Estimating', False], ['annual', False], ['farming', False], ['GHG', False], ['emissions', False], ['(including', False], ['Nitrous', False], ['Oxide', False], ['(N2O)', False], ['from', False], ['soils)', False], ['from', False], ['biofuels', False], ['crops', False], ['in', False], ['EU-NUTS2', False], ['regions', False], ['.', False]]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "CORPUS_PATH = \"leipzig-corpus\"\n",
    "TRAIN_FILE = \"run-on-train.json\"\n",
    "TEST_FILE = \"run-on-test.json\"\n",
    "\n",
    "def iter_islast(iterable):\n",
    "    it = iter(iterable)\n",
    "    prev = it.__next__()\n",
    "    for item in it:\n",
    "        yield prev, False\n",
    "        prev = item\n",
    "    yield (prev[:-1], prev[-1]), True\n",
    "\n",
    "def extract_tokens_file(filename):\n",
    "    tokens = []\n",
    "    if filename.endswith(\"eng-eu_web_2015_10K-sentences.txt\"):\n",
    "        print(\"Processing: {}\".format(filename))\n",
    "        with open(filename, \"r\") as f:\n",
    "\n",
    "            text = f.read().split(\"\\n\")\n",
    "            res = []\n",
    "            run_on_mode = False\n",
    "            for line in text:\n",
    "                if line:\n",
    "                    line = line.split(\"\\t\")[1]\n",
    "                    for token, is_last in iter_islast(line.split(\" \")):\n",
    "                        if is_last:\n",
    "                            token1, token2 = token\n",
    "                            #around 70% that it is run on\n",
    "                            is_run_on = random.randint(1, 10) < 7\n",
    "                            if is_run_on:\n",
    "                                res.append([token1, True])\n",
    "                                run_on_mode = True\n",
    "                            else:\n",
    "                                #this is the end of sentence\n",
    "                                res.append([token1, False])\n",
    "                                res.append([token2, False])\n",
    "                                tokens.append(res)\n",
    "                                res = []\n",
    "                        else:\n",
    "                            if run_on_mode:\n",
    "                                if random.randint(0, 1): #this is 50% that run on is lower\n",
    "                                    res.append([token.lower(), False])\n",
    "                                    continue\n",
    "                                run_on_mode = False\n",
    "                            res.append([token, False])\n",
    "    return tokens\n",
    "\n",
    "with open(TRAIN_FILE, \"w\") as f:\n",
    "    tokens = [extract_tokens_file(\"{}/{}\".format(CORPUS_PATH, fl)) for fl in os.listdir(CORPUS_PATH)]\n",
    "    tokens = [item for sublist in tokens for item in sublist]\n",
    "    print(\"Extracted {} sentences\".format(len(tokens)))\n",
    "    print(tokens[:2])\n",
    "    res = json.dumps(tokens, indent=4)\n",
    "    f.write(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    label = sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        'word.lower()': word.lower(),\n",
    "#         'word[-3:]': word[-3:],\n",
    "#         'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(f):\n",
    "    features, labels = [], []\n",
    "    with open(f, \"r\") as fl:\n",
    "#         import ipdb; ipdb.set_trace()\n",
    "        for sent in  json.loads(fl.read())[:3000]:\n",
    "            for i in range(len(sent)):\n",
    "                f, label = word2features(sent, i) \n",
    "                features.append(f)\n",
    "                labels.append(label)\n",
    "    return features, labels\n",
    "train_features, train_labels = get_tokens(TRAIN_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word.lower()': '01.08.2011',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isdigit()': False,\n",
       "  'BOS': True,\n",
       "  '+1:word.lower()': 'public',\n",
       "  '+1:word.istitle()': True,\n",
       "  '+1:word.isupper()': False},\n",
       " {'word.lower()': 'public',\n",
       "  'word.isupper()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isdigit()': False,\n",
       "  '-1:word.lower()': '01.08.2011',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'call',\n",
       "  '+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train labels Counter({False: 153728, True: 4644})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# print(\"test labels {}\".format(Counter(test_labels)))\n",
    "print(\"train labels {}\".format(Counter(train_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_features, train_labels, test_size=0.33)\n",
    "vec = DictVectorizer()\n",
    "x_train = vec.fit_transform(X_train).toarray()\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "ls = LogisticRegression()\n",
    "ls.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test = vec.transform(X_test).toarray()\n",
    "# y_pred = ls.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "# print(classification_report(y_test, y_pred))\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.98      1.00      0.99      4542\n",
      "       True       0.78      0.25      0.38       155\n",
      "\n",
      "avg / total       0.97      0.97      0.97      4697\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "validation_features, validation_labels = get_tokens(TEST_FILE)\n",
    "x_validation = vec.transform(validation_features).toarray()\n",
    "validation_pred = ls.predict(x_validation)\n",
    "print(classification_report(validation_labels, validation_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def top_features(vectorizer, clf, n):\n",
    "#     \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "#     feature_names = vectorizer.get_feature_names()\n",
    "#     for i, class_label in enumerate(clf.classes_):\n",
    "#         top = np.argsort(clf.coef_[i])\n",
    "#         reversed_top = top[::-1]\n",
    "#         print(\"%s: %s\" % (class_label,\n",
    "#               \" \".join(feature_names[j] for j in reversed_top[:n])))\n",
    "# top_features(vec, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
