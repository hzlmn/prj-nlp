{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from bz2 import BZ2File\n",
    "from collections import OrderedDict\n",
    "\n",
    "from conllu import parse, parse_tree\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fiction.cased.lemmatized.300d.bz2\r\n",
      "fiction.cased.lemmatized.word2vec.300d.bz2\r\n"
     ]
    }
   ],
   "source": [
    "!ls *.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_vecs(bz2_path):\n",
    "    map2id = {}\n",
    "    map2word = {}\n",
    "    weights = None\n",
    "    i = 0\n",
    "    with BZ2File(bz2_path) as archive:\n",
    "        line = archive.readline()\n",
    "        while line:\n",
    "            line = line.decode('utf-8')\n",
    "            if i == 0:\n",
    "                sizes = [int(s.strip()) for s in line.split(' ')]\n",
    "                print('input sizes: ', sizes)\n",
    "                weights = np.zeros(sizes)\n",
    "            else:\n",
    "                chunks = line.split(' ', maxsplit=1)\n",
    "                word, num_string = chunks[0], chunks[1]\n",
    "                nums = np.fromstring(num_string, sep=' ')\n",
    "                weights[i - 1, :] = nums\n",
    "                map2id[word] = i - 1\n",
    "                map2word[i - 1] = word\n",
    "            line = archive.readline()\n",
    "            i += 1\n",
    "            \n",
    "    print(f'read {i} total lines')\n",
    "    return weights, map2id, map2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conllu_file(filename):\n",
    "    with open(filename) as input_file:\n",
    "        text = input_file.read()\n",
    "        result = parse(text)\n",
    "    return result\n",
    "    \n",
    "    \n",
    "train = read_conllu_file('../../../../UD_Ukrainian-IU/uk_iu-ud-train.conllu')\n",
    "test = read_conllu_file('../../../../UD_Ukrainian-IU/uk_iu-ud-dev.conllu')\n",
    "val = read_conllu_file('../../../../UD_Ukrainian-IU/uk_iu-ud-test.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "deprels = set()\n",
    "for sent in train:\n",
    "    for tok in sent:\n",
    "        deprels.update([tok['deprel']])\n",
    "deprels = list(deprels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_SIZE = len(deprels)\n",
    "labeler = LabelEncoder()\n",
    "labeler.fit(deprels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sizes:  [59181, 300]\n",
      "read 59182 total lines\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix, map2id, map2word = load_vecs(\"fiction.cased.lemmatized.300d.bz2\")\n",
    "# embedding_matrix, map2id, map2word = load_vecs(\"fiction.cased.lemmatized.word2vec.300d.bz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = OrderedDict([('id', 0), ('form', 'ROOT'), ('lemma', 'ROOT'), ('upostag', 'ROOT'),\n",
    "                    ('xpostag', None), ('feats', None), ('head', None), ('deprel', None),\n",
    "                    ('deps', None), ('misc', None)])\n",
    "\n",
    "def unwrap_gold_relations(tree):\n",
    "    return [(tok['id'], head(tok)) for tok in tree]\n",
    "\n",
    "def head(tok):\n",
    "    return tok['head'] if 'head' in tok else 0\n",
    "\n",
    "def vectorize(sentences, map2id, labeler):\n",
    "    output = pd.DataFrame()\n",
    "    for sentence in sentences:\n",
    "        rels = unwrap_gold_relations(sentence)\n",
    "        toks = [ROOT] + sentence\n",
    "        for (child, head) in rels:\n",
    "            output = output.append({'head': map2id[toks[head]['lemma']] if toks[head]['lemma'] in map2id else 0,\n",
    "                                    'child': map2id[toks[child]['lemma']] if toks[child]['lemma'] in map2id else 0,\n",
    "                                    'deprel': toks[child]['deprel']}, ignore_index=True)\n",
    "    \n",
    "    return output[['child', 'head']].astype(int), labeler.transform(output['deprel'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationsData(Dataset):\n",
    "    def __init__(self, Xs, ys):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.Xs = Xs\n",
    "        self.ys = ys\n",
    "        self.size = len(Xs)\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.Xs[index:index+1].values[0]\n",
    "        target = self.ys[index]\n",
    "        return x, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationModel(nn.Module):\n",
    "    def __init__(self, embedding_matrix, output_size):\n",
    "        super(RelationModel, self).__init__()\n",
    "        \n",
    "        vocab_size = embedding_matrix.shape[0]\n",
    "        vector_size = embedding_matrix.shape[1]\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, vector_size)\n",
    "        self.embeddings.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        self.dense1 = nn.Linear(vector_size * 2, 1000)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(1000, output_size)\n",
    "        self.activation2 = nn.Sigmoid()\n",
    "    def forward(self, in1, in2):\n",
    "        emb1 = self.embeddings(in1)\n",
    "        emb2 = self.embeddings(in2)\n",
    "        in_cat = torch.cat([emb1, emb2], 1)\n",
    "        out = self.dense1(in_cat)\n",
    "        out = self.activation1(out)\n",
    "        out = self.dense2(out)\n",
    "        y_pred = F.softmax(out, dim=1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished in 307.47s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "X_train, y_train = vectorize(train, map2id, labeler)\n",
    "print(f'finished in {time.time() - start:.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished in 28.34s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "X_test, y_test = vectorize(test, map2id, labeler)\n",
    "print(f'finished in {time.time() - start:.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "num_workers = 8\n",
    "lr = 0.000001\n",
    "num_epochs = 20\n",
    "print_freq = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = RelationsData(X_train, y_train)\n",
    "loader_train = DataLoader(ds_train, batch_size=batch_size, num_workers=num_workers, drop_last=True)\n",
    "\n",
    "ds_test = RelationsData(X_test, y_test)\n",
    "loader_test = DataLoader(ds_test, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(embedding_matrix, output_size):\n",
    "    return RelationModel(embedding_matrix, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(embedding_matrix, output_size=OUTPUT_SIZE)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 51/146 [00:13<00:23,  4.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 3.117732286453247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 101/146 [00:26<00:11,  4.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 3.1844699382781982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:37<00:00,  4.10it/s]\n",
      " 35%|███▍      | 51/146 [00:13<00:23,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 3.1176395416259766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 101/146 [00:25<00:11,  4.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 3.1843268871307373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:36<00:00,  4.14it/s]\n",
      " 35%|███▍      | 51/146 [00:12<00:23,  4.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 3.1175529956817627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 101/146 [00:25<00:11,  4.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 3.184185266494751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:36<00:00,  4.12it/s]\n",
      " 35%|███▍      | 51/146 [00:13<00:23,  4.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 3.1174721717834473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 101/146 [00:25<00:11,  4.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 3.1840474605560303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:36<00:00,  4.09it/s]\n",
      " 35%|███▍      | 51/146 [00:13<00:23,  4.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 3.1173946857452393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 101/146 [00:25<00:11,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 3.1839089393615723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:36<00:00,  4.13it/s]\n",
      " 35%|███▍      | 51/146 [00:13<00:23,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 3.117323160171509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 101/146 [00:25<00:11,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 3.18377423286438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:36<00:00,  4.11it/s]\n",
      " 35%|███▍      | 51/146 [00:13<00:23,  4.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 3.117255449295044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 101/146 [00:25<00:11,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 3.183642625808716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:36<00:00,  4.17it/s]\n",
      " 35%|███▍      | 51/146 [00:13<00:23,  4.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 3.117192268371582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 101/146 [00:25<00:11,  4.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 3.183511972427368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:37<00:00,  3.84it/s]\n",
      " 35%|███▍      | 51/146 [00:14<00:25,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 3.117131233215332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 101/146 [00:27<00:12,  3.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 3.183382987976074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:40<00:00,  3.44it/s]\n",
      " 35%|███▍      | 51/146 [00:13<00:23,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 3.117074728012085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 101/146 [00:26<00:10,  4.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 3.183255910873413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:38<00:00,  4.04it/s]\n",
      " 35%|███▍      | 51/146 [00:13<00:26,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 3.117020606994629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 101/146 [00:26<00:13,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 3.183130979537964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:38<00:00,  4.03it/s]\n",
      " 35%|███▍      | 51/146 [00:15<00:29,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 3.1169705390930176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 101/146 [00:27<00:10,  4.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 3.1830079555511475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:38<00:00,  4.13it/s]\n",
      " 35%|███▍      | 51/146 [00:12<00:23,  4.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 3.1169216632843018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 101/146 [00:25<00:11,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 3.182884454727173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:37<00:00,  3.90it/s]\n",
      " 35%|███▍      | 51/146 [00:14<00:24,  3.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 3.1168763637542725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 101/146 [00:26<00:10,  4.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 3.182762622833252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:38<00:00,  3.80it/s]\n",
      " 35%|███▍      | 51/146 [00:14<00:25,  3.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 3.1168341636657715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 101/146 [00:27<00:11,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 3.182644844055176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:39<00:00,  4.17it/s]\n",
      " 35%|███▍      | 51/146 [00:13<00:25,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 3.1167924404144287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 101/146 [00:27<00:12,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 3.1825308799743652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:39<00:00,  4.03it/s]\n",
      " 35%|███▍      | 51/146 [00:14<00:24,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 3.116751194000244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 101/146 [00:26<00:11,  4.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 3.182420492172241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:38<00:00,  3.76it/s]\n",
      " 35%|███▍      | 51/146 [00:12<00:22,  4.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 3.116711378097534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 101/146 [00:25<00:10,  4.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 3.182313919067383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:35<00:00,  4.19it/s]\n",
      " 35%|███▍      | 51/146 [00:13<00:25,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 3.116673469543457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 101/146 [00:27<00:12,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 3.182209014892578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:38<00:00,  4.16it/s]\n",
      " 35%|███▍      | 51/146 [00:13<00:23,  4.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 3.1166341304779053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 101/146 [00:26<00:11,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 3.182103157043457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:38<00:00,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last loss:  3.1147189140319824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print_freq = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for step, (x, y) in enumerate(tqdm(loader_train)):\n",
    "        x1 = x[:, 0].type(torch.LongTensor)\n",
    "        x2 = x[:, 1].type(torch.LongTensor)\n",
    "\n",
    "        y_pred = model(x1, x2)\n",
    "        loss = criterion(y_pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (step % print_freq) == 0 and step > 0:\n",
    "            print(step, loss.item())\n",
    "print('last loss: ', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ran this multiple time, reducing learning rate by 10 each time\n",
    "# Possible additional feaures / steps\n",
    "# grandchild / sibling count / how deep from head\n",
    "# more layers / dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on(loader, model, criterion):\n",
    "    res = []\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for (x, target) in tqdm(loader):\n",
    "            # compute output\n",
    "            x1 = x[:, 0].type(torch.LongTensor)\n",
    "            x2 = x[:, 1].type(torch.LongTensor)\n",
    "\n",
    "            output = model(x1, x2).data.cpu().numpy()\n",
    "            classes = np.argmax(output, axis=1)\n",
    "            res = np.append(res, classes)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:00<00:00, 11.19it/s]\n"
     ]
    }
   ],
   "source": [
    "result = predict_on(loader_test, model, criterion)\n",
    "result = [int(x) for x in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "                acl       0.00      0.00      0.00       184\n",
      "              advcl       0.00      0.00      0.00       109\n",
      "           advcl:sp       0.00      0.00      0.00         4\n",
      "          advcl:svc       0.00      0.00      0.00         6\n",
      "             advmod       0.87      0.90      0.89       484\n",
      "               amod       0.81      0.76      0.78       846\n",
      "              appos       0.00      0.00      0.00        71\n",
      "                aux       0.00      0.00      0.00        19\n",
      "               case       0.98      1.00      0.99       945\n",
      "                 cc       0.91      0.96      0.94       358\n",
      "              ccomp       0.00      0.00      0.00        50\n",
      "           compound       0.00      0.00      0.00        64\n",
      "       compound:svc       0.20      0.43      0.27       475\n",
      "               conj       0.00      0.00      0.00        57\n",
      "           conj:svc       0.00      0.00      0.00        52\n",
      "                cop       0.90      0.97      0.94       226\n",
      "              csubj       0.00      0.00      0.00        12\n",
      "                det       0.00      0.00      0.00         2\n",
      "         det:numgov       0.68      0.65      0.67       139\n",
      "         det:nummod       0.00      0.00      0.00         5\n",
      "          discourse       0.00      0.00      0.00         7\n",
      "         dislocated       0.00      0.00      0.00        26\n",
      "               expl       0.00      0.00      0.00         3\n",
      "              fixed       0.00      0.00      0.00        18\n",
      "               flat       0.00      0.00      0.00        52\n",
      "       flat:foreign       0.00      0.00      0.00         1\n",
      "          flat:name       0.00      0.00      0.00       149\n",
      "        flat:repeat       0.00      0.00      0.00        10\n",
      "         flat:title       0.78      0.84      0.81       205\n",
      "           goeswith       0.65      0.64      0.64      1044\n",
      "               iobj       0.50      0.44      0.47       634\n",
      "               list       0.00      0.00      0.00        24\n",
      "               mark       0.00      0.00      0.00        43\n",
      "               nmod       0.00      0.00      0.00        44\n",
      "              nsubj       0.30      0.35      0.33       488\n",
      "         nsubj:pass       0.57      0.47      0.52       709\n",
      "             nummod       0.00      0.00      0.00         2\n",
      "         nummod:gov       0.00      0.00      0.00       100\n",
      "                obj       0.00      0.00      0.00        14\n",
      "                obl       0.67      0.93      0.78      1987\n",
      "             orphan       0.77      0.81      0.79       577\n",
      "          parataxis       0.00      0.00      0.00        91\n",
      "parataxis:discourse       0.00      0.00      0.00        35\n",
      "\n",
      "        avg / total       0.60      0.66      0.62     10371\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1428: UserWarning: labels size, 43, does not match size of target_names, 49\n",
      "  .format(len(labels), len(target_names))\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, result, target_names=labeler.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welp, got to some level (which partially could be explained by the fact that not all labels were present in training data) – so be it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.657795776684987\n"
     ]
    }
   ],
   "source": [
    "# Given we are using gold parsing trees, we can just do TP ratio on words\n",
    "# Other possible thing would be doing LAS by sentences and averaging\n",
    "def LAS_words(y_gold, y_pred):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for pair in zip(y_gold, y_pred):\n",
    "        total += 1\n",
    "        if pair[0] == pair[1]:\n",
    "            correct += 1\n",
    "    return correct / total\n",
    "print(LAS_words(y_test, result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
