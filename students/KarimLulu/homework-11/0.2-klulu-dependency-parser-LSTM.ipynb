{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, Input, Dense, Flatten, Conv1D, concatenate, Activation, LSTM, Dropout, Reshape, Lambda\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import regularizers, Model, Sequential\n",
    "from keras.models import load_model\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from conllu import parse, parse_tree\n",
    "from pathlib import Path\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import bz2\n",
    "import json\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport parsing\n",
    "from parsing import Parser, Parse\n",
    "\n",
    "%aimport helpers\n",
    "from helpers import read_embeddings, ROOT, clean_deprel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_filename = \"ubercorpus.lowercased.tokenized.300d.bz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path.home() / \"repos/UD_Ukrainian-IU\"\n",
    "\n",
    "with list(data_dir.glob(\"*train*\"))[0].open() as f:\n",
    "    data = f.read()\n",
    "trees = parse(data)\n",
    "\n",
    "with list(data_dir.glob(\"*test*\"))[0].open() as f:\n",
    "    test_data = f.read()\n",
    "test_trees = parse(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(trees, form=\"form\"):\n",
    "    word_index = {}\n",
    "    pos_index = {}\n",
    "    dep_index = {}\n",
    "    for tree in trees:\n",
    "        for word in tree:\n",
    "            deprel = word[\"deprel\"]\n",
    "            word_id = len(word_index)+1\n",
    "            pos_id = len(pos_index)+1\n",
    "            dep_id = len(dep_index)+1\n",
    "            word_t = word[form].lower()\n",
    "            word_pos = word[\"upostag\"]\n",
    "            word_index[word_t] = word_index.get(word_t, word_id)\n",
    "            pos_index[word_pos] = pos_index.get(word_pos, pos_id)\n",
    "            dep_index[deprel] = dep_index.get(deprel, dep_id)\n",
    "\n",
    "    word_index[ROOT[\"form\"]] = len(word_index)+1\n",
    "    pos_index[ROOT[\"upostag\"]] = len(pos_index)+1\n",
    "    return word_index, pos_index, dep_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(trees, parser):\n",
    "    o_labels = []\n",
    "    o_features = []\n",
    "    for tree in trees:\n",
    "        labels, features, _ = parser.parse(tree)\n",
    "        o_labels.extend(labels)\n",
    "        o_features.extend(features)\n",
    "    return o_labels, o_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index, pos_index, dep_index = build_vocabulary(trees+test_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_2_vec, ndim, _ = read_embeddings(filename=vec_filename, word_index=word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_VEC = np.zeros(ndim, np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index)+1, ndim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_matrix[i] = word_2_vec.get(word, DEFAULT_VEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_stack_context(depth, stack, data):\n",
    "        if depth >= 3:\n",
    "            return data[stack[-1][\"id\"]], data[stack[-2][\"id\"]], data[stack[-3][\"id\"]]\n",
    "        elif depth >= 2:\n",
    "            return data[stack[-1][\"id\"]], data[stack[-2][\"id\"]], 0\n",
    "        elif depth == 1:\n",
    "            return data[stack[-1][\"id\"]], 0, 0\n",
    "        else:\n",
    "            return 0, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_buffer_context(k, buffer, data):\n",
    "        if k >= 3:\n",
    "            return data[buffer[0][\"id\"]], data[buffer[1][\"id\"]], data[buffer[2][\"id\"]]\n",
    "        elif k >= 2:\n",
    "            return data[buffer[0][\"id\"]], data[buffer[1][\"id\"]], 0\n",
    "        elif k == 1:\n",
    "            return data[buffer[0][\"id\"]], 0, 0\n",
    "        else:\n",
    "            return 0, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parse_context(word, deps, data, left=True):\n",
    "    if not word or word == -1:\n",
    "        return 0, (0, 0), (0, 0)\n",
    "    deps = deps[word[\"id\"]]\n",
    "    num = len(deps)\n",
    "    if not num:\n",
    "        return num, (0, 0), (0, 0)\n",
    "    elif num==1:\n",
    "        return num, (data[deps[0][0]], deps[0][1]), (0, 0)\n",
    "    else:\n",
    "        temp = sorted(deps, key=lambda x: x[0], reverse=left)\n",
    "        return num, (data[deps[0][0]], deps[0][1]), (data[deps[1][0]], deps[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_builder(stack, queue, tree, parse=None, word_index=word_index, \n",
    "                    pos_index=pos_index, dep_index=dep_index, form=\"form\",\n",
    "                    log=0):\n",
    "    words = []\n",
    "    tags = []\n",
    "    deps = []\n",
    "    depth = len(stack)\n",
    "    q_len = len(queue)\n",
    "    if ROOT not in tree:\n",
    "        tree = [ROOT, *tree]\n",
    "    \n",
    "    s0, s1, s2 = get_stack_context(depth, stack, tree)\n",
    "    q0, q1, q2 = get_buffer_context(q_len, queue, tree)\n",
    "    \n",
    "    # Left two child of the top stack\n",
    "    Ns0l, s0l1, s0l2 = get_parse_context(s0, parse.lefts, tree, left=True)  \n",
    "    # Right two child of the top stack\n",
    "    Ns0r, s0r1, s0r2 = get_parse_context(s0, parse.rights, tree, left=False)\n",
    "    # Left two child of the second element on stack\n",
    "    Ns1l, s1l1, s1l2 = get_parse_context(s1, parse.lefts, tree, left=True)\n",
    "    # Left two child of the second element on stack\n",
    "    Ns1r, s1r1, s1r2 = get_parse_context(s1, parse.rights, tree, left=False)\n",
    "    \n",
    "    _, s0l1l1, _ = get_parse_context(s0l1[0], parse.lefts, tree, left=True)\n",
    "    _, s0r1r1, _ = get_parse_context(s0r1[0], parse.rights, tree, left=False)\n",
    "    _, s1l1l1, _ = get_parse_context(s1l1[0], parse.lefts, tree, left=True)\n",
    "    _, s1r1r1, _ = get_parse_context(s1r1[0], parse.rights, tree, left=False)\n",
    "    \n",
    "    if log:\n",
    "        if s0:\n",
    "            print(s0[\"form\"], \"id =\", s0[\"id\"], parse.lefts[s0[\"id\"]],  parse.rights[s0[\"id\"]])\n",
    "            print()\n",
    "            print(\"Top stack (rights): \", s0r1, s0r2)\n",
    "            print()\n",
    "            print(\"Right for right: \", s0r1r1)\n",
    "            print()\n",
    "            print(\"Top stack (lefts): \", s0l1, s0l2)\n",
    "            print()\n",
    "            print(\"Left for left: \", s0l1l1)\n",
    "            print()\n",
    "            print()\n",
    "        print(\"=\"*20)\n",
    "        if s1:\n",
    "            print(s1[\"form\"], \"id =\", s1[\"id\"], parse.lefts[s1[\"id\"]],  parse.rights[s1[\"id\"]])\n",
    "            print()\n",
    "            print(\"Second stack (rights): \", s1r1, s1r2)\n",
    "            print()\n",
    "            print(\"Right for right: \", s1r1r1)\n",
    "            print()\n",
    "            print(\"Second stack (lefts): \", s1l1, s1l2)\n",
    "            print()\n",
    "            print(\"Left for left: \", s1l1l1)\n",
    "            print()\n",
    "            print()\n",
    "        print(\"=\"*20)\n",
    "    deps = [dep_index.get(s0l1[-1], 0), dep_index.get(s0l2[-1], 0),\n",
    "            dep_index.get(s0r1[-1], 0), dep_index.get(s0r2[-1], 0),\n",
    "            dep_index.get(s1l1[-1], 0), dep_index.get(s1l2[-1], 0),\n",
    "            dep_index.get(s1r1[-1], 0), dep_index.get(s1r2[-1], 0),\n",
    "            dep_index.get(s0l1l1[-1], 0), dep_index.get(s0r1r1[-1], 0),\n",
    "            dep_index.get(s1l1l1[-1], 0), dep_index.get(s1r1r1[-1], 0)]\n",
    "    \n",
    "    for x in [s0, s1, s2, q0, q1, q2, \n",
    "              s0l1[0], s0l2[0], s0r1[0], s0r2[0], \n",
    "              s1l1[0], s1l2[0], s1r1[0], s1r2[0],\n",
    "              s0l1l1[0], s0r1r1[0], s1l1l1[0], s1r1r1[0]]:\n",
    "        if x:\n",
    "            word = x[form].lower() if x[\"id\"] else \"ROOT\"\n",
    "            word_idx = word_index.get(word)\n",
    "            pos_idx = pos_index.get(x[\"upostag\"])\n",
    "            words.append(word_idx)\n",
    "            tags.append(pos_idx)\n",
    "        else:\n",
    "            words.append(x)\n",
    "            tags.append(x)\n",
    "    \n",
    "    dist = s0[\"id\"] - q0[\"id\"] if q0 and s0 else 0\n",
    "    nums = [dist, Ns0l, Ns0r, Ns1r, Ns1l]\n",
    "    features = [*words, *tags, *deps, *nums]\n",
    "    return features, len(words), len(tags), len(deps), len(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n=N):\n",
    "    for i in range(0, len(l)-n+1):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(trees, parser, feature_extractor):\n",
    "    o_labels = []\n",
    "    o_features = []\n",
    "    k = 0\n",
    "    for tree in trees:\n",
    "        labels, features, _, n_w, n_t, n_d, n_num = parser.parse(tree, feature_extractor=feature_extractor)\n",
    "        feature_chunks = list(chunks(features))\n",
    "        label_chunks = list(chunks(labels))\n",
    "        o_labels.extend(label_chunks)\n",
    "        o_features.extend(feature_chunks)\n",
    "        if tree in test_trees:\n",
    "           k += len(feature_chunks)\n",
    "    return o_labels, o_features, n_w, n_t, n_d, n_num, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = Parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, features, n_w, n_t, n_d, n_num, k = get_data(trees+test_trees, parser, feature_builder)\n",
    "n_unq = len(np.unique([el for x in labels for el in x]))\n",
    "label_index = parser.label_index.copy()\n",
    "#X = np.asarray(features)\n",
    "X = pad_sequences(sequences=features, maxlen=N, padding=\"post\")\n",
    "#y = to_categorical(labels)\n",
    "y = to_categorical(pad_sequences(labels, padding=\"post\", value=n_unq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = X.shape[0]-k#\n",
    "#n_train = 154709"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X[:n_train], X[n_train:]\n",
    "y_train, y_test = y[:n_train], y[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_layer = Embedding(len(word_index)+1,\n",
    "                            ndim,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_shape=(N,n_w),\n",
    "                            trainable=0,\n",
    "                            #mask_zero=True\n",
    "                            #input_length=n_w\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embedding_layer = Embedding(len(pos_index)+1,\n",
    "                                100,\n",
    "                                input_shape=(N, n_t),\n",
    "                                trainable=1,\n",
    "                                #mask_zero=True,\n",
    "                                #input_length=n_t\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_embedding_layer = Embedding(len(dep_index)+1,\n",
    "                                100,\n",
    "                                input_shape=(N, n_d),\n",
    "                                trainable=1,\n",
    "                                #mask_zero=True\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sequence_input = Input(shape=(N, n_w, ), dtype='int32')\n",
    "word_embedded_sequences = word_embedding_layer(word_sequence_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sequence_input = Input(shape=(N, n_t, ), dtype='int32')\n",
    "pos_embedded_sequences = pos_embedding_layer(pos_sequence_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_sequence_input = Input(shape=(N, n_d, ), dtype='int32')\n",
    "dep_embedded_sequences = dep_embedding_layer(dep_sequence_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = Input(shape=(N, n_num,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = Reshape((N, -1))(word_embedded_sequences)\n",
    "pos = Reshape((N, -1))(pos_embedded_sequences)\n",
    "dep = Reshape((N, -1))(dep_embedded_sequences)\n",
    "x = concatenate(inputs=[word, pos, dep, features], axis=-1)\n",
    "x = LSTM(256, return_sequences=True)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "preds = Dense(n_unq, activation='softmax',\n",
    "             kernel_regularizer=regularizers.l2(1e-8))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[word_sequence_input, pos_sequence_input, dep_sequence_input, features], outputs=preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adagrad',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 5, 18)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 5, 18)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            (None, 5, 12)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 5, 18, 300)   7751400     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 5, 18, 100)   1900        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 5, 12, 100)   5200        input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_11 (Reshape)            (None, 5, 5400)      0           embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_12 (Reshape)            (None, 5, 1800)      0           embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_13 (Reshape)            (None, 5, 1200)      0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            (None, 5, 5)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 5, 8405)      0           reshape_11[0][0]                 \n",
      "                                                                 reshape_12[0][0]                 \n",
      "                                                                 reshape_13[0][0]                 \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   (None, 5, 256)       8869888     concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 5, 256)       0           lstm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 5, 85)        21845       dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 16,650,233\n",
      "Trainable params: 8,898,833\n",
      "Non-trainable params: 7,751,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32716 samples, validate on 6426 samples\n",
      "Epoch 1/6\n",
      "32716/32716 [==============================] - 102s 3ms/step - loss: 0.6187 - acc: 0.8274 - val_loss: 0.3851 - val_acc: 0.8776\n",
      "Epoch 2/6\n",
      "32716/32716 [==============================] - 100s 3ms/step - loss: 0.2937 - acc: 0.9093 - val_loss: 0.3546 - val_acc: 0.8837\n",
      "Epoch 3/6\n",
      "32716/32716 [==============================] - 100s 3ms/step - loss: 0.2215 - acc: 0.9329 - val_loss: 0.3282 - val_acc: 0.8912\n",
      "Epoch 4/6\n",
      "32716/32716 [==============================] - 101s 3ms/step - loss: 0.1763 - acc: 0.9482 - val_loss: 0.3218 - val_acc: 0.8942\n",
      "Epoch 5/6\n",
      "32716/32716 [==============================] - 105s 3ms/step - loss: 0.1434 - acc: 0.9599 - val_loss: 0.3205 - val_acc: 0.8943\n",
      "Epoch 6/6\n",
      "32716/32716 [==============================] - 111s 3ms/step - loss: 0.1185 - acc: 0.9682 - val_loss: 0.3241 - val_acc: 0.8948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd1984994e0>"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    [X_train[:, :, :n_w], X_train[:, :, n_w:n_w+n_t], X_train[:, :, n_w+n_t:n_w+n_t+n_d], X_train[:, :, n_w+n_t+n_d:]], \n",
    "          y_train, \n",
    "    validation_data=([X_test[:, :, :n_w], X_test[:, :, n_w:n_w+n_t], X_test[:, :, n_w+n_t:n_w+n_t+n_d], X_test[:, :, n_w+n_t+n_d:]], y_test), \n",
    "          epochs=6, \n",
    "          batch_size=128, \n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LUAS(parser, trees, oracle=None, feature_extractor=None):\n",
    "    total, tpL, tpU, failed = 0, 0, 0, 0\n",
    "    for tree in trees:\n",
    "        golden = [(node[\"id\"], node[\"head\"], node[\"deprel\"]) for node in tree]\n",
    "        try:\n",
    "            _, _, predicted, *_ = parser_2.parse(tree, oracle=oracle, update_label_index=False,\n",
    "                                               feature_extractor=feature_extractor)\n",
    "            total += len(golden)\n",
    "            tpL += len(set(golden).intersection(set(predicted)))\n",
    "            tpU += len(set([(c,h) for c,h,_ in golden]).intersection([(c,h) for c,h,_ in predicted]))\n",
    "        except:\n",
    "            failed += 1\n",
    "    return total, tpL, tpU, failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed: 9\n",
      "Total: 14769\n",
      "Correctly defined (unlabeled): 11227\n",
      "Correctly defined (labeled): 10703\n",
      "UAS: 0.76\n",
      "LAS: 0.725\n"
     ]
    }
   ],
   "source": [
    "total, tpL, tpU, failed = LUAS(parser, test_trees, model, feature_extractor=feature_builder)\n",
    "print(\"Failed:\", failed)\n",
    "print(\"Total:\", total)\n",
    "print(\"Correctly defined (unlabeled):\", tpU)\n",
    "print(\"Correctly defined (labeled):\", tpL)\n",
    "print(\"UAS:\", round(tpU / total, 3))\n",
    "print(\"LAS:\", round(tpL / total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"LSTM.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
