{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import jellyfish\n",
    "import itertools as it\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import dill\n",
    "from Levenshtein import distance\n",
    "import time\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import calc_metrics, importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS = \"snli_1.0.zip\"\n",
    "MAPPING = {\"contradiction\": -1,\n",
    "           \"neutral\": 0, \n",
    "           \"entailment\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en\", disable=[\"textcat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Reading / Converting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data):\n",
    "    for d in data:\n",
    "        prem = d[\"sentence1\"]\n",
    "        hyp = d[\"sentence2\"]\n",
    "        label = d[\"gold_label\"]\n",
    "        if label != \"-\":\n",
    "            yield prem, hyp, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(prefix=\"test\", folder=\"snli_1.0\"):\n",
    "    filename = os.path.join(folder, folder+\"_\"+prefix+\".jsonl\")\n",
    "    with ZipFile(CORPUS) as f:\n",
    "        with f.open(filename) as f_in, open(prefix+\".txt\", \"w+\") as f_out:\n",
    "            lines = (json.loads(line.decode()) for line in f_in) \n",
    "            f_out.write(\"\\t\".join([\"sentence1\", \"sentence2\", \"gold_label\"])+\"\\n\")\n",
    "            for record in get_data(lines):\n",
    "                f_out.write(\"\\t\".join(record)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_prepared(prefix, sep=\"\\t\"):\n",
    "    df = pd.read_csv(prefix+\".txt\", sep=sep)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prefix in [\"test\", \"dev\", \"train\"]:\n",
    "    prepare_data(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev = read_prepared(\"dev\")\n",
    "df_test = read_prepared(\"test\")\n",
    "df_train = read_prepared(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(549361, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(9824, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(9842, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape\n",
    "df_test.shape\n",
    "df_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "entailment       0.333868\n",
       "contradiction    0.333451\n",
       "neutral          0.332681\n",
       "Name: gold_label, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "entailment       0.342834\n",
       "contradiction    0.329499\n",
       "neutral          0.327667\n",
       "Name: gold_label, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "entailment       0.338244\n",
       "contradiction    0.333062\n",
       "neutral          0.328693\n",
       "Name: gold_label, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.gold_label.value_counts(normalize=True)\n",
    "df_test.gold_label.value_counts(normalize=True)\n",
    "df_dev.gold_label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_spacy(df, return_df=False, disable=[\"textcat\"], n_threads=4, size=5000):\n",
    "    doc_prems = nlp.pipe(df[\"sentence1\"].values, disable=disable, n_threads=n_threads, batch_size=size)\n",
    "    doc_hyps = nlp.pipe(df[\"sentence2\"].values, disable=disable, n_threads=n_threads, batch_size=size)\n",
    "    output = zip(doc_prems, doc_hyps, df[\"gold_label\"].values)\n",
    "    if return_df:\n",
    "        return pd.DataFrame.from_records(output, columns=[\"sentence1\", \"sentence2\", \"gold_label\"])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_spacy_docs(docs, name=\"train\"):\n",
    "    vocab_bytes = nlp.vocab.to_bytes()\n",
    "    doc_bytes = [(prem.to_bytes(tensor=False, user_data=False), hyp.to_bytes(tensor=False, user_data=False), label) \n",
    "                 for prem, hyp, label in docs]\n",
    "    with open(name+\"_spacy.bin\", \"wb+\") as handle:\n",
    "        dill.dump((doc_bytes, vocab_bytes), handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spacy_docs(name=\"train\"):\n",
    "    with open(name+\"_spacy.bin\", \"rb\") as handle:\n",
    "        doc_bytes, vocab_bytes = dill.load(handle)\n",
    "        \n",
    "    nlp.vocab.from_bytes(vocab_bytes)\n",
    "    docs = [(Doc(nlp.vocab).from_bytes(prem), Doc(nlp.vocab).from_bytes(hyp), label) \n",
    "            for prem, hyp, label in doc_bytes]\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_dump(df, name=\"train\", n_threads=4, size=5000):\n",
    "    docs = apply_spacy(df, return_df=False, n_threads=n_threads, size=size)\n",
    "    dump_spacy_docs(docs, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_all(n=5000):\n",
    "    apply_dump(df_dev, \"dev\") \n",
    "    apply_dump(df_test, \"test\")\n",
    "    apply_dump(df_train.iloc[:n], \"train\", size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all():\n",
    "    test_docs = load_spacy_docs(\"test\")\n",
    "    train_docs = load_spacy_docs(\"train\")\n",
    "    dev_docs = load_spacy_docs(\"dev\")\n",
    "    return train_docs, test_docs, dev_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00010991096496582031\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "n = 100000\n",
    "#dump_all(n)\n",
    "print(time.time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_dump(df_train.iloc[:n], \"train_sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs, test_docs, dev_docs = load_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "SDX = jellyfish.soundex\n",
    "semcor_ic = wordnet_ic.ic('ic-semcor.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_spacy_to_nltk(tok):\n",
    "    if tok.pos_ == \"VERB\":\n",
    "        return wn.VERB\n",
    "    elif tok.pos_ == \"ADV\":\n",
    "        return wn.ADV\n",
    "    elif tok.pos_ in [\"NOUN\", \"PROPN\", \"PRON\"]:\n",
    "        return wn.NOUN\n",
    "    elif tok.pos_ == \"ADJ\":\n",
    "        return wn.ADJ\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stop(doc):\n",
    "    return (token for token in doc if not token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pos(doc, POS=[\"NOUN\"]):\n",
    "    return (token for token in doc if token.pos_ in POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_to_toks(doc, how=\"lower\"):\n",
    "    if how == \"lower\":\n",
    "        return [tok.lower_ for tok in doc]\n",
    "    elif how == \"lemma\":\n",
    "        return [tok.lemma_ if tok.lemma_!=\"-PRON-\" else tok.lower_ for tok in doc]\n",
    "    elif how == \"text\":\n",
    "        return [tok.text for tok in doc]\n",
    "    elif how == \"text_ws\":\n",
    "        return [tok.text_with_ws for tok in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_ents(prem, hyp, threshold=-2, binary=True):\n",
    "    ents_1 = prem.ents\n",
    "    ents_2 = hyp.ents\n",
    "    score = 0\n",
    "    for hyp_ent in ents_2:\n",
    "        hyp_text = \"\".join(token.text_with_ws for token in filter_stop(hyp_ent))\n",
    "        for prem_ent in ents_1:\n",
    "            prem_text = \"\".join(token.text_with_ws for token in filter_stop(prem_ent))\n",
    "            dist = -1 * distance(SDX(prem_text), SDX(hyp_text))\n",
    "            if dist >= threshold:\n",
    "                break\n",
    "        else:\n",
    "            score += 1\n",
    "            if binary:\n",
    "                return score\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_diff(prem, hyp):\n",
    "    return len(hyp.text) - len(prem.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BLEU(prem_tokens, hyp_tokens, weights=[0.25]*4):\n",
    "    return sentence_bleu([prem_tokens], hyp_tokens, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_lin_sim(a, b, pos_a, pos_b):\n",
    "    s1 = wn.synsets(a, pos=pos_a)\n",
    "    s2 = wn.synsets(b, pos=pos_b)\n",
    "    try:\n",
    "        sims = [a.lin_similarity(b, semcor_ic) for a,b in it.product(s1, s2) if a.pos()==b.pos()]\n",
    "        if not sims:\n",
    "            return 0.0\n",
    "        return np.max(sims)\n",
    "    except:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity_with_sim(prem, hyp):\n",
    "    s1 = set([(t.lower_, pos_spacy_to_nltk(t)) for t in prem])\n",
    "    s2 = set([(t.lower_, pos_spacy_to_nltk(t)) for t in hyp])\n",
    "    if not s1 and not s2:\n",
    "        return 0, 0\n",
    "    overlap = 0\n",
    "    for t2, pos2 in s2:\n",
    "        for t1, pos1 in s1:\n",
    "            if t1 == t2 or find_max_lin_sim(t1, t2, pos1, pos2) >= THRESHOLD:\n",
    "                overlap += 1\n",
    "                break\n",
    "    try:\n",
    "        return overlap / (len(s1)+len(s2)-overlap), overlap\n",
    "    except:\n",
    "        return 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(prem_tokens, hyp_tokens):\n",
    "    s1 = set(prem_tokens)\n",
    "    s2 = set(hyp_tokens)\n",
    "    if not s1 and not s2:\n",
    "        return 0, 0\n",
    "    return len(s1 & s2) / len(s1 | s2), len(s1 & s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipngram(doc, n=2):\n",
    "    return zip(*[doc[i:] for i in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    return set(lemma.name() for syns in wn.synsets(word) for lemma in syns.lemmas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCLUDE_LEMMAS = [\"be\", \"have\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verb_matching(prem, hyp, binary=True):\n",
    "    prem_verbs = [tok for tok in doc_to_toks(filter_pos(prem, [\"VERB\"]), \"lemma\") if tok not in EXCLUDE_LEMMAS]\n",
    "    hyp_verbs = [tok for tok in doc_to_toks(filter_pos(hyp, [\"VERB\"]), \"lemma\") if tok not in EXCLUDE_LEMMAS]\n",
    "    score = 0\n",
    "    for v1 in hyp_verbs:\n",
    "        for v2 in prem_verbs:\n",
    "            if v1 == v2 or v1 in get_synonyms(v2):\n",
    "                break\n",
    "        else:\n",
    "            score += 1\n",
    "            if binary:\n",
    "                return score\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(docs):\n",
    "    features = []\n",
    "    for prem, hyp, _ in docs:\n",
    "        record = {}\n",
    "        prem_tokens = doc_to_toks(prem, how=\"lower\")\n",
    "        hyp_tokens = doc_to_toks(hyp, how=\"lower\")\n",
    "        # Unlexicalized features\n",
    "        record[\"NE_Score\"] = compare_ents(prem, hyp)\n",
    "        record[\"len_diff\"] = len_diff(prem, hyp)\n",
    "        record[\"BLEU\"] = BLEU(prem_tokens, hyp_tokens)\n",
    "        \n",
    "        how = \"lower\"\n",
    "        record[\"jaccard_total\"], record[\"overlap_total\"] = jaccard_similarity_with_sim(prem, hyp)\n",
    "        \n",
    "        POS = [\"NOUN\", \"PRON\", \"PROPN\"]\n",
    "        record[\"jaccard_NOUN\"], record[\"overlap_NOUN\"] = jaccard_similarity_with_sim(filter_pos(prem, POS), filter_pos(hyp, POS))\n",
    "        \n",
    "        POS = [\"VERB\"]\n",
    "        record[\"jaccard_VERB\"], record[\"overlap_VERB\"] = jaccard_similarity_with_sim(filter_pos(prem, POS), filter_pos(hyp, POS))\n",
    "        \n",
    "        POS = [\"ADJ\"]\n",
    "        record[\"jaccard_ADJ\"], record[\"overlap_ADJ\"] = jaccard_similarity_with_sim(filter_pos(prem, POS), filter_pos(hyp, POS))\n",
    "        \n",
    "        POS = [\"ADV\"]\n",
    "        record[\"jaccard_ADV\"], record[\"overlap_ADV\"] = jaccard_similarity_with_sim(filter_pos(prem, POS), filter_pos(hyp, POS))\n",
    "        # Unigrams & Bigrams\n",
    "        for tok in hyp_tokens:\n",
    "            record[f\"w={tok}\"] = 1\n",
    "            \n",
    "        for w1, w2 in zipngram(hyp_tokens, 2):\n",
    "            record[f\"w1={w1},w2={w2}\"] = 1\n",
    "        \n",
    "        # Cross - unigrams with the same POS\n",
    "        for w1, w2 in it.product(prem, hyp):\n",
    "            if w1.pos_ == w2.pos_:\n",
    "                record[f\"cross-w1={w1.lemma_},w2={w2.lemma_}\"] = 1\n",
    "        \n",
    "        # Cross - bigrams\n",
    "        for b1, b2 in it.product(zipngram(prem, 2), zipngram(hyp, 2)):\n",
    "            if b1[-1].pos_ == b2[-1].pos_:\n",
    "                record[f\"cross-w11={b1[0].lemma_},w12={b1[1].lemma_},w21={b2[0].lemma_},w22={b2[1].lemma_}\"] = 1\n",
    "        \n",
    "        # Verb matching useing WordNet\n",
    "        record[\"verb_match\"] = verb_matching(prem, hyp)\n",
    "        \n",
    "        features.append(record)\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = DictVectorizer(sparse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = build_features(train_docs)\n",
    "_, _, train_labels = zip(*train_docs)\n",
    "v_train = vectorizer.fit_transform(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = build_features(test_docs)\n",
    "_, _, test_labels = zip(*test_docs)\n",
    "v_test = vectorizer.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_features = build_features(dev_docs)\n",
    "_, _, dev_labels = zip(*dev_docs)\n",
    "v_dev = vectorizer.transform(dev_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C=1, random_state=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=25, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(v_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance(clf.classes_, clf.coef_, vectorizer.feature_names_, n=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.787\n",
      "Precision: 0.787\n",
      "F1: 0.786\n",
      "Accuracy: 0.787\n",
      "\n",
      "Confusion matrix:\n",
      "               pred_contradiction  pred_entailment  pred_neutral\n",
      "contradiction                2548              304           385\n",
      "entailment                    161             2857           350\n",
      "neutral                       399              496          2324\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.82      0.79      0.80      3237\n",
      "   entailment       0.78      0.85      0.81      3368\n",
      "      neutral       0.76      0.72      0.74      3219\n",
      "\n",
      "  avg / total       0.79      0.79      0.79      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(v_test)\n",
    "output, report, conf_matrix = calc_metrics(test_labels, pred, proba=None, labels=clf.classes_, \n",
    "                                           print_=True, mode=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.784\n",
      "Precision: 0.784\n",
      "F1: 0.784\n",
      "Accuracy: 0.784\n",
      "\n",
      "Confusion matrix:\n",
      "               pred_contradiction  pred_entailment  pred_neutral\n",
      "contradiction                2599              291           388\n",
      "entailment                    179             2829           321\n",
      "neutral                       435              507          2293\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.81      0.79      0.80      3278\n",
      "   entailment       0.78      0.85      0.81      3329\n",
      "      neutral       0.76      0.71      0.74      3235\n",
      "\n",
      "  avg / total       0.78      0.78      0.78      9842\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(v_dev)\n",
    "output, report, conf_matrix = calc_metrics(dev_labels, pred, proba=None, labels=clf.classes_, \n",
    "                                           print_=True, mode=\"weighted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
