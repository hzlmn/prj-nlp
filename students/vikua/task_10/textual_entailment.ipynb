{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy \n",
    "import gensim\n",
    "\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_labels</th>\n",
       "      <th>captionID</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>pairID</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence1_binary_parse</th>\n",
       "      <th>sentence1_parse</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>sentence2_binary_parse</th>\n",
       "      <th>sentence2_parse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[neutral]</td>\n",
       "      <td>3416050480.jpg#4</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3416050480.jpg#4r1n</td>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
       "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
       "      <td>A person is training his horse for a competition.</td>\n",
       "      <td>( ( A person ) ( ( is ( ( training ( his horse...</td>\n",
       "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[contradiction]</td>\n",
       "      <td>3416050480.jpg#4</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>3416050480.jpg#4r1c</td>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
       "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
       "      <td>A person is at a diner, ordering an omelette.</td>\n",
       "      <td>( ( A person ) ( ( ( ( is ( at ( a diner ) ) )...</td>\n",
       "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[entailment]</td>\n",
       "      <td>3416050480.jpg#4</td>\n",
       "      <td>entailment</td>\n",
       "      <td>3416050480.jpg#4r1e</td>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
       "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
       "      <td>A person is outdoors, on a horse.</td>\n",
       "      <td>( ( A person ) ( ( ( ( is outdoors ) , ) ( on ...</td>\n",
       "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[neutral]</td>\n",
       "      <td>2267923837.jpg#2</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2267923837.jpg#2r1n</td>\n",
       "      <td>Children smiling and waving at camera</td>\n",
       "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
       "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
       "      <td>They are smiling at their parents</td>\n",
       "      <td>( They ( are ( smiling ( at ( their parents ) ...</td>\n",
       "      <td>(ROOT (S (NP (PRP They)) (VP (VBP are) (VP (VB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[entailment]</td>\n",
       "      <td>2267923837.jpg#2</td>\n",
       "      <td>entailment</td>\n",
       "      <td>2267923837.jpg#2r1e</td>\n",
       "      <td>Children smiling and waving at camera</td>\n",
       "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
       "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
       "      <td>There are children present</td>\n",
       "      <td>( There ( ( are children ) present ) )</td>\n",
       "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NN...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  annotator_labels         captionID     gold_label               pairID  \\\n",
       "0        [neutral]  3416050480.jpg#4        neutral  3416050480.jpg#4r1n   \n",
       "1  [contradiction]  3416050480.jpg#4  contradiction  3416050480.jpg#4r1c   \n",
       "2     [entailment]  3416050480.jpg#4     entailment  3416050480.jpg#4r1e   \n",
       "3        [neutral]  2267923837.jpg#2        neutral  2267923837.jpg#2r1n   \n",
       "4     [entailment]  2267923837.jpg#2     entailment  2267923837.jpg#2r1e   \n",
       "\n",
       "                                           sentence1  \\\n",
       "0  A person on a horse jumps over a broken down a...   \n",
       "1  A person on a horse jumps over a broken down a...   \n",
       "2  A person on a horse jumps over a broken down a...   \n",
       "3              Children smiling and waving at camera   \n",
       "4              Children smiling and waving at camera   \n",
       "\n",
       "                              sentence1_binary_parse  \\\n",
       "0  ( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...   \n",
       "1  ( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...   \n",
       "2  ( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...   \n",
       "3  ( Children ( ( ( smiling and ) waving ) ( at c...   \n",
       "4  ( Children ( ( ( smiling and ) waving ) ( at c...   \n",
       "\n",
       "                                     sentence1_parse  \\\n",
       "0  (ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...   \n",
       "1  (ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...   \n",
       "2  (ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...   \n",
       "3  (ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...   \n",
       "4  (ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...   \n",
       "\n",
       "                                           sentence2  \\\n",
       "0  A person is training his horse for a competition.   \n",
       "1      A person is at a diner, ordering an omelette.   \n",
       "2                  A person is outdoors, on a horse.   \n",
       "3                  They are smiling at their parents   \n",
       "4                         There are children present   \n",
       "\n",
       "                              sentence2_binary_parse  \\\n",
       "0  ( ( A person ) ( ( is ( ( training ( his horse...   \n",
       "1  ( ( A person ) ( ( ( ( is ( at ( a diner ) ) )...   \n",
       "2  ( ( A person ) ( ( ( ( is outdoors ) , ) ( on ...   \n",
       "3  ( They ( are ( smiling ( at ( their parents ) ...   \n",
       "4             ( There ( ( are children ) present ) )   \n",
       "\n",
       "                                     sentence2_parse  \n",
       "0  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...  \n",
       "1  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...  \n",
       "2  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...  \n",
       "3  (ROOT (S (NP (PRP They)) (VP (VBP are) (VP (VB...  \n",
       "4  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NN...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_json('../../../../data/snli_1.0/snli_1.0_train.jsonl', lines=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = train[['sentence1', 'sentence2', 'gold_label']]\n",
    "tr = tr[tr['gold_label'] != '-']\n",
    "\n",
    "dev = pd.read_json('../../../../data/snli_1.0/snli_1.0_dev.jsonl', lines=True)[['sentence1', 'sentence2', 'gold_label']]\n",
    "dev = dev[dev['gold_label'] != '-']\n",
    "test = pd.read_json('../../../../data/snli_1.0/snli_1.0_test.jsonl', lines=True)[['sentence1', 'sentence2', 'gold_label']]\n",
    "test = test[test['gold_label'] != '-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 20000\n",
    "c = resample(tr[tr['gold_label'] == 'contradiction'], replace=False, n_samples=n_samples, random_state=1234)\n",
    "e = resample(tr[tr['gold_label'] == 'entailment'], replace=False, n_samples=n_samples, random_state=1234)\n",
    "n = resample(tr[tr['gold_label'] == 'neutral'], replace=False, n_samples=n_samples, random_state=1234)\n",
    "\n",
    "tr = pd.concat([c, e, n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['ner', 'textcat'])\n",
    "\n",
    "def spacy_parse(df):\n",
    "    docs = []\n",
    "    for doc in nlp.pipe(df['sentence1'].values, batch_size=200, n_threads=16):\n",
    "        docs.append(doc)\n",
    "    df['t'] = pd.Series(docs, index=df.index)\n",
    "    \n",
    "    docs = []\n",
    "    for doc in nlp.pipe(df['sentence2'].values, batch_size=200, n_threads=16):\n",
    "        docs.append(doc)\n",
    "    df['h'] = pd.Series(docs, index=df.index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stop_words(df): \n",
    "    df['t'] = df['t'].apply(lambda doc: [x for x in doc if x.lemma_ not in stop_words])\n",
    "    df['t'] = df['t'].apply(lambda doc: [x for x in doc if x.pos_ != 'PUNCT'])\n",
    "    \n",
    "    df['h'] = df['h'].apply(lambda doc: [x for x in doc if x.lemma_ not in stop_words])\n",
    "    df['h'] = df['h'].apply(lambda doc: [x for x in doc if x.pos_ != 'PUNCT'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 14s, sys: 42.3 s, total: 6min 57s\n",
      "Wall time: 4min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "tr = spacy_parse(tr)\n",
    "dev = spacy_parse(dev)\n",
    "test = spacy_parse(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.9 s, sys: 105 ms, total: 4 s\n",
      "Wall time: 4 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "tr = remove_stop_words(tr)\n",
    "dev = remove_stop_words(dev)\n",
    "test = remove_stop_words(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>t</th>\n",
       "      <th>h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>307667</th>\n",
       "      <td>The street sweeping crew cleans the street wit...</td>\n",
       "      <td>A street crew cleans an empty street.</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>[street, sweeping, crew, cleans, street, crowd...</td>\n",
       "      <td>[street, crew, cleans, empty, street]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209053</th>\n",
       "      <td>A surfer walking into the ocean</td>\n",
       "      <td>There is a surfer sitting on the ground.</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>[surfer, walking, ocean]</td>\n",
       "      <td>[surfer, sitting, ground]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288898</th>\n",
       "      <td>There is a man lying in the sun next to the oc...</td>\n",
       "      <td>A man is sitting on a bench at the playground.</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>[man, lying, sun, next, ocean]</td>\n",
       "      <td>[man, sitting, bench, playground]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268571</th>\n",
       "      <td>Two women waiting at the subway station.</td>\n",
       "      <td>A man sleeping in the subway station.</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>[Two, women, waiting, subway, station]</td>\n",
       "      <td>[man, sleeping, subway, station]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378325</th>\n",
       "      <td>Workers wearing orange reflective vests walkin...</td>\n",
       "      <td>Workers are on a break and sitting down playin...</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>[Workers, wearing, orange, reflective, vests, ...</td>\n",
       "      <td>[Workers, break, sitting, playing, cards]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence1  \\\n",
       "307667  The street sweeping crew cleans the street wit...   \n",
       "209053                    A surfer walking into the ocean   \n",
       "288898  There is a man lying in the sun next to the oc...   \n",
       "268571           Two women waiting at the subway station.   \n",
       "378325  Workers wearing orange reflective vests walkin...   \n",
       "\n",
       "                                                sentence2     gold_label  \\\n",
       "307667              A street crew cleans an empty street.  contradiction   \n",
       "209053           There is a surfer sitting on the ground.  contradiction   \n",
       "288898     A man is sitting on a bench at the playground.  contradiction   \n",
       "268571              A man sleeping in the subway station.  contradiction   \n",
       "378325  Workers are on a break and sitting down playin...  contradiction   \n",
       "\n",
       "                                                        t  \\\n",
       "307667  [street, sweeping, crew, cleans, street, crowd...   \n",
       "209053                           [surfer, walking, ocean]   \n",
       "288898                     [man, lying, sun, next, ocean]   \n",
       "268571             [Two, women, waiting, subway, station]   \n",
       "378325  [Workers, wearing, orange, reflective, vests, ...   \n",
       "\n",
       "                                                h  \n",
       "307667      [street, crew, cleans, empty, street]  \n",
       "209053                  [surfer, sitting, ground]  \n",
       "288898          [man, sitting, bench, playground]  \n",
       "268571           [man, sleeping, subway, station]  \n",
       "378325  [Workers, break, sitting, playing, cards]  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic = wordnet_ic.ic('ic-brown.dat')\n",
    "\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    "    if tag.startswith('V'):\n",
    "        return 'v'\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    "    if tag.startswith('R'):\n",
    "        return 'r'\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_synset(token): \n",
    "    wn_tag = penn_to_wn(token.tag_)\n",
    "    try: \n",
    "        return wn.synsets(token.text, wn_tag)[0]\n",
    "    except: \n",
    "        return None\n",
    "    \n",
    "    \n",
    "def sentence_similarity(sentence1, sentence2, sim_func): \n",
    "    sentence1_synsets = [x for x in sentence1 if x.lemma_ not in stopWords]\n",
    "    sentence2_synsets = [x for x in sentence2 if x.lemma_ not in stopWords]\n",
    "\n",
    "    sentence1_synsets = [get_synset(x) for x in sentence1_synsets]\n",
    "    sentence2_synsets = [get_synset(x) for x in sentence2_synsets]\n",
    "    \n",
    "    sentence1_synsets = [x for x in sentence1_synsets if x is not None]\n",
    "    sentence2_synsets = [x for x in sentence2_synsets if x is not None]\n",
    "        \n",
    "    score, count = 0.0, 0\n",
    "    for s1 in sentence1_synsets: \n",
    "        scores = [sim_func(s1, s2) for s2 in sentence2_synsets]\n",
    "        if scores:\n",
    "            best_score = max(scores)\n",
    "        else: \n",
    "            best_score = 0\n",
    "            \n",
    "        if best_score is not None: \n",
    "            score += best_score\n",
    "            count += 1\n",
    "    if count == 0: \n",
    "        return 0 \n",
    "    else:\n",
    "        return score / count\n",
    "\n",
    "\n",
    "def path_similarity(sentence1, sentence2): \n",
    "    def closure(x, y): \n",
    "        r = x.path_similarity(y)\n",
    "        return r if r else 0\n",
    "    return sentence_similarity(sentence1, sentence2, closure)\n",
    "\n",
    "\n",
    "def lin_similarity(sentence1, sentence2): \n",
    "    def closure(x, y): \n",
    "        try: \n",
    "            r = x.lin_similarity(y, ic)\n",
    "        except: \n",
    "            r = None\n",
    "        return r if r else 0\n",
    "    return sentence_similarity(sentence1, sentence2, closure)\n",
    "\n",
    "\n",
    "def res_similarity(sentence1, sentence2): \n",
    "    def closure(x, y): \n",
    "        try: \n",
    "            r = x.res_similarity(y, ic)\n",
    "        except: \n",
    "            r = None\n",
    "        return r if r else 0\n",
    "    return sentence_similarity(sentence1, sentence2, closure)\n",
    "\n",
    "\n",
    "def wup_similarity(sentence1, sentence2): \n",
    "    def closure(x, y): \n",
    "        r = x.wup_similarity(y)\n",
    "        return r if r else 0\n",
    "    return sentence_similarity(sentence1, sentence2, closure)\n",
    "\n",
    "\n",
    "def jcn_similarity(sentence1, sentence2): \n",
    "    def closure(x, y): \n",
    "        try: \n",
    "            r = x.jcn_similarity(y, ic)\n",
    "        except: \n",
    "            r = None\n",
    "        return r if r else 0\n",
    "\n",
    "    return sentence_similarity(sentence1, sentence2, closure)\n",
    "\n",
    "\n",
    "def lch_similarity(sentence1, sentence2): \n",
    "    def closure(x, y): \n",
    "        if x.pos() != y.pos(): \n",
    "            r = 0\n",
    "        else: \n",
    "            r = x.lch_similarity(y)\n",
    "        return r if r else 0\n",
    "    return sentence_similarity(sentence1, sentence2, closure)\n",
    "\n",
    "\n",
    "def apply_all_functions(df): \n",
    "    df['path_similarity'] = df[['t', 'h']].apply(lambda x: path_similarity(x['t'], x['h']), axis=1)\n",
    "    print('path_similarity computed')\n",
    "    df['lin_similarity'] = df[['t', 'h']].apply(lambda x: lin_similarity(x['t'], x['h']), axis=1)\n",
    "    print('lin_similarity computed')\n",
    "    df['res_similarity'] = df[['t', 'h']].apply(lambda x: res_similarity(x['t'], x['h']), axis=1)\n",
    "    print('res_similarity computed')\n",
    "    df['wup_similarity'] = df[['t', 'h']].apply(lambda x: wup_similarity(x['t'], x['h']), axis=1)\n",
    "    print('wup_similarity computed')\n",
    "    df['jcn_similarity'] = df[['t', 'h']].apply(lambda x: jcn_similarity(x['t'], x['h']), axis=1)\n",
    "    print('jcn_similarity computed')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path_similarity computed\n",
      "lin_similarity computed\n",
      "res_similarity computed\n",
      "wup_similarity computed\n",
      "jcn_similarity computed\n",
      "path_similarity computed\n",
      "lin_similarity computed\n",
      "res_similarity computed\n",
      "wup_similarity computed\n",
      "jcn_similarity computed\n",
      "path_similarity computed\n",
      "lin_similarity computed\n",
      "res_similarity computed\n",
      "wup_similarity computed\n",
      "jcn_similarity computed\n",
      "CPU times: user 9min 20s, sys: 11.7 s, total: 9min 32s\n",
      "Wall time: 9min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tr = apply_all_functions(tr)\n",
    "dev = apply_all_functions(dev)\n",
    "test = apply_all_functions(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>t</th>\n",
       "      <th>h</th>\n",
       "      <th>path_similarity</th>\n",
       "      <th>lin_similarity</th>\n",
       "      <th>res_similarity</th>\n",
       "      <th>wup_similarity</th>\n",
       "      <th>jcn_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>307667</th>\n",
       "      <td>The street sweeping crew cleans the street wit...</td>\n",
       "      <td>A street crew cleans an empty street.</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>[street, sweeping, crew, cleans, street, crowd...</td>\n",
       "      <td>[street, crew, cleans, empty, street]</td>\n",
       "      <td>0.586806</td>\n",
       "      <td>0.595422</td>\n",
       "      <td>5.394790e+00</td>\n",
       "      <td>0.736772</td>\n",
       "      <td>5.000000e+299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209053</th>\n",
       "      <td>A surfer walking into the ocean</td>\n",
       "      <td>There is a surfer sitting on the ground.</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>[surfer, walking, ocean]</td>\n",
       "      <td>[surfer, sitting, ground]</td>\n",
       "      <td>0.472222</td>\n",
       "      <td>0.368524</td>\n",
       "      <td>3.333333e+299</td>\n",
       "      <td>0.614815</td>\n",
       "      <td>3.333333e+299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288898</th>\n",
       "      <td>There is a man lying in the sun next to the oc...</td>\n",
       "      <td>A man is sitting on a bench at the playground.</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>[man, lying, sun, next, ocean]</td>\n",
       "      <td>[man, sitting, bench, playground]</td>\n",
       "      <td>0.295000</td>\n",
       "      <td>0.259588</td>\n",
       "      <td>1.710657e+00</td>\n",
       "      <td>0.446845</td>\n",
       "      <td>2.000000e+299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268571</th>\n",
       "      <td>Two women waiting at the subway station.</td>\n",
       "      <td>A man sleeping in the subway station.</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>[Two, women, waiting, subway, station]</td>\n",
       "      <td>[man, sleeping, subway, station]</td>\n",
       "      <td>0.515385</td>\n",
       "      <td>0.571579</td>\n",
       "      <td>4.876810e+00</td>\n",
       "      <td>0.632581</td>\n",
       "      <td>4.000000e+299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378325</th>\n",
       "      <td>Workers wearing orange reflective vests walkin...</td>\n",
       "      <td>Workers are on a break and sitting down playin...</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>[Workers, wearing, orange, reflective, vests, ...</td>\n",
       "      <td>[Workers, break, sitting, playing, cards]</td>\n",
       "      <td>0.212037</td>\n",
       "      <td>0.177669</td>\n",
       "      <td>9.857330e-01</td>\n",
       "      <td>0.339191</td>\n",
       "      <td>1.111111e+299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence1  \\\n",
       "307667  The street sweeping crew cleans the street wit...   \n",
       "209053                    A surfer walking into the ocean   \n",
       "288898  There is a man lying in the sun next to the oc...   \n",
       "268571           Two women waiting at the subway station.   \n",
       "378325  Workers wearing orange reflective vests walkin...   \n",
       "\n",
       "                                                sentence2     gold_label  \\\n",
       "307667              A street crew cleans an empty street.  contradiction   \n",
       "209053           There is a surfer sitting on the ground.  contradiction   \n",
       "288898     A man is sitting on a bench at the playground.  contradiction   \n",
       "268571              A man sleeping in the subway station.  contradiction   \n",
       "378325  Workers are on a break and sitting down playin...  contradiction   \n",
       "\n",
       "                                                        t  \\\n",
       "307667  [street, sweeping, crew, cleans, street, crowd...   \n",
       "209053                           [surfer, walking, ocean]   \n",
       "288898                     [man, lying, sun, next, ocean]   \n",
       "268571             [Two, women, waiting, subway, station]   \n",
       "378325  [Workers, wearing, orange, reflective, vests, ...   \n",
       "\n",
       "                                                h  path_similarity  \\\n",
       "307667      [street, crew, cleans, empty, street]         0.586806   \n",
       "209053                  [surfer, sitting, ground]         0.472222   \n",
       "288898          [man, sitting, bench, playground]         0.295000   \n",
       "268571           [man, sleeping, subway, station]         0.515385   \n",
       "378325  [Workers, break, sitting, playing, cards]         0.212037   \n",
       "\n",
       "        lin_similarity  res_similarity  wup_similarity  jcn_similarity  \n",
       "307667        0.595422    5.394790e+00        0.736772   5.000000e+299  \n",
       "209053        0.368524   3.333333e+299        0.614815   3.333333e+299  \n",
       "288898        0.259588    1.710657e+00        0.446845   2.000000e+299  \n",
       "268571        0.571579    4.876810e+00        0.632581   4.000000e+299  \n",
       "378325        0.177669    9.857330e-01        0.339191   1.111111e+299  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(tr['gold_label'])\n",
    "y_dev = le.transform(dev['gold_label'])\n",
    "y_test = le.transform(test['gold_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tr[['path_similarity', 'lin_similarity', 'res_similarity', 'wup_similarity', 'jcn_similarity']]\n",
    "X_dev = dev[['path_similarity', 'lin_similarity', 'res_similarity', 'wup_similarity', 'jcn_similarity']]\n",
    "X_test = test[['path_similarity', 'lin_similarity', 'res_similarity', 'wup_similarity', 'jcn_similarity']]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_dev = scaler.transform(X_dev)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.43      0.62      0.51      3278\n",
      "          1       0.46      0.53      0.49      3329\n",
      "          2       0.32      0.13      0.19      3235\n",
      "\n",
      "avg / total       0.41      0.43      0.40      9842\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.39609149797552184"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SGDClassifier(loss='log', penalty='elasticnet', n_jobs=-1, learning_rate='optimal')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_dev_pred = clf.predict(X_dev)\n",
    "print(classification_report(y_dev, y_dev_pred))\n",
    "f1_score(y_dev, y_dev_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.44      0.57      0.50      3278\n",
      "          1       0.46      0.53      0.49      3329\n",
      "          2       0.39      0.21      0.27      3235\n",
      "\n",
      "avg / total       0.43      0.44      0.42      9842\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.42095178772069414"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm\n",
    "clf = lightgbm.LGBMClassifier(boosting_type='gbdt', objective='multiclass', n_estimators=300, reg_lambda=0.1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_dev_pred = clf.predict(X_dev)\n",
    "print(classification_report(y_dev, y_dev_pred))\n",
    "f1_score(y_dev, y_dev_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train = tr['t'].apply(lambda doc: ' '.join(['{}_{}'.format(x.lemma_, x.pos_) for x in doc]))\n",
    "h_train = tr['h'].apply(lambda doc: ' '.join(['{}_{}'.format(x.lemma_, x.pos_) for x in doc]))\n",
    "\n",
    "t_dev = dev['t'].apply(lambda doc: ' '.join(['{}_{}'.format(x.lemma_, x.pos_) for x in doc]))\n",
    "h_dev = dev['h'].apply(lambda doc: ' '.join(['{}_{}'.format(x.lemma_, x.pos_) for x in doc]))\n",
    "\n",
    "t_test = test['t'].apply(lambda doc: ' '.join(['{}_{}'.format(x.lemma_, x.pos_) for x in doc]))\n",
    "h_test = test['h'].apply(lambda doc: ' '.join(['{}_{}'.format(x.lemma_, x.pos_) for x in doc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_t = TfidfVectorizer()\n",
    "t_train = vect_t.fit_transform(t_train).toarray()\n",
    "t_dev = vect_t.transform(t_dev).toarray()\n",
    "t_test = vect_t.transform(t_test).toarray()\n",
    "\n",
    "vect_h = TfidfVectorizer()\n",
    "h_train = vect_h.fit_transform(h_train).toarray()\n",
    "h_dev = vect_h.transform(h_dev).toarray()\n",
    "h_test = vect_h.transform(h_test).toarray()\n",
    "\n",
    "X_train = np.hstack((t_train, h_train))\n",
    "X_dev = np.hstack((t_dev, h_dev))\n",
    "X_test = np.hstack((t_test, h_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      0.44      0.54      3278\n",
      "          1       0.59      0.66      0.63      3329\n",
      "          2       0.56      0.71      0.63      3235\n",
      "\n",
      "avg / total       0.62      0.60      0.60      9842\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5979082320731927"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SGDClassifier(loss='log', penalty='elasticnet', n_jobs=-1, learning_rate='optimal')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_dev_pred = clf.predict(X_dev)\n",
    "print(classification_report(y_dev, y_dev_pred))\n",
    "f1_score(y_dev, y_dev_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.63      0.67      3278\n",
      "          1       0.68      0.74      0.71      3329\n",
      "          2       0.63      0.67      0.65      3235\n",
      "\n",
      "avg / total       0.68      0.68      0.68      9842\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.678847595595491"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm\n",
    "clf = lightgbm.LGBMClassifier(boosting_type='gbdt', objective='multiclass', n_estimators=300, reg_lambda=0.1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_dev_pred = clf.predict(X_dev)\n",
    "print(classification_report(y_dev, y_dev_pred))\n",
    "f1_score(y_dev, y_dev_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('/Users/victor/Downloads/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel/__main__.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if __name__ == '__main__':\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel/__main__.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec, dim):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = dim\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "    \n",
    "    \n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec, dim):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = dim\n",
    "\n",
    "    def fit(self, X):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = MeanEmbeddingVectorizer(w2v, 300)\n",
    "X_train_t = vect.transform(tr['t'].apply(lambda doc: [x.text.lower() for x in doc]).values)\n",
    "X_train_h = vect.transform(tr['h'].apply(lambda doc: [x.text.lower() for x in doc]).values)\n",
    "\n",
    "X_dev_t = vect.transform(dev['t'].apply(lambda doc: [x.text.lower() for x in doc]).values)\n",
    "X_dev_h = vect.transform(dev['h'].apply(lambda doc: [x.text.lower() for x in doc]).values)\n",
    "\n",
    "X_train = np.hstack((X_train_t, X_train_h))\n",
    "X_dev = np.hstack((X_dev_t, X_dev_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.58      0.58      3278\n",
      "          1       0.59      0.63      0.61      3329\n",
      "          2       0.60      0.56      0.58      3235\n",
      "\n",
      "avg / total       0.59      0.59      0.59      9842\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5877005052429903"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SGDClassifier(loss='log', penalty='elasticnet', n_jobs=-1, learning_rate='optimal')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_dev_pred = clf.predict(X_dev)\n",
    "print(classification_report(y_dev, y_dev_pred))\n",
    "f1_score(y_dev, y_dev_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.67      0.64      0.66      3278\n",
      "          1       0.69      0.69      0.69      3329\n",
      "          2       0.61      0.63      0.62      3235\n",
      "\n",
      "avg / total       0.66      0.66      0.66      9842\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.65518661860427"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm\n",
    "clf = lightgbm.LGBMClassifier(boosting_type='gbdt', objective='multiclass', n_estimators=300, reg_lambda=0.1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_dev_pred = clf.predict(X_dev)\n",
    "print(classification_report(y_dev, y_dev_pred))\n",
    "f1_score(y_dev, y_dev_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_t = TfidfEmbeddingVectorizer(w2v, 300)\n",
    "vect_h = TfidfEmbeddingVectorizer(w2v, 300)\n",
    "\n",
    "t_train = tr['t'].apply(lambda doc: [x.text.lower() for x in doc]).values\n",
    "vect_t.fit(t_train)\n",
    "h_train = tr['h'].apply(lambda doc: [x.text.lower() for x in doc]).values\n",
    "vect_h.fit(h_train)\n",
    "\n",
    "X_train_t = vect_t.transform(t_train)\n",
    "X_train_h = vect_h.transform(h_train)\n",
    "\n",
    "X_dev_t = vect_t.transform(dev['t'].apply(lambda doc: [x.text.lower() for x in doc]).values)\n",
    "X_dev_h = vect_h.transform(dev['h'].apply(lambda doc: [x.text.lower() for x in doc]).values)\n",
    "\n",
    "X_train = np.hstack((X_train_t, X_train_h))\n",
    "X_dev = np.hstack((X_dev_t, X_dev_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.45      0.51      3278\n",
      "          1       0.51      0.64      0.57      3329\n",
      "          2       0.54      0.53      0.54      3235\n",
      "\n",
      "avg / total       0.55      0.54      0.54      9842\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5385851789427484"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SGDClassifier(loss='log', penalty='elasticnet', n_jobs=-1, learning_rate='optimal')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_dev_pred = clf.predict(X_dev)\n",
    "print(classification_report(y_dev, y_dev_pred))\n",
    "f1_score(y_dev, y_dev_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.65      0.65      3278\n",
      "          1       0.67      0.67      0.67      3329\n",
      "          2       0.61      0.62      0.61      3235\n",
      "\n",
      "avg / total       0.65      0.65      0.65      9842\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6449966066741241"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm\n",
    "clf = lightgbm.LGBMClassifier(boosting_type='gbdt', objective='multiclass', n_estimators=300, reg_lambda=0.1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_dev_pred = clf.predict(X_dev)\n",
    "print(classification_report(y_dev, y_dev_pred))\n",
    "f1_score(y_dev, y_dev_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tr['t'].apply(lambda doc: ['{}_{}'.format(x.lemma_, x.pos_) for x in doc])\n",
    "h = tr['h'].apply(lambda doc: ['{}_{}'.format(x.lemma_, x.pos_) for x in doc]) \n",
    "sentences = t.tolist() + h.tolist()\n",
    "\n",
    "model = gensim.models.Word2Vec(sentences, size=200, window=5, min_count=5, workers=8, iter=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel/__main__.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))\n",
    "vect = MeanEmbeddingVectorizer(w2v, 200)\n",
    "\n",
    "X_train_t = vect.transform(tr['t'].apply(lambda doc: ['{}_{}'.format(x.lemma_, x.pos_) for x in doc]).values)\n",
    "X_train_h = vect.transform(tr['h'].apply(lambda doc: ['{}_{}'.format(x.lemma_, x.pos_) for x in doc]).values)\n",
    "\n",
    "X_dev_t = vect.transform(dev['t'].apply(lambda doc: ['{}_{}'.format(x.lemma_, x.pos_) for x in doc]).values)\n",
    "X_dev_h = vect.transform(dev['h'].apply(lambda doc: ['{}_{}'.format(x.lemma_, x.pos_) for x in doc]).values)\n",
    "\n",
    "X_train = np.hstack((X_train_t, X_train_h))\n",
    "X_dev = np.hstack((X_dev_t, X_dev_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.40      0.48      3278\n",
      "          1       0.50      0.77      0.60      3329\n",
      "          2       0.61      0.48      0.54      3235\n",
      "\n",
      "avg / total       0.57      0.55      0.54      9842\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5410131091002927"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SGDClassifier(loss='log', penalty='elasticnet', n_jobs=-1, learning_rate='optimal')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_dev_pred = clf.predict(X_dev)\n",
    "print(classification_report(y_dev, y_dev_pred))\n",
    "f1_score(y_dev, y_dev_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.60      0.62      3278\n",
      "          1       0.65      0.68      0.66      3329\n",
      "          2       0.61      0.61      0.61      3235\n",
      "\n",
      "avg / total       0.63      0.63      0.63      9842\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6312161834768871"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm\n",
    "clf = lightgbm.LGBMClassifier(boosting_type='gbdt', objective='multiclass', n_estimators=300, reg_lambda=0.1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_dev_pred = clf.predict(X_dev)\n",
    "print(classification_report(y_dev, y_dev_pred))\n",
    "f1_score(y_dev, y_dev_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tr['t'].apply(lambda doc: ['{}_{}'.format(x.lemma_, x.pos_) for x in doc])\n",
    "h = tr['h'].apply(lambda doc: ['{}_{}'.format(x.lemma_, x.pos_) for x in doc]) \n",
    "sentences = t.tolist() + h.tolist()\n",
    "\n",
    "model = gensim.models.fasttext.FastText(sentences, size=300, iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel/__main__.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = MeanEmbeddingVectorizer(w2v, 300)\n",
    "\n",
    "X_train_t = vect.transform(tr['t'].apply(lambda doc: ['{}_{}'.format(x.lemma_, x.pos_) for x in doc]).values)\n",
    "X_train_h = vect.transform(tr['h'].apply(lambda doc: ['{}_{}'.format(x.lemma_, x.pos_) for x in doc]).values)\n",
    "\n",
    "X_dev_t = vect.transform(dev['t'].apply(lambda doc: ['{}_{}'.format(x.lemma_, x.pos_) for x in doc]).values)\n",
    "X_dev_h = vect.transform(dev['h'].apply(lambda doc: ['{}_{}'.format(x.lemma_, x.pos_) for x in doc]).values)\n",
    "\n",
    "X_train = np.hstack((X_train_t, X_train_h))\n",
    "X_dev = np.hstack((X_dev_t, X_dev_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.48      0.50      3278\n",
      "          1       0.58      0.41      0.48      3329\n",
      "          2       0.49      0.68      0.57      3235\n",
      "\n",
      "avg / total       0.53      0.52      0.52      9842\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5167023914799794"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SGDClassifier(loss='log', penalty='elasticnet', n_jobs=-1, learning_rate='optimal')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_dev_pred = clf.predict(X_dev)\n",
    "print(classification_report(y_dev, y_dev_pred))\n",
    "f1_score(y_dev, y_dev_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.58      0.60      3278\n",
      "          1       0.63      0.65      0.64      3329\n",
      "          2       0.60      0.61      0.61      3235\n",
      "\n",
      "avg / total       0.62      0.62      0.62      9842\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6152842755787167"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm\n",
    "clf = lightgbm.LGBMClassifier(boosting_type='gbdt', objective='multiclass', n_estimators=300, reg_lambda=0.1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_dev_pred = clf.predict(X_dev)\n",
    "print(classification_report(y_dev, y_dev_pred))\n",
    "f1_score(y_dev, y_dev_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
