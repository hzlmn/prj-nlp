{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim \n",
    "    \n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from conllu import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/Users/victor/Project/data/UD_Ukrainian-IU'\n",
    "train_path = os.path.join(dataset_path, 'uk_iu-ud-train.conllu')\n",
    "dev_path = os.path.join(dataset_path, 'uk_iu-ud-dev.conllu')\n",
    "test_path = os.path.join(dataset_path, 'uk_iu-ud-test.conllu')\n",
    "\n",
    "with open(train_path, 'r') as f: \n",
    "    content = f.read()\n",
    "    train = parse(content)\n",
    "\n",
    "with open(dev_path, 'r') as f: \n",
    "    content = f.read()\n",
    "    dev = parse(content)\n",
    "\n",
    "with open(test_path, 'r') as f: \n",
    "    content = f.read()\n",
    "    test = parse(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_record(data): \n",
    "    records = []\n",
    "    for sentence in data: \n",
    "        references = {rec['id']: rec for rec in sentence}\n",
    "        for word in sentence: \n",
    "            head_id = word['head']\n",
    "            if head_id == 0:\n",
    "                head = 'root'\n",
    "            else:\n",
    "                head = references[word['head']]['lemma']\n",
    "            records.append({'child': word['lemma'].lower(), \n",
    "                            'head': head.lower(),\n",
    "                            'y': word['deprel'].split(':')[0]})\n",
    "    return records\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(to_record(train))\n",
    "dev_df = pd.DataFrame(to_record(dev))\n",
    "test_df = pd.DataFrame(to_record(test))\n",
    "\n",
    "df = pd.concat([train_df, dev_df, test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "oe = OneHotEncoder()\n",
    "\n",
    "y = le.fit_transform(df['y'].values).reshape(-1, 1)\n",
    "oe.fit(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(pd.concat([train_df, dev_df, test_df])['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('/Users/victor/Downloads/fiction.lowercased.tokenized.word2vec.300d', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel/__main__.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  from ipykernel import kernelapp as app\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel/__main__.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    }
   ],
   "source": [
    "vocab = dict()\n",
    "for i, w in enumerate(w2v_model.wv.index2entity):\n",
    "    vocab[w] = i + 1\n",
    "\n",
    "vocabulary_size = len(w2v_model.wv.index2entity)\n",
    "embedding_size = 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocabulary_size + 1, embedding_size))\n",
    "\n",
    "for k, v in vocab.items(): \n",
    "    embedding_matrix[v] = w2v_model[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2index(df): \n",
    "    X_train_child = []\n",
    "    X_train_head = []\n",
    "    for _, row in df.iterrows(): \n",
    "        child_idx = vocab.get(row['child'], None)\n",
    "        child_idx = child_idx if child_idx else 0\n",
    "        head_idx = vocab.get(row['head'], None)\n",
    "        head_idx = head_idx if head_idx else 0\n",
    "        \n",
    "        X_train_child.append(child_idx)\n",
    "        X_train_head.append(head_idx)\n",
    "    return np.expand_dims(np.array(X_train_child), axis=1), np.expand_dims(np.array(X_train_head), axis=1)\n",
    "\n",
    "X_train_child, X_train_head = word2index(train_df)\n",
    "X_dev_child, X_dev_head = word2index(dev_df)\n",
    "X_test_child, X_test_head = word2index(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = le.transform(train_df['y']).reshape(-1, 1)\n",
    "y_train = oe.transform(y_train).toarray()\n",
    "\n",
    "y_dev = le.transform(dev_df['y']).reshape(-1, 1)\n",
    "y_dev = oe.transform(y_dev).toarray()\n",
    "\n",
    "y_test = le.transform(test_df['y']).reshape(-1, 1)\n",
    "y_test = oe.transform(y_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev, epoch: 0, step: 100, loss: 3.4550044536590576, accuracy: 0.1279529482126236\n",
      "Dev, epoch: 0, step: 200, loss: 3.4549901485443115, accuracy: 0.1279529482126236\n",
      "Dev, epoch: 0, step: 300, loss: 3.4549901485443115, accuracy: 0.1279529482126236\n",
      "Dev, epoch: 0, step: 400, loss: 3.4549901485443115, accuracy: 0.1279529482126236\n",
      "Dev, epoch: 0, step: 500, loss: 3.4549901485443115, accuracy: 0.1279529482126236\n",
      "Dev, epoch: 1, step: 600, loss: 2.884950637817383, accuracy: 0.3638993203639984\n",
      "Dev, epoch: 1, step: 700, loss: 2.7456514835357666, accuracy: 0.38704079389572144\n",
      "Dev, epoch: 1, step: 800, loss: 2.745649814605713, accuracy: 0.38704079389572144\n",
      "Dev, epoch: 1, step: 900, loss: 2.745649814605713, accuracy: 0.38704079389572144\n",
      "Dev, epoch: 1, step: 1000, loss: 2.745649814605713, accuracy: 0.38704079389572144\n",
      "Dev, epoch: 1, step: 1100, loss: 2.745649814605713, accuracy: 0.38704079389572144\n",
      "Dev, epoch: 2, step: 1200, loss: 2.496044874191284, accuracy: 0.44016969203948975\n",
      "Dev, epoch: 2, step: 1300, loss: 2.483358860015869, accuracy: 0.4420981705188751\n",
      "Dev, epoch: 2, step: 1400, loss: 2.483358860015869, accuracy: 0.4420981705188751\n",
      "Dev, epoch: 2, step: 1500, loss: 2.483358860015869, accuracy: 0.4420981705188751\n",
      "Dev, epoch: 2, step: 1600, loss: 2.483358860015869, accuracy: 0.4420981705188751\n",
      "Dev, epoch: 2, step: 1700, loss: 2.483358860015869, accuracy: 0.4420981705188751\n",
      "Dev, epoch: 3, step: 1800, loss: 2.3338558673858643, accuracy: 0.45887571573257446\n",
      "Dev, epoch: 3, step: 1900, loss: 2.332252264022827, accuracy: 0.4586828649044037\n",
      "Dev, epoch: 3, step: 2000, loss: 2.332252264022827, accuracy: 0.4586828649044037\n",
      "Dev, epoch: 3, step: 2100, loss: 2.332252264022827, accuracy: 0.4586828649044037\n",
      "Dev, epoch: 3, step: 2200, loss: 2.332252264022827, accuracy: 0.4586828649044037\n",
      "Dev, epoch: 3, step: 2300, loss: 2.332252264022827, accuracy: 0.4586828649044037\n",
      "Dev, epoch: 4, step: 2400, loss: 2.2383248805999756, accuracy: 0.4759425222873688\n",
      "Dev, epoch: 4, step: 2500, loss: 2.238081693649292, accuracy: 0.4759425222873688\n",
      "Dev, epoch: 4, step: 2600, loss: 2.238081693649292, accuracy: 0.4759425222873688\n",
      "Dev, epoch: 4, step: 2700, loss: 2.238081693649292, accuracy: 0.4759425222873688\n",
      "Dev, epoch: 4, step: 2800, loss: 2.238081693649292, accuracy: 0.4759425222873688\n",
      "Dev, epoch: 4, step: 2900, loss: 2.238081693649292, accuracy: 0.4759425222873688\n",
      "Dev, epoch: 5, step: 3000, loss: 2.1716666221618652, accuracy: 0.4925272464752197\n",
      "Dev, epoch: 5, step: 3100, loss: 2.1716370582580566, accuracy: 0.4925272464752197\n",
      "Dev, epoch: 5, step: 3200, loss: 2.1716370582580566, accuracy: 0.4925272464752197\n",
      "Dev, epoch: 5, step: 3300, loss: 2.1716370582580566, accuracy: 0.4925272464752197\n",
      "Dev, epoch: 5, step: 3400, loss: 2.1716370582580566, accuracy: 0.4925272464752197\n",
      "Dev, epoch: 5, step: 3500, loss: 2.1716370582580566, accuracy: 0.4925272464752197\n",
      "Dev, epoch: 6, step: 3600, loss: 2.1309144496917725, accuracy: 0.49532350897789\n",
      "Dev, epoch: 6, step: 3700, loss: 2.1309127807617188, accuracy: 0.49532350897789\n",
      "Dev, epoch: 6, step: 3800, loss: 2.1309127807617188, accuracy: 0.49532350897789\n",
      "Dev, epoch: 6, step: 3900, loss: 2.1309127807617188, accuracy: 0.49532350897789\n",
      "Dev, epoch: 6, step: 4000, loss: 2.1309127807617188, accuracy: 0.49532350897789\n",
      "Dev, epoch: 6, step: 4100, loss: 2.1309127807617188, accuracy: 0.49532350897789\n",
      "Dev, epoch: 7, step: 4200, loss: 2.123643636703491, accuracy: 0.4908880591392517\n",
      "Dev, epoch: 7, step: 4300, loss: 2.1236443519592285, accuracy: 0.4908880591392517\n",
      "Dev, epoch: 7, step: 4400, loss: 2.1236443519592285, accuracy: 0.4908880591392517\n",
      "Dev, epoch: 7, step: 4500, loss: 2.1236443519592285, accuracy: 0.4908880591392517\n",
      "Dev, epoch: 7, step: 4600, loss: 2.1236443519592285, accuracy: 0.4908880591392517\n",
      "Dev, epoch: 8, step: 4700, loss: 2.0842134952545166, accuracy: 0.49869829416275024\n",
      "Dev, epoch: 8, step: 4800, loss: 2.0749003887176514, accuracy: 0.5013017058372498\n",
      "Dev, epoch: 8, step: 4900, loss: 2.0749003887176514, accuracy: 0.5013017058372498\n",
      "Dev, epoch: 8, step: 5000, loss: 2.0749003887176514, accuracy: 0.5013017058372498\n",
      "Dev, epoch: 8, step: 5100, loss: 2.0749003887176514, accuracy: 0.5013017058372498\n",
      "Dev, epoch: 8, step: 5200, loss: 2.0749003887176514, accuracy: 0.5013017058372498\n",
      "Dev, epoch: 9, step: 5300, loss: 2.0840234756469727, accuracy: 0.4946485459804535\n",
      "Dev, epoch: 9, step: 5400, loss: 2.0881810188293457, accuracy: 0.49590203166007996\n",
      "Dev, epoch: 9, step: 5500, loss: 2.0881810188293457, accuracy: 0.49590203166007996\n",
      "Dev, epoch: 9, step: 5600, loss: 2.0881810188293457, accuracy: 0.49590203166007996\n",
      "Dev, epoch: 9, step: 5700, loss: 2.0881810188293457, accuracy: 0.49590203166007996\n",
      "Dev, epoch: 9, step: 5800, loss: 2.0881810188293457, accuracy: 0.49590203166007996\n",
      "Dev, epoch: 10, step: 5900, loss: 2.0488858222961426, accuracy: 0.5077620148658752\n",
      "Dev, epoch: 10, step: 6000, loss: 2.0494420528411865, accuracy: 0.5078584551811218\n",
      "Dev, epoch: 10, step: 6100, loss: 2.0494420528411865, accuracy: 0.5078584551811218\n",
      "Dev, epoch: 10, step: 6200, loss: 2.0494420528411865, accuracy: 0.5078584551811218\n",
      "Dev, epoch: 10, step: 6300, loss: 2.0494420528411865, accuracy: 0.5078584551811218\n",
      "Dev, epoch: 10, step: 6400, loss: 2.0494420528411865, accuracy: 0.5078584551811218\n",
      "Dev, epoch: 11, step: 6500, loss: 2.095529794692993, accuracy: 0.5065085291862488\n",
      "Dev, epoch: 11, step: 6600, loss: 2.0961837768554688, accuracy: 0.5064120888710022\n",
      "Dev, epoch: 11, step: 6700, loss: 2.0961837768554688, accuracy: 0.5064120888710022\n",
      "Dev, epoch: 11, step: 6800, loss: 2.0961837768554688, accuracy: 0.5064120888710022\n",
      "Dev, epoch: 11, step: 6900, loss: 2.0961837768554688, accuracy: 0.5064120888710022\n",
      "Dev, epoch: 11, step: 7000, loss: 2.0961837768554688, accuracy: 0.5064120888710022\n",
      "Dev, epoch: 12, step: 7100, loss: 2.0547759532928467, accuracy: 0.5120046138763428\n",
      "Dev, epoch: 12, step: 7200, loss: 2.054877758026123, accuracy: 0.511908233165741\n",
      "Dev, epoch: 12, step: 7300, loss: 2.054877758026123, accuracy: 0.511908233165741\n",
      "Dev, epoch: 12, step: 7400, loss: 2.054877758026123, accuracy: 0.511908233165741\n",
      "Dev, epoch: 12, step: 7500, loss: 2.054877758026123, accuracy: 0.511908233165741\n",
      "Dev, epoch: 12, step: 7600, loss: 2.054877758026123, accuracy: 0.511908233165741\n",
      "Dev, epoch: 13, step: 7700, loss: 2.118839979171753, accuracy: 0.5090155005455017\n",
      "Dev, epoch: 13, step: 7800, loss: 2.118884801864624, accuracy: 0.5090155005455017\n",
      "Dev, epoch: 13, step: 7900, loss: 2.118884801864624, accuracy: 0.5090155005455017\n",
      "Dev, epoch: 13, step: 8000, loss: 2.118884801864624, accuracy: 0.5090155005455017\n",
      "Dev, epoch: 13, step: 8100, loss: 2.118884801864624, accuracy: 0.5090155005455017\n",
      "Dev, epoch: 13, step: 8200, loss: 2.118884801864624, accuracy: 0.5090155005455017\n",
      "Dev, epoch: 14, step: 8300, loss: 2.078930139541626, accuracy: 0.5089191198348999\n",
      "Dev, epoch: 14, step: 8400, loss: 2.0789339542388916, accuracy: 0.5089191198348999\n",
      "Dev, epoch: 14, step: 8500, loss: 2.0789339542388916, accuracy: 0.5089191198348999\n",
      "Dev, epoch: 14, step: 8600, loss: 2.0789339542388916, accuracy: 0.5089191198348999\n",
      "Dev, epoch: 14, step: 8700, loss: 2.0789339542388916, accuracy: 0.5089191198348999\n",
      "Dev, epoch: 15, step: 8800, loss: 2.0700502395629883, accuracy: 0.5152829885482788\n",
      "Dev, epoch: 15, step: 8900, loss: 2.11165189743042, accuracy: 0.5173078775405884\n",
      "Dev, epoch: 15, step: 9000, loss: 2.1116528511047363, accuracy: 0.5173078775405884\n",
      "Dev, epoch: 15, step: 9100, loss: 2.1116528511047363, accuracy: 0.5173078775405884\n",
      "Dev, epoch: 15, step: 9200, loss: 2.1116528511047363, accuracy: 0.5173078775405884\n",
      "Dev, epoch: 15, step: 9300, loss: 2.1116528511047363, accuracy: 0.5173078775405884\n",
      "Dev, epoch: 16, step: 9400, loss: 2.078314781188965, accuracy: 0.5153794288635254\n",
      "Dev, epoch: 16, step: 9500, loss: 2.082913637161255, accuracy: 0.5130652785301208\n",
      "Dev, epoch: 16, step: 9600, loss: 2.082913637161255, accuracy: 0.5130652785301208\n",
      "Dev, epoch: 16, step: 9700, loss: 2.082913637161255, accuracy: 0.5130652785301208\n",
      "Dev, epoch: 16, step: 9800, loss: 2.082913637161255, accuracy: 0.5130652785301208\n",
      "Dev, epoch: 16, step: 9900, loss: 2.082913637161255, accuracy: 0.5130652785301208\n",
      "Dev, epoch: 17, step: 10000, loss: 2.151081085205078, accuracy: 0.5085334181785583\n",
      "Dev, epoch: 17, step: 10100, loss: 2.1546008586883545, accuracy: 0.5083405375480652\n",
      "Dev, epoch: 17, step: 10200, loss: 2.1546008586883545, accuracy: 0.5083405375480652\n",
      "Dev, epoch: 17, step: 10300, loss: 2.1546008586883545, accuracy: 0.5083405375480652\n",
      "Dev, epoch: 17, step: 10400, loss: 2.1546008586883545, accuracy: 0.5083405375480652\n",
      "Dev, epoch: 17, step: 10500, loss: 2.1546008586883545, accuracy: 0.5083405375480652\n",
      "Dev, epoch: 18, step: 10600, loss: 2.1220076084136963, accuracy: 0.5121974945068359\n",
      "Dev, epoch: 18, step: 10700, loss: 2.122286796569824, accuracy: 0.5121010541915894\n",
      "Dev, epoch: 18, step: 10800, loss: 2.122286796569824, accuracy: 0.5121010541915894\n",
      "Dev, epoch: 18, step: 10900, loss: 2.122286796569824, accuracy: 0.5121010541915894\n",
      "Dev, epoch: 18, step: 11000, loss: 2.122286796569824, accuracy: 0.5121010541915894\n",
      "Dev, epoch: 18, step: 11100, loss: 2.122286796569824, accuracy: 0.5121010541915894\n",
      "Dev, epoch: 19, step: 11200, loss: 2.121661901473999, accuracy: 0.5130652785301208\n",
      "Dev, epoch: 19, step: 11300, loss: 2.1217310428619385, accuracy: 0.5129688382148743\n",
      "Dev, epoch: 19, step: 11400, loss: 2.1217310428619385, accuracy: 0.5129688382148743\n",
      "Dev, epoch: 19, step: 11500, loss: 2.1217310428619385, accuracy: 0.5129688382148743\n",
      "Dev, epoch: 19, step: 11600, loss: 2.1217310428619385, accuracy: 0.5129688382148743\n",
      "Dev, epoch: 19, step: 11700, loss: 2.1217310428619385, accuracy: 0.5129688382148743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev F1 score:  0.23858094607202268 Accuracy:  0.51296884\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.11      0.19       184\n",
      "          1       0.12      0.05      0.07       119\n",
      "          2       0.66      0.61      0.64       484\n",
      "          3       0.41      0.63      0.50       846\n",
      "          4       0.00      0.00      0.00        71\n",
      "          5       1.00      0.11      0.19        19\n",
      "          6       0.99      0.94      0.97       945\n",
      "          7       0.89      0.78      0.83       358\n",
      "          8       0.00      0.00      0.00        50\n",
      "          9       0.00      0.00      0.00        64\n",
      "         10       0.06      0.03      0.04       475\n",
      "         11       0.66      0.75      0.70        57\n",
      "         12       0.00      0.00      0.00        52\n",
      "         13       0.00      0.00      0.00         0\n",
      "         14       0.71      0.88      0.79       240\n",
      "         15       0.47      0.49      0.48       139\n",
      "         16       0.00      0.00      0.00         5\n",
      "         17       0.00      0.00      0.00         7\n",
      "         18       0.00      0.00      0.00        26\n",
      "         19       0.55      0.08      0.14       223\n",
      "         20       0.00      0.00      0.00         0\n",
      "         21       0.00      0.00      0.00        10\n",
      "         22       0.00      0.00      0.00         0\n",
      "         23       0.69      0.75      0.71       205\n",
      "         24       0.37      0.43      0.39      1044\n",
      "         25       0.34      0.15      0.21       658\n",
      "         26       0.00      0.00      0.00        87\n",
      "         27       0.24      0.27      0.26       488\n",
      "         28       0.35      0.21      0.27       709\n",
      "         29       0.00      0.00      0.00         2\n",
      "         30       0.07      0.13      0.09       114\n",
      "         31       0.52      0.88      0.66      1987\n",
      "         32       0.00      0.00      0.00         0\n",
      "         33       0.62      0.34      0.44       577\n",
      "         34       0.00      0.00      0.00         0\n",
      "         35       0.67      0.02      0.03       126\n",
      "\n",
      "avg / total       0.49      0.51      0.47     10371\n",
      "\n",
      "Test F1 score:  0.25421302952277475 Accuracy:  0.5256041\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.12      0.20       176\n",
      "          1       0.06      0.03      0.04       149\n",
      "          2       0.70      0.59      0.64       644\n",
      "          3       0.41      0.56      0.47      1459\n",
      "          4       0.00      0.00      0.00       105\n",
      "          5       1.00      0.22      0.36        27\n",
      "          6       0.99      0.95      0.97      1373\n",
      "          7       0.90      0.79      0.84       554\n",
      "          8       0.00      0.00      0.00        83\n",
      "          9       0.00      0.00      0.00        81\n",
      "         10       0.08      0.03      0.04       787\n",
      "         11       0.63      0.93      0.75        80\n",
      "         12       0.00      0.00      0.00        47\n",
      "         13       0.00      0.00      0.00         1\n",
      "         14       0.83      0.85      0.84       446\n",
      "         15       0.46      0.44      0.45       183\n",
      "         16       0.00      0.00      0.00         0\n",
      "         17       0.00      0.00      0.00        13\n",
      "         18       1.00      0.03      0.06        31\n",
      "         19       0.54      0.14      0.22       367\n",
      "         20       0.00      0.00      0.00         2\n",
      "         21       0.00      0.00      0.00        36\n",
      "         22       0.00      0.00      0.00         3\n",
      "         23       0.66      0.74      0.70       233\n",
      "         24       0.44      0.51      0.47      1702\n",
      "         25       0.34      0.18      0.23       796\n",
      "         26       1.00      0.02      0.03       117\n",
      "         27       0.25      0.27      0.26       627\n",
      "         28       0.40      0.26      0.31      1025\n",
      "         29       0.00      0.00      0.00         6\n",
      "         30       0.11      0.16      0.13       191\n",
      "         31       0.50      0.87      0.64      2677\n",
      "         32       0.00      0.00      0.00         0\n",
      "         33       0.63      0.39      0.48       783\n",
      "         34       0.00      0.00      0.00         5\n",
      "         35       0.00      0.00      0.00       130\n",
      "\n",
      "avg / total       0.51      0.53      0.49     14939\n",
      "\n"
     ]
    }
   ],
   "source": [
    "l2_reg_lambda = 0\n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    input_child = tf.placeholder(tf.int32, shape=[None, 1], name='input_child')\n",
    "    input_head = tf.placeholder(tf.int32, shape=[None, 1], name='input_head')\n",
    "    input_y = tf.placeholder(tf.float32, shape=[None, num_classes], name='input_y')\n",
    "    dropout_keep_prob = tf.placeholder(tf.float32, name='dropout_keep_prob')\n",
    "\n",
    "    l2_loss = tf.constant(0.0)\n",
    "    \n",
    "    with tf.name_scope('Embedding'):\n",
    "        W = tf.get_variable('word_embeddings', shape=[vocabulary_size + 1, embedding_size], \n",
    "                            initializer=tf.constant_initializer(embedding_matrix), \n",
    "                            trainable=False)\n",
    "        child_embedded_chars = tf.nn.embedding_lookup(W, input_child)\n",
    "        child_embedded_chars_reshaped = tf.reshape(child_embedded_chars, shape=[-1, 300])\n",
    "\n",
    "        head_embedded_chars = tf.nn.embedding_lookup(W, input_head)\n",
    "        head_embedded_chars_reshaped = tf.reshape(head_embedded_chars, shape=[-1, 300])\n",
    "\n",
    "        embeddings = tf.concat([child_embedded_chars_reshaped, head_embedded_chars_reshaped], \n",
    "                               axis=1)\n",
    "    \n",
    "    with tf.name_scope('Hidden1'): \n",
    "        hidden_1 = tf.layers.dense(embeddings, units=1024, activation=tf.nn.relu, \n",
    "                                   kernel_initializer=tf.contrib.layers.variance_scaling_initializer(mode=\"FAN_AVG\"), \n",
    "                                   name='hidden1')\n",
    "        hidden_1 = tf.nn.dropout(hidden_1, dropout_keep_prob)\n",
    "        \n",
    "    with tf.name_scope('Output'): \n",
    "        W = tf.get_variable('W', shape=[1024, num_classes], \n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.get_variable('b', shape=[num_classes], \n",
    "                            initializer=tf.zeros_initializer())\n",
    "        l2_loss += tf.nn.l2_loss(W)\n",
    "        l2_loss += tf.nn.l2_loss(b)\n",
    "        scores = tf.nn.xw_plus_b(hidden_1, W, b, name='scores')\n",
    "        predictions = tf.argmax(scores, axis=1, name='predictionss')\n",
    "        y_pred = tf.one_hot(predictions, depth=num_classes)\n",
    "    \n",
    "    with tf.name_scope('loss'): \n",
    "        losses = tf.nn.softmax_cross_entropy_with_logits(logits=scores, labels=input_y)\n",
    "        loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "        \n",
    "    with tf.name_scope('accuracy'): \n",
    "        correct_predictions = tf.equal(predictions, tf.argmax(input_y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_predictions, 'float'), name='accuracy')\n",
    "        \n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "    grad_and_vars = optimizer.compute_gradients(loss)\n",
    "    train_op = optimizer.apply_gradients(grad_and_vars, global_step=global_step)\n",
    "\n",
    "    \n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=False, log_device_placement=True)\n",
    "    with tf.Session(config=session_conf) as session: \n",
    "        session.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch in range(num_epochs): \n",
    "            num_batches = int(X_train_head.shape[0] / batch_size)\n",
    "            shuffle_indices = np.random.permutation(num_batches)\n",
    "            current_index = 0\n",
    "            for batch in range(num_batches): \n",
    "                idx = shuffle_indices[current_index:current_index + batch_size]\n",
    "                feed_dict = {\n",
    "                    input_child: X_train_child[idx], \n",
    "                    input_head: X_train_head[idx],\n",
    "                    input_y: y_train[idx],\n",
    "                    dropout_keep_prob: 0.3\n",
    "                }\n",
    "                _, step, l, a = session.run([train_op, global_step, loss, accuracy], feed_dict=feed_dict)\n",
    "\n",
    "                current_step = tf.train.global_step(session, global_step)\n",
    "                if current_step % 100 == 0 and current_step != 0:\n",
    "                    feed_dict = {\n",
    "                        input_child: X_dev_child,\n",
    "                        input_head: X_dev_head,\n",
    "                        input_y: y_dev,\n",
    "                        dropout_keep_prob: 1\n",
    "                    }\n",
    "                    step, l, a = session.run([global_step, loss, accuracy], feed_dict=feed_dict)\n",
    "                    print('Dev, epoch: {}, step: {}, loss: {}, accuracy: {}'.format(epoch, step, l, a))\n",
    "                    \n",
    "                current_index += batch_size\n",
    "        \n",
    "        feed_dict = {\n",
    "            input_child: X_dev_child,\n",
    "            input_head: X_dev_head,\n",
    "            input_y: y_dev, \n",
    "            dropout_keep_prob: 1\n",
    "        }\n",
    "        y_p, a = session.run([y_pred, accuracy], feed_dict=feed_dict)\n",
    "        print('Dev F1 score: ', f1_score(y_dev, y_p, average='macro'), 'Accuracy: ', a)\n",
    "        print(classification_report(y_dev, y_p))\n",
    "\n",
    "        feed_dict = {\n",
    "            input_child: X_test_child,\n",
    "            input_head: X_test_head,\n",
    "            input_y: y_test, \n",
    "            dropout_keep_prob: 1\n",
    "        }\n",
    "        y_p, a = session.run([y_pred, accuracy], feed_dict=feed_dict)\n",
    "        print('Test F1 score: ', f1_score(y_test, y_p, average='macro'), 'Accuracy: ', a)\n",
    "        print(classification_report(y_test, y_p))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
