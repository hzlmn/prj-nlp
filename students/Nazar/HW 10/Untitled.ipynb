{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv(\"snli_1.0/snli_1.0_train.txt\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"snli_1.0/snli_1.0_test.txt\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['gold_label','sentence1_binary_parse','sentence2_binary_parse','sentence1_parse','sentence2_parse','sentence1','sentence2','captionID','pairID','label1','label2','label3','label4','label5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns=columns\n",
    "test.columns=columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "contradiction    183384\n",
       "entailment       183384\n",
       "neutral          183384\n",
       "Name: label1, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['label1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral          3334\n",
       "entailment       3333\n",
       "contradiction    3333\n",
       "Name: label1, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['label1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(550152, 14)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gold_label</th>\n",
       "      <th>sentence1_binary_parse</th>\n",
       "      <th>sentence2_binary_parse</th>\n",
       "      <th>sentence1_parse</th>\n",
       "      <th>sentence2_parse</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>captionID</th>\n",
       "      <th>pairID</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "      <th>label3</th>\n",
       "      <th>label4</th>\n",
       "      <th>label5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>contradiction</td>\n",
       "      <td>( ( A boy ) ( ( is ( ( jumping ( on skateboard...</td>\n",
       "      <td>( ( The boy ) ( ( ( skates down ) ( the sidewa...</td>\n",
       "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...</td>\n",
       "      <td>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ skate...</td>\n",
       "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
       "      <td>The boy skates down the sidewalk.</td>\n",
       "      <td>3691670743.jpg#0</td>\n",
       "      <td>3691670743.jpg#0r1c</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>entailment</td>\n",
       "      <td>( ( A boy ) ( ( is ( ( jumping ( on skateboard...</td>\n",
       "      <td>( ( The boy ) ( ( does ( a ( skateboarding tri...</td>\n",
       "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...</td>\n",
       "      <td>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ does)...</td>\n",
       "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
       "      <td>The boy does a skateboarding trick.</td>\n",
       "      <td>3691670743.jpg#0</td>\n",
       "      <td>3691670743.jpg#0r1e</td>\n",
       "      <td>entailment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>neutral</td>\n",
       "      <td>( ( A boy ) ( ( is ( ( jumping ( on skateboard...</td>\n",
       "      <td>( ( The boy ) ( ( is ( wearing ( safety equipm...</td>\n",
       "      <td>(ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...</td>\n",
       "      <td>(ROOT (S (NP (DT The) (NN boy)) (VP (VBZ is) (...</td>\n",
       "      <td>A boy is jumping on skateboard in the middle o...</td>\n",
       "      <td>The boy is wearing safety equipment.</td>\n",
       "      <td>3691670743.jpg#0</td>\n",
       "      <td>3691670743.jpg#0r1n</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      gold_label                             sentence1_binary_parse  \\\n",
       "6  contradiction  ( ( A boy ) ( ( is ( ( jumping ( on skateboard...   \n",
       "7     entailment  ( ( A boy ) ( ( is ( ( jumping ( on skateboard...   \n",
       "8        neutral  ( ( A boy ) ( ( is ( ( jumping ( on skateboard...   \n",
       "\n",
       "                              sentence2_binary_parse  \\\n",
       "6  ( ( The boy ) ( ( ( skates down ) ( the sidewa...   \n",
       "7  ( ( The boy ) ( ( does ( a ( skateboarding tri...   \n",
       "8  ( ( The boy ) ( ( is ( wearing ( safety equipm...   \n",
       "\n",
       "                                     sentence1_parse  \\\n",
       "6  (ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...   \n",
       "7  (ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...   \n",
       "8  (ROOT (S (NP (DT A) (NN boy)) (VP (VBZ is) (VP...   \n",
       "\n",
       "                                     sentence2_parse  \\\n",
       "6  (ROOT (S (NP (DT The) (NN boy)) (VP (VBZ skate...   \n",
       "7  (ROOT (S (NP (DT The) (NN boy)) (VP (VBZ does)...   \n",
       "8  (ROOT (S (NP (DT The) (NN boy)) (VP (VBZ is) (...   \n",
       "\n",
       "                                           sentence1  \\\n",
       "6  A boy is jumping on skateboard in the middle o...   \n",
       "7  A boy is jumping on skateboard in the middle o...   \n",
       "8  A boy is jumping on skateboard in the middle o...   \n",
       "\n",
       "                              sentence2         captionID  \\\n",
       "6     The boy skates down the sidewalk.  3691670743.jpg#0   \n",
       "7   The boy does a skateboarding trick.  3691670743.jpg#0   \n",
       "8  The boy is wearing safety equipment.  3691670743.jpg#0   \n",
       "\n",
       "                pairID         label1 label2 label3 label4 label5  \n",
       "6  3691670743.jpg#0r1c  contradiction    NaN    NaN    NaN    NaN  \n",
       "7  3691670743.jpg#0r1e     entailment    NaN    NaN    NaN    NaN  \n",
       "8  3691670743.jpg#0r1n        neutral    NaN    NaN    NaN    NaN  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[6:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A boy is jumping on skateboard in the middle of a red bridge.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['sentence1'].values[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The boy skates down the sidewalk.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['sentence2'].values[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The boy does a skateboarding trick.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['sentence2'].values[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The boy is wearing safety equipment.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['sentence2'].values[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ідеї\n",
    "\n",
    "\n",
    "1вчити абстакції наприклад  \n",
    "    broken down airplane = person is outdoors\n",
    "    on a horse =  jumps on a horse\n",
    "    \n",
    "2 дивитися по ворднету наскільки далеко ці слова\n",
    "\n",
    "3 зробити реплейс усіх займеників одним словом і іменників теж оскількі в цих реченнях мова иде про одне и теж\n",
    "\n",
    "4 Більшість речень має 2 стейтменти, можливо варто їх окремо розглядати і порівнювати окремо напрклад от \n",
    "    in the middle of a red bridge !=  down the sidewalk\n",
    "    \n",
    "    \n",
    "    умови будуть такі \n",
    "            якщо в 1му і другому речені є ця частина речення тоді порівнюємо\n",
    "            якщо нема вважаємо що ця частина збігається\n",
    "            \n",
    "    це можна також використати як окрему фічу\n",
    "    \n",
    "5 не використовувати голден трі - оскільки потім я зможу цю модель перевикористати\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sbj - verb - object spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk import Tree\n",
    "en_nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roots(nlp_doc):\n",
    "    root = None \n",
    "    for token in nlp_doc:\n",
    "        if token.dep_.lower() =='root':\n",
    "            root = token\n",
    "            break\n",
    "    conjs = [child for child in nlp_doc if child.dep_ in ['conj','advcl','acl','advmod','relcl']] \n",
    "    conjs = conjs+[child for child in nlp_doc if (child.dep_ in ['pobj']) and (child.pos_ in ['VERB'])] \n",
    "    conjs.insert(0,root)\n",
    "    return conjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_nouns(doc):\n",
    "    viewed_tokens_for_merge=[]\n",
    "    merge_ranges=[]\n",
    "    for token in doc:\n",
    "        if token in viewed_tokens_for_merge:\n",
    "            continue\n",
    "        tmp_compaund_token=token\n",
    "        \n",
    "        last_token=tmp_compaund_token\n",
    "        while((tmp_compaund_token.dep_ in ['compound'])):#'nummod'\n",
    "            viewed_tokens_for_merge.append(token)\n",
    "            if (tmp_compaund_token.head.pos_ in ['NOUN','PROPN']):\n",
    "                last_token=tmp_compaund_token.head\n",
    "            tmp_compaund_token = tmp_compaund_token.head\n",
    "            \n",
    "        if token!=tmp_compaund_token:\n",
    "            merge_ranges.append((token.idx, last_token.idx+last_token.__len__()))\n",
    "            \n",
    "    for merge_range in merge_ranges:\n",
    "        doc.merge(merge_range[0],merge_range[1])\n",
    "    return doc\n",
    "\n",
    "def merge_werbs(doc):\n",
    "    merge_ranges=[]\n",
    "    for token in doc:\n",
    "        if(token.head.tag_ in ['VERB','VBG']) and (token.dep_ in ['prt']):\n",
    "            start = token.idx\n",
    "            end = token.head.idx\n",
    "            if start<end:\n",
    "                merge_ranges.append((token.idx, token.head.idx+token.head.__len__()))\n",
    "            else:\n",
    "                merge_ranges.append((token.head.idx, token.idx+token.__len__()))\n",
    "    \n",
    "    for merge_range in merge_ranges:\n",
    "        doc.merge(merge_range[0],merge_range[1])\n",
    "    return doc\n",
    "\n",
    "def get_doc(text):\n",
    "    nlp_doc = en_nlp(text)\n",
    "    nlp_doc = merge_nouns(nlp_doc)\n",
    "    nlp_doc = merge_werbs(nlp_doc)  \n",
    "    return nlp_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_types={\n",
    "    'subj_dep':['nsubj','nsubjpass','compound'],\n",
    "    'vb_dep':['prt','pobj'],\n",
    "    \"obj_dep\":['dobj','prep','acomp']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_types_important_pos={\n",
    "    'subj_dep':['NOUN','ADJ','PROPN'],\n",
    "    'vb_dep':['VERB'],\n",
    "    \"obj_dep\":['NOUN','ADJ','PROPN']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An old man with a package poses in front of an advertisement.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(An, 'det', 'DET', man),\n",
       " (old, 'amod', 'ADJ', man),\n",
       " (man, 'ROOT', 'NOUN', man),\n",
       " (with, 'prep', 'ADP', man),\n",
       " (a, 'det', 'DET', poses),\n",
       " (package, 'compound', 'NOUN', poses),\n",
       " (poses, 'pobj', 'VERB', with),\n",
       " (in, 'prep', 'ADP', poses),\n",
       " (front, 'pobj', 'NOUN', in),\n",
       " (of, 'prep', 'ADP', front),\n",
       " (an, 'det', 'DET', advertisement),\n",
       " (advertisement, 'pobj', 'NOUN', of),\n",
       " (., 'punct', 'PUNCT', man)]"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'An old man with a package poses in front of an advertisement.'\n",
    "print(text)\n",
    "nlp_doc = get_doc(text)\n",
    "[(i, i.dep_, i.pos_,i.head) for i in nlp_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[man, poses]"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_doc = get_doc(text)\n",
    "roots = get_roots(nlp_doc)\n",
    "roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dep(token, parts,dep_type):\n",
    "    parts[dep_type].append(token)\n",
    "    for sub_tokens in token.children:\n",
    "        if sub_tokens.pos_ not in ['VERB']:\n",
    "            add_dep(sub_tokens,parts,dep_type)\n",
    "        else:\n",
    "            parts[dep_type].append(sub_tokens)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "def can_replace_subject(first,second):\n",
    "    l =  second.idx - first.idx - len(first.text)\n",
    "    if (l < 10):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parts(nlp_doc,roots):\n",
    "    all_parts=[]\n",
    "    for root in roots:\n",
    "        parts={}\n",
    "\n",
    "        for dep_type in dep_types:\n",
    "            parts[dep_type]=[]\n",
    "            for token in root.children:\n",
    "                if  token.dep_ in dep_types[dep_type]:\n",
    "                    add_dep(token, parts,dep_type)\n",
    "        \n",
    "        # if root was set as an Noun\n",
    "        if root.tag_ != 'NN':\n",
    "            parts['vb_dep'].insert(0,root)\n",
    "        else:\n",
    "            parts['subj_dep'].insert(0,root)\n",
    "        \n",
    "        #remove irrelevant tags  \n",
    "        all_parts.append(cleanup_parts(parts))\n",
    "        \n",
    "    # duplicate noun from first item if missed\n",
    "    for i in range(1,len(all_parts)):\n",
    "        \n",
    "        if len(all_parts[i]['subj_dep'])==0:\n",
    "            all_parts[i]['subj_dep']=all_parts[i-1]['subj_dep']\n",
    "        elif len(all_parts[i-1]['subj_dep'])>0 and can_replace_subject(all_parts[i-1]['subj_dep'][0],all_parts[i]['subj_dep'][0]):\n",
    "            # should optimized better \n",
    "            all_parts[i]['subj_dep']=all_parts[i-1]['subj_dep']\n",
    "        \n",
    "    #cleanup parts only with noun due to incorect work of pos tagger\n",
    "    result=[]\n",
    "    \n",
    "    for parts in all_parts:\n",
    "        if (((len(parts['vb_dep'])>0) and  (parts['vb_dep'][0].text!='is')) or (len(parts['obj_dep'])>0)):\n",
    "            result.append(parts)\n",
    "        \n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_parts(parts):\n",
    "    clean_part={}\n",
    "    for part_type in parts:\n",
    "        clean_part[part_type]=[]\n",
    "        for token in parts[part_type]:\n",
    "            if token.pos_ in dep_types_important_pos[part_type]:\n",
    "                clean_part[part_type].append(token)\n",
    "    return clean_part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get most common tags in each group(subj, vb, object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_tags = {\n",
    "    'subj_dep':[],\n",
    "    'vb_dep':[],\n",
    "    \"obj_dep\":[]\n",
    "}\n",
    "\n",
    "for text in test['sentence2'].values:\n",
    "    nlp_doc = get_doc(text)\n",
    "    roots = get_roots(nlp_doc)\n",
    "    for root in roots:\n",
    "        parts = get_parts(nlp_doc,root)\n",
    "        for part_type in parts:\n",
    "            most_frequent_tags[part_type]=most_frequent_tags[part_type]+[i.pos_ for i in parts[part_type]]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj_dep\n",
      "[('NOUN', 9789), ('DET', 6925), ('ADJ', 1246), ('NUM', 973), ('ADP', 735), ('CCONJ', 288), ('VERB', 240), ('PRON', 204), ('PROPN', 116), ('ADV', 46)]\n",
      "====================\n",
      "vb_dep\n",
      "[('VERB', 8936), ('NOUN', 1263), ('PART', 258), ('ADP', 27), ('ADJ', 23), ('DET', 14), ('ADV', 13), ('PROPN', 7), ('INTJ', 3), ('CCONJ', 2)]\n",
      "====================\n",
      "obj_dep\n",
      "[('NOUN', 9537), ('ADP', 5582), ('DET', 5452), ('ADJ', 2527), ('VERB', 625), ('PART', 229), ('PROPN', 219), ('ADV', 168), ('PRON', 149), ('CCONJ', 112)]\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "for part_type in most_frequent_tags:\n",
    "    cnt = Counter(most_frequent_tags[part_type])\n",
    "    print(part_type)\n",
    "    print(cnt.most_common(10))\n",
    "    print(\"====================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use only most frequet and important tags, fix root in those cases where root is NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a statue that not many people seem to be interested in.\n",
      "[{'subj_dep': [people, many], 'vb_dep': [seem], 'obj_dep': []}]\n",
      "======================================\n",
      "Tons of people are gathered around the statue.\n",
      "[{'subj_dep': [Tons, people], 'vb_dep': [gathered], 'obj_dep': [statue]}]\n",
      "======================================\n",
      "A Land Rover is splashing water as it crosses a river.\n",
      "[{'subj_dep': [Land Rover], 'vb_dep': [splashing], 'obj_dep': [water]}, {'subj_dep': [Land Rover], 'vb_dep': [crosses], 'obj_dep': [river]}]\n",
      "======================================\n",
      "A vehicle is crossing a river.\n",
      "[{'subj_dep': [vehicle], 'vb_dep': [crossing], 'obj_dep': [river]}]\n",
      "======================================\n",
      "A sedan is stuck in the middle of a river.\n",
      "[{'subj_dep': [sedan], 'vb_dep': [stuck], 'obj_dep': [middle, river]}]\n",
      "======================================\n",
      "A man playing banjo on the floor.\n",
      "[{'subj_dep': [man], 'vb_dep': [playing], 'obj_dep': [banjo, floor]}]\n",
      "======================================\n",
      "A man playing guitar on stage.\n",
      "[{'subj_dep': [man], 'vb_dep': [playing], 'obj_dep': [guitar, stage]}]\n",
      "======================================\n",
      "A man is performing for cash.\n",
      "[{'subj_dep': [man], 'vb_dep': [performing], 'obj_dep': [cash]}]\n",
      "======================================\n",
      "A doctor is looking at a book\n",
      "[{'subj_dep': [doctor], 'vb_dep': [looking], 'obj_dep': [book]}]\n",
      "======================================\n",
      "A man is eating pb and j\n",
      "[{'subj_dep': [man], 'vb_dep': [eating], 'obj_dep': [pb, j]}]\n",
      "======================================\n"
     ]
    }
   ],
   "source": [
    "for text in test['sentence2'].values[10:20]:\n",
    "    print(text)\n",
    "    nlp_doc = get_doc(text)\n",
    "    roots = get_roots(nlp_doc)\n",
    "    parts =get_parts(nlp_doc,roots)\n",
    "    print(parts)\n",
    "    print('======================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Errors: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-211-7a2101482491>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-211-7a2101482491>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    The woman has been shot.\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "The woman has been shot.\n",
    "{'subj_dep': [woman], 'vb_dep': [shot], 'obj_dep': []}\n",
    "======================================\n",
    "The statue is offensive and people are mad that it is on display.\n",
    "{'subj_dep': [statue], 'vb_dep': [is], 'obj_dep': [offensive]}\n",
    "======================================\n",
    "There is a statue that not many people seem to be interested in.\n",
    "{'subj_dep': [], 'vb_dep': [is], 'obj_dep': []}\n",
    "======================================\n",
    "A Land Rover is splashing water as it crosses a river.\n",
    "{'subj_dep': [], 'vb_dep': [splashing], 'obj_dep': [water]}\n",
    "======================================\n",
    "A man playing banjo on the floor.\n",
    "{'subj_dep': [], 'vb_dep': [], 'obj_dep': []}\n",
    "======================================\n",
    "The man is inside.\n",
    "{'subj_dep': [man], 'vb_dep': [is], 'obj_dep': []}\n",
    "======================================\n",
    "Three firefighters putting out a fire inside of a subway station.\n",
    "{'subj_dep': [], 'vb_dep': [], 'obj_dep': []}\n",
    "======================================\n",
    "Three firefighters coming up from a subway station.\n",
    "{'subj_dep': [], 'vb_dep': [], 'obj_dep': []}\n",
    "======================================\n",
    "Three firefighters playing cards inside a fire station.\n",
    "{'subj_dep': [], 'vb_dep': [], 'obj_dep': []}\n",
    "======================================\n",
    "The boy is playing on the swings after school.\n",
    "{'subj_dep': [boy], 'vb_dep': [playing], 'obj_dep': [swings, school]}\n",
    "======================================\n",
    "An archeologist wearing a hat squats to examine the site for a dig\n",
    "{'subj_dep': [], 'vb_dep': [], 'obj_dep': []}\n",
    "======================================\n",
    "A squatting woman wearing a hat touching the ground.\n",
    "{'subj_dep': [], 'vb_dep': [], 'obj_dep': []}\n",
    "======================================\n",
    "A woman wearing a sun bonnet planting a garden.\n",
    "{'subj_dep': [], 'vb_dep': [], 'obj_dep': []}\n",
    "======================================\n",
    "a guy near a building stands by two other men\n",
    "{'subj_dep': [guy, building], 'vb_dep': [stands], 'obj_dep': [men, other]}\n",
    "======================================\n",
    "a busy man stands with bodyguards\n",
    "{'subj_dep': [man, busy], 'vb_dep': [stands], 'obj_dep': [bodyguards]}\n",
    "======================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cool bro\n",
      "cool bro\n",
      "cool bro\n",
      "cool bro\n",
      "cool bro\n",
      "cool bro\n",
      "cool bro\n",
      "There is a statue that not many people seem to be interested in.\n",
      "['{subj_dep: [people, many], vb_dep: [seem], obj_dep: []}']\n",
      "['{subj_dep: [not, people, many], vb_dep: [interested], obj_dep: [statue]}']\n",
      "++++++++++\n"
     ]
    }
   ],
   "source": [
    "tests=[\n",
    "    ('The statue is offensive and people are mad that it is on display',\n",
    "     [\n",
    "        {'subj_dep': ['statue'], 'vb_dep': ['is'], 'obj_dep': ['offensive']},\n",
    "        {'subj_dep': ['people'], 'vb_dep': ['are'], 'obj_dep': ['mad']}\n",
    "     ]),\n",
    "    ('A Land Rover is splashing water as it crosses a river.',\n",
    "     [\n",
    "        {'subj_dep': ['Land Rover'], 'vb_dep': ['splashing'], 'obj_dep': ['water']},\n",
    "        {'subj_dep': ['Land Rover'], 'vb_dep': ['crosses'], 'obj_dep': ['river']}\n",
    "     ]),\n",
    "    ('A woman wearing a sun bonnet planting a garden',\n",
    "     [\n",
    "        {'subj_dep': ['woman'], 'vb_dep': ['planting'], 'obj_dep': ['garden']},\n",
    "        {'subj_dep': ['woman'], 'vb_dep': ['wearing'], 'obj_dep': ['sun bonnet']}\n",
    "     ]),\n",
    "    ('Three firefighters putting out a fire inside of a subway station',\n",
    "     [\n",
    "        {'subj_dep': ['firefighters'], 'vb_dep': ['putting out'], 'obj_dep': ['fire']},\n",
    "        {'subj_dep': ['firefighters'], 'vb_dep': [], 'obj_dep': ['subway station']}\n",
    "     ]),\n",
    "    ('An archeologist wearing a hat squats to examine the site for a dig',\n",
    "     [\n",
    "        {'subj_dep': ['archeologist'], 'vb_dep': ['wearing'], 'obj_dep': ['hat squats']},\n",
    "        {'subj_dep': ['archeologist'], 'vb_dep': ['examine'], 'obj_dep': ['site', 'dig']}\n",
    "     ]),\n",
    "    ('A man poses in front of an ad.',\n",
    "     [\n",
    "        {'subj_dep': ['man'], 'vb_dep': ['poses'], 'obj_dep': ['front', 'ad']}\n",
    "     ]),\n",
    "    ('An old man with a package poses in front of an advertisement.',\n",
    "     [\n",
    "        {'subj_dep': ['man'], 'vb_dep': ['poses'], 'obj_dep': ['front', 'advertisement']}\n",
    "     ]),\n",
    "    ('There is a statue that not many people seem to be interested in.',\n",
    "     [\n",
    "        {'subj_dep': ['not','people', 'many'], 'vb_dep': ['interested'], 'obj_dep': ['statue']}\n",
    "     ])\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def get_actual(text):\n",
    "    nlp_doc = get_doc(text)\n",
    "    result= []\n",
    "    for item in get_parts(nlp_doc,get_roots(nlp_doc)):\n",
    "        result.append(str(item).replace(\"'\",''))\n",
    "    return result\n",
    "\n",
    "def get_test(features):\n",
    "    result= []\n",
    "    for item in features:\n",
    "        result.append(str(item).replace(\"'\",''))\n",
    "    return result\n",
    "\n",
    "for test_text, features in tests:\n",
    "    actual = get_actual(test_text)\n",
    "    test_=get_test(features)\n",
    "    if(actual != test_):\n",
    "        print(test_text)\n",
    "        print(actual)\n",
    "        print(test_)\n",
    "        print('++++++++++')\n",
    "    else:\n",
    "        print(\"cool bro\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['many people seem', 'statue', 'interested']"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#мабуть потрібно перефразовувати,  поки не знаю як обробити\n",
    "#можливо застосувати page rank\n",
    "\n",
    "from rake_nltk import Rake\n",
    "r = Rake()\n",
    "r.extract_keywords_from_text('There is a statue that not many people seem to be interested in.')\n",
    "r.get_ranked_phrases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An old man with a package poses in front of an advertisement.\n",
      "['{subj_dep: [man], vb_dep: [poses], obj_dep: [front, advertisement]}']\n",
      "\n",
      "entailment\n",
      "A man poses in front of an ad.\n",
      "['{subj_dep: [man], vb_dep: [poses], obj_dep: [front, ad]}']\n",
      "\n",
      "neutral\n",
      "A man poses in front of an ad for beer.\n",
      "['{subj_dep: [man], vb_dep: [poses], obj_dep: [front, ad, beer]}']\n",
      "\n",
      "contradiction\n",
      "A man walks by an ad.\n",
      "['{subj_dep: [man], vb_dep: [walks], obj_dep: [ad]}']\n",
      "\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "test_idx=2\n",
    "print(test['sentence1'].values[test_idx*3])\n",
    "print(get_actual(test['sentence1'].values[test_idx*3]))\n",
    "print('')\n",
    "\n",
    "print(test['label1'].values[test_idx*3])\n",
    "print(test['sentence2'].values[test_idx*3])\n",
    "print(get_actual(test['sentence2'].values[test_idx*3]))\n",
    "print('')\n",
    "\n",
    "print(test['label1'].values[test_idx*3+1])\n",
    "print(test['sentence2'].values[test_idx*3+1])\n",
    "print(get_actual(test['sentence2'].values[test_idx*3+1]))\n",
    "print('')\n",
    "\n",
    "print(test['label1'].values[test_idx*3+2])\n",
    "print(test['sentence2'].values[test_idx*3+2])\n",
    "print(get_actual(test['sentence2'].values[test_idx*3+2]))\n",
    "print('')\n",
    "print('=========================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A man poses in front of an ad.'"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['sentence2'].values[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{subj_dep: [man], vb_dep: [poses], obj_dep: [front, ad]}']"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_actual(test['sentence2'].values[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{subj_dep: [man], vb_dep: [poses], obj_dep: [front, ad, beer]}']"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_actual(test['sentence2'].values[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{subj_dep: [man], vb_dep: [walks], obj_dep: [ad]}']"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_actual(test['sentence2'].values[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 довжина речення\n",
    "2 якщо значень в масиві > 1  вибирати найбільш коротчі кобінації\n",
    "3 по кожній х груп знайти вектори слів і прирівняти до гіпотези\n",
    "4 те саме тільки з ворднетом\n",
    "5 кількість значень"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get similarity by words word2vec and wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('../../Projects/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/grycshuknazar/anaconda2/envs/env_36/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7609457089782209"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity(\"dog\", \"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "heart_types[i].wup_similarity(heart_types[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "wordnet_tags={\n",
    "    'ADJ':wn.ADJ,\n",
    "    'ADV':wn.ADV,\n",
    "    'NOUN':wn.NOUN,\n",
    "    'PROPN':wn.NOUN,\n",
    "    'VERB':wn.VERB}\n",
    "\n",
    "def get_wordnet_token(token):\n",
    "    return wn.synsets(token.lemma_,pos=wordnet_tags[token.pos_])\n",
    "\n",
    "def get_word_net_similarity(token1, token2):\n",
    "    word1 = get_wordnet_token(token1)\n",
    "    word2 = get_wordnet_token(token2)\n",
    "    return word1[0].wup_similarity(word2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_words(token1, token2):\n",
    "    wordnet_sim_fail=False\n",
    "    wcv_sim_fail=False\n",
    "    \n",
    "    if token1.lemma_ == token2.lemma_:\n",
    "        return 1\n",
    "    \n",
    "    try:\n",
    "        wcv_sim = model.wv.similarity(token1.lemma_, token2.lemma_)\n",
    "    except:\n",
    "        wcv_sim=None\n",
    "        wcv_sim_fail=True\n",
    "        \n",
    "    try:\n",
    "        wordnet_sim = get_word_net_similarity(token1,token2)\n",
    "    except:\n",
    "        wordnet_sim =None\n",
    "        wordnet_sim_fail=True\n",
    "        \n",
    "    if (wcv_sim_fail or wcv_sim==None)  and (wordnet_sim==None or wordnet_sim_fail):\n",
    "        return 0\n",
    "    \n",
    "    if wcv_sim_fail or wcv_sim==None:\n",
    "        return wordnet_sim\n",
    "    \n",
    "    if wordnet_sim_fail or wordnet_sim == None:\n",
    "         return wcv_sim\n",
    "    \n",
    "    return max(wcv_sim, wordnet_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'obj_dep': [front, ad], 'subj_dep': [man], 'vb_dep': [poses]}]"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_doc = get_doc('A man poses in front of an ad.')\n",
    "roots = get_roots(nlp_doc)\n",
    "parts1 =get_parts(nlp_doc,roots)\n",
    "parts1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ad"
      ]
     },
     "execution_count": 605,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts1[0]['obj_dep'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'obj_dep': [front, advertisement], 'subj_dep': [man], 'vb_dep': [poses]}]"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_doc = get_doc('An old man with a package poses in front of an advertisement.')\n",
    "roots = get_roots(nlp_doc)\n",
    "parts2 =get_parts(nlp_doc,roots)\n",
    "parts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "advertisement"
      ]
     },
     "execution_count": 607,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts2[0]['obj_dep'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/grycshuknazar/anaconda2/envs/env_36/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_words(parts1[0]['obj_dep'][1],parts2[0]['obj_dep'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get block similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_block_similarity(block1, block2):\n",
    "    if len(block1)==0 or len(block2)==0:\n",
    "        return 0, None\n",
    "    skiped_block2_indexes = []\n",
    "    max_in_row=[]\n",
    "    result_matrix =[]\n",
    "    for id1,token_block1 in enumerate(block1):\n",
    "        result_matrix.append([])\n",
    "        for id2, token_block2 in enumerate(block2):\n",
    "            if id2 in skiped_block2_indexes:\n",
    "                result_matrix[id1].append(0)\n",
    "            else:\n",
    "                result_matrix[id1].append(compare_words(token_block1,token_block2))\n",
    "                \n",
    "        max_index = np.argmax(result_matrix[id1])\n",
    "\n",
    "\n",
    "        skiped_block2_indexes.append(max_index)\n",
    "        max_in_row.append(result_matrix[id1][max_index])\n",
    "    return np.mean(max_in_row), result_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/grycshuknazar/anaconda2/envs/env_36/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06872196507308677"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res, matrix = get_block_similarity(parts1[0]['obj_dep'],parts2[0]['obj_dep'])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "blok_names = ['obj_dep','subj_dep', 'vb_dep']\n",
    "\n",
    "def get_main_hipotes_similarity(parts1, parts2):\n",
    "    skiped_block2_indexes = []\n",
    "    max_in_row=[]\n",
    "    result_matrix =[]\n",
    "    for id1,part1 in enumerate(parts1):\n",
    "        result_matrix.append([])\n",
    "        blocks=[]\n",
    "        for id2, part2 in enumerate(parts2):\n",
    "            block={}\n",
    "            if id2 in skiped_block2_indexes:\n",
    "                result_matrix[id1].append(0)\n",
    "            else:\n",
    "                block={}\n",
    "                for block_name in blok_names:\n",
    "                    block[block_name] = get_block_similarity(part1[block_name],part2[block_name])[0]\n",
    "                #print(list(block.values()))\n",
    "                result_matrix[id1].append(np.mean(list(block.values())))\n",
    "            blocks.append(block)\n",
    "        max_index = np.argmax(result_matrix[id1])\n",
    "        skiped_block2_indexes.append(max_index)\n",
    "        max_in_row.append(blocks[max_index])\n",
    "    return max_in_row, result_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'obj_dep': [fire], 'subj_dep': [firefighters], 'vb_dep': [putting out]},\n",
       " {'obj_dep': [subway station], 'subj_dep': [firefighters], 'vb_dep': []}]"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_doc = get_doc('Three firefighters putting out a fire inside of a subway station')\n",
    "roots = get_roots(nlp_doc)\n",
    "parts2 =get_parts(nlp_doc,roots)\n",
    "parts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'obj_dep': [front, ad], 'subj_dep': [man], 'vb_dep': [poses]}]"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'obj_dep': [fire], 'subj_dep': [firefighters], 'vb_dep': [putting out]},\n",
       " {'obj_dep': [subway station], 'subj_dep': [firefighters], 'vb_dep': []}]"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/grycshuknazar/anaconda2/envs/env_36/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([{'obj_dep': 0.06872196507308677,\n",
       "   'subj_dep': 0.631578947368421,\n",
       "   'vb_dep': 0.3333333333333333}],\n",
       " [[0.3445447485916137, 0.22969456238801023]])"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_main_hipotes_similarity(parts1,parts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gry_similarity_feature(features, parts1,parts2):\n",
    "    similarity=None\n",
    "    if len(parts2)>len(parts1):\n",
    "        similarity, scores = get_main_hipotes_similarity(parts1,parts2)\n",
    "    else:\n",
    "        similarity, scores = get_main_hipotes_similarity(parts2,parts1)\n",
    "    for idx,item in enumerate(similarity):\n",
    "        for part_type in item:\n",
    "            features['similarity_{0}_{1}'.format(idx,part_type)]=item[part_type]\n",
    "    for idr,row in enumerate(scores):\n",
    "        for idx,item in enumerate(row):\n",
    "            features['avg_scores_{0}_{1}'.format(idr,idx)]=item\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/grycshuknazar/anaconda2/envs/env_36/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg_scores_0_0': 0.3445447485916137,\n",
       " 'avg_scores_0_1': 0.22969456238801023,\n",
       " 'similarity_0_obj_dep': 0.06872196507308677,\n",
       " 'similarity_0_subj_dep': 0.631578947368421,\n",
       " 'similarity_0_vb_dep': 0.3333333333333333}"
      ]
     },
     "execution_count": 570,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gry_similarity_feature(parts1,parts2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get length feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'obj_dep': [offensive], 'subj_dep': [statue], 'vb_dep': [is]},\n",
       " {'obj_dep': [mad], 'subj_dep': [people], 'vb_dep': [are]}]"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'The statue is offensive and people are mad that it is on display'\n",
    "nlp_doc = get_doc(text)\n",
    "roots = get_roots(nlp_doc)\n",
    "parts =get_parts(nlp_doc,roots)\n",
    "parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'main_0_obj_dep_len': 1,\n",
       " 'main_0_subj_dep_len': 1,\n",
       " 'main_0_vb_dep_len': 1,\n",
       " 'main_1_obj_dep_len': 1,\n",
       " 'main_1_subj_dep_len': 1,\n",
       " 'main_1_vb_dep_len': 1,\n",
       " 'main_groups_count': 2,\n",
       " 'main_text_len': 64}"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_length_features(features, text, parts, prefix):\n",
    "\n",
    "    text_len = len(text)\n",
    "    features[prefix+'text_len']=text_len\n",
    "\n",
    "    groups_count = len(parts)\n",
    "    features[prefix+'groups_count']=groups_count\n",
    "\n",
    "    for i in range(0,groups_count):\n",
    "        for key in parts[i]:\n",
    "            features[prefix+'{0}_{1}_len'.format(i,key)]=len(parts[i][key])\n",
    "    \n",
    "    return features\n",
    "\n",
    "get_length_features({}, text,parts, 'main_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect and vectorize features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(text1,text2):\n",
    "    features={}\n",
    "    text1=str(text1)\n",
    "    text2=str(text2)\n",
    "    nlp_doc = get_doc(text1)\n",
    "    roots = get_roots(nlp_doc)\n",
    "    parts1 =get_parts(nlp_doc,roots)\n",
    "    features = get_length_features(features, text1, parts1, 'main_')\n",
    "    \n",
    "    nlp_doc = get_doc(text2)\n",
    "    roots = get_roots(nlp_doc)\n",
    "    parts2 =get_parts(nlp_doc,roots)\n",
    "    features = get_length_features(features, text2, parts2, 'hip_')\n",
    "    \n",
    "    features = gry_similarity_feature(features,parts1,parts2)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/grycshuknazar/anaconda2/envs/env_36/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg_scores_0_0': 0.30868481724189223,\n",
       " 'avg_scores_0_1': 0.22480022253793977,\n",
       " 'avg_scores_1_0': 0,\n",
       " 'avg_scores_1_1': 0.10711761372303215,\n",
       " 'hip_0_obj_dep_len': 1,\n",
       " 'hip_0_subj_dep_len': 1,\n",
       " 'hip_0_vb_dep_len': 1,\n",
       " 'hip_1_obj_dep_len': 1,\n",
       " 'hip_1_subj_dep_len': 1,\n",
       " 'hip_1_vb_dep_len': 0,\n",
       " 'hip_groups_count': 2,\n",
       " 'hip_text_len': 64,\n",
       " 'main_0_obj_dep_len': 1,\n",
       " 'main_0_subj_dep_len': 1,\n",
       " 'main_0_vb_dep_len': 1,\n",
       " 'main_1_obj_dep_len': 1,\n",
       " 'main_1_subj_dep_len': 1,\n",
       " 'main_1_vb_dep_len': 1,\n",
       " 'main_groups_count': 2,\n",
       " 'main_text_len': 64,\n",
       " 'similarity_0_obj_dep': 0.1260544517256766,\n",
       " 'similarity_0_subj_dep': 0.4,\n",
       " 'similarity_0_vb_dep': 0.4,\n",
       " 'similarity_1_obj_dep': 0.13953465935091464,\n",
       " 'similarity_1_subj_dep': 0.18181818181818182,\n",
       " 'similarity_1_vb_dep': 0}"
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_features('The statue is offensive and people are mad that it is on display','Three firefighters putting out a fire inside of a subway station')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y(arr):\n",
    "    X=[]\n",
    "    y=[]\n",
    "    for idx in tqdm(range(0,len(arr))):\n",
    "        try:\n",
    "            item=arr[idx]\n",
    "            X.append(get_features(item[0],item[1]))\n",
    "            y.append(item[2])\n",
    "        except:\n",
    "            print(idx)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "413"
      ]
     },
     "execution_count": 613,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/550152 [00:00<?, ?it/s]/Users/grycshuknazar/anaconda2/envs/env_36/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if __name__ == '__main__':\n",
      " 17%|█▋        | 91490/550152 [25:10<2:06:10, 60.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91479\n",
      "91480\n",
      "91481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 311133/550152 [1:26:50<1:06:42, 59.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311124\n",
      "311125\n",
      "311126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 550152/550152 [2:34:30<00:00, 59.34it/s]  \n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = get_X_y(train[['sentence1','sentence2','label1']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'avg_scores_0_0': 0.4344420298772225,\n",
       "  'hip_0_obj_dep_len': 3,\n",
       "  'hip_0_subj_dep_len': 1,\n",
       "  'hip_0_vb_dep_len': 1,\n",
       "  'hip_groups_count': 1,\n",
       "  'hip_text_len': 49,\n",
       "  'main_0_obj_dep_len': 2,\n",
       "  'main_0_subj_dep_len': 1,\n",
       "  'main_0_vb_dep_len': 1,\n",
       "  'main_groups_count': 1,\n",
       "  'main_text_len': 54,\n",
       "  'similarity_0_obj_dep': 0.10332608963166771,\n",
       "  'similarity_0_subj_dep': 1.0,\n",
       "  'similarity_0_vb_dep': 0.2},\n",
       " {'avg_scores_0_0': 0.5246376811594202,\n",
       "  'avg_scores_0_1': 0.4222222222222223,\n",
       "  'hip_0_obj_dep_len': 1,\n",
       "  'hip_0_subj_dep_len': 1,\n",
       "  'hip_0_vb_dep_len': 1,\n",
       "  'hip_1_obj_dep_len': 1,\n",
       "  'hip_1_subj_dep_len': 1,\n",
       "  'hip_1_vb_dep_len': 1,\n",
       "  'hip_groups_count': 2,\n",
       "  'hip_text_len': 45,\n",
       "  'main_0_obj_dep_len': 2,\n",
       "  'main_0_subj_dep_len': 1,\n",
       "  'main_0_vb_dep_len': 1,\n",
       "  'main_groups_count': 1,\n",
       "  'main_text_len': 54,\n",
       "  'similarity_0_obj_dep': 0.17391304347826086,\n",
       "  'similarity_0_subj_dep': 1.0,\n",
       "  'similarity_0_vb_dep': 0.4},\n",
       " {'avg_scores_0_0': 0.5654320987654321,\n",
       "  'hip_0_obj_dep_len': 1,\n",
       "  'hip_0_subj_dep_len': 1,\n",
       "  'hip_0_vb_dep_len': 1,\n",
       "  'hip_groups_count': 1,\n",
       "  'hip_text_len': 33,\n",
       "  'main_0_obj_dep_len': 2,\n",
       "  'main_0_subj_dep_len': 1,\n",
       "  'main_0_vb_dep_len': 1,\n",
       "  'main_groups_count': 1,\n",
       "  'main_text_len': 54,\n",
       "  'similarity_0_obj_dep': 0.2962962962962963,\n",
       "  'similarity_0_subj_dep': 1.0,\n",
       "  'similarity_0_vb_dep': 0.4},\n",
       " {'avg_scores_0_0': 0.3333333333333333,\n",
       "  'avg_scores_0_1': 0.2634920634920635,\n",
       "  'hip_0_obj_dep_len': 2,\n",
       "  'hip_0_subj_dep_len': 0,\n",
       "  'hip_0_vb_dep_len': 1,\n",
       "  'hip_groups_count': 1,\n",
       "  'hip_text_len': 33,\n",
       "  'main_0_obj_dep_len': 0,\n",
       "  'main_0_subj_dep_len': 1,\n",
       "  'main_0_vb_dep_len': 1,\n",
       "  'main_1_obj_dep_len': 1,\n",
       "  'main_1_subj_dep_len': 1,\n",
       "  'main_1_vb_dep_len': 1,\n",
       "  'main_groups_count': 2,\n",
       "  'main_text_len': 37,\n",
       "  'similarity_0_obj_dep': 0,\n",
       "  'similarity_0_subj_dep': 0,\n",
       "  'similarity_0_vb_dep': 1.0},\n",
       " {'avg_scores_0_0': 0.08333333333333333,\n",
       "  'avg_scores_0_1': 0.10392624897027257,\n",
       "  'hip_0_obj_dep_len': 1,\n",
       "  'hip_0_subj_dep_len': 0,\n",
       "  'hip_0_vb_dep_len': 1,\n",
       "  'hip_groups_count': 1,\n",
       "  'hip_text_len': 26,\n",
       "  'main_0_obj_dep_len': 0,\n",
       "  'main_0_subj_dep_len': 1,\n",
       "  'main_0_vb_dep_len': 1,\n",
       "  'main_1_obj_dep_len': 1,\n",
       "  'main_1_subj_dep_len': 1,\n",
       "  'main_1_vb_dep_len': 1,\n",
       "  'main_groups_count': 2,\n",
       "  'main_text_len': 37,\n",
       "  'similarity_0_obj_dep': 0.06177874691081769,\n",
       "  'similarity_0_subj_dep': 0,\n",
       "  'similarity_0_vb_dep': 0.25},\n",
       " {'avg_scores_0_0': 0.6,\n",
       "  'avg_scores_0_1': 0.5333333333333333,\n",
       "  'hip_0_obj_dep_len': 0,\n",
       "  'hip_0_subj_dep_len': 1,\n",
       "  'hip_0_vb_dep_len': 1,\n",
       "  'hip_groups_count': 1,\n",
       "  'hip_text_len': 21,\n",
       "  'main_0_obj_dep_len': 0,\n",
       "  'main_0_subj_dep_len': 1,\n",
       "  'main_0_vb_dep_len': 1,\n",
       "  'main_1_obj_dep_len': 1,\n",
       "  'main_1_subj_dep_len': 1,\n",
       "  'main_1_vb_dep_len': 1,\n",
       "  'main_groups_count': 2,\n",
       "  'main_text_len': 37,\n",
       "  'similarity_0_obj_dep': 0,\n",
       "  'similarity_0_subj_dep': 1.0,\n",
       "  'similarity_0_vb_dep': 0.8},\n",
       " {'avg_scores_0_0': 0.6369047619047619,\n",
       "  'hip_0_obj_dep_len': 1,\n",
       "  'hip_0_subj_dep_len': 1,\n",
       "  'hip_0_vb_dep_len': 1,\n",
       "  'hip_groups_count': 1,\n",
       "  'hip_text_len': 33,\n",
       "  'main_0_obj_dep_len': 4,\n",
       "  'main_0_subj_dep_len': 1,\n",
       "  'main_0_vb_dep_len': 1,\n",
       "  'main_groups_count': 1,\n",
       "  'main_text_len': 61,\n",
       "  'similarity_0_obj_dep': 0.625,\n",
       "  'similarity_0_subj_dep': 1.0,\n",
       "  'similarity_0_vb_dep': 0.2857142857142857},\n",
       " {'avg_scores_0_0': 0.5370649444227332,\n",
       "  'hip_0_obj_dep_len': 1,\n",
       "  'hip_0_subj_dep_len': 1,\n",
       "  'hip_0_vb_dep_len': 1,\n",
       "  'hip_groups_count': 1,\n",
       "  'hip_text_len': 35,\n",
       "  'main_0_obj_dep_len': 4,\n",
       "  'main_0_subj_dep_len': 1,\n",
       "  'main_0_vb_dep_len': 1,\n",
       "  'main_groups_count': 1,\n",
       "  'main_text_len': 61,\n",
       "  'similarity_0_obj_dep': 0.21119483326819963,\n",
       "  'similarity_0_subj_dep': 1.0,\n",
       "  'similarity_0_vb_dep': 0.4},\n",
       " {'avg_scores_0_0': 0.5179487179487179,\n",
       "  'hip_0_obj_dep_len': 1,\n",
       "  'hip_0_subj_dep_len': 1,\n",
       "  'hip_0_vb_dep_len': 1,\n",
       "  'hip_groups_count': 1,\n",
       "  'hip_text_len': 36,\n",
       "  'main_0_obj_dep_len': 4,\n",
       "  'main_0_subj_dep_len': 1,\n",
       "  'main_0_vb_dep_len': 1,\n",
       "  'main_groups_count': 1,\n",
       "  'main_text_len': 61,\n",
       "  'similarity_0_obj_dep': 0.15384615384615385,\n",
       "  'similarity_0_subj_dep': 1.0,\n",
       "  'similarity_0_vb_dep': 0.4},\n",
       " {'avg_scores_0_0': 0.5555732626242357,\n",
       "  'avg_scores_0_1': 0.5,\n",
       "  'avg_scores_0_2': 0.2714390212846723,\n",
       "  'avg_scores_1_0': 0,\n",
       "  'avg_scores_1_1': 0.14972222651231834,\n",
       "  'avg_scores_1_2': 0.24707602339181287,\n",
       "  'hip_0_obj_dep_len': 2,\n",
       "  'hip_0_subj_dep_len': 2,\n",
       "  'hip_0_vb_dep_len': 1,\n",
       "  'hip_1_obj_dep_len': 0,\n",
       "  'hip_1_subj_dep_len': 2,\n",
       "  'hip_1_vb_dep_len': 1,\n",
       "  'hip_2_obj_dep_len': 1,\n",
       "  'hip_2_subj_dep_len': 2,\n",
       "  'hip_2_vb_dep_len': 1,\n",
       "  'hip_groups_count': 3,\n",
       "  'hip_text_len': 75,\n",
       "  'main_0_obj_dep_len': 5,\n",
       "  'main_0_subj_dep_len': 2,\n",
       "  'main_0_vb_dep_len': 1,\n",
       "  'main_1_obj_dep_len': 1,\n",
       "  'main_1_subj_dep_len': 4,\n",
       "  'main_1_vb_dep_len': 1,\n",
       "  'main_groups_count': 2,\n",
       "  'main_text_len': 139,\n",
       "  'similarity_0_obj_dep': 0.266719787872707,\n",
       "  'similarity_0_subj_dep': 1.0,\n",
       "  'similarity_0_vb_dep': 0.4,\n",
       "  'similarity_1_obj_dep': 0.3333333333333333,\n",
       "  'similarity_1_subj_dep': 0.15789473684210525,\n",
       "  'similarity_1_vb_dep': 0.25}]"
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_train.txt',X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]/Users/grycshuknazar/anaconda2/envs/env_36/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if __name__ == '__main__':\n",
      "100%|██████████| 10000/10000 [02:46<00:00, 60.10it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = get_X_y(test[['sentence1','sentence2','label1']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "feature_vectorizer= DictVectorizer()\n",
    "X_train_ = feature_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ = feature_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder=LabelEncoder()\n",
    "y_train_ = label_encoder.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_ = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrm = RandomForestClassifier(max_depth=20, n_estimators=20, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=20, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=4,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 692,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrm.fit(X_train_, y_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.59      0.69      0.64    183382\n",
      "   entailment       0.68      0.67      0.68    183382\n",
      "      neutral       0.65      0.55      0.60    183382\n",
      "\n",
      "  avg / total       0.64      0.64      0.64    550146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = lrm.predict(X_train_)\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = label_encoder.classes_\n",
    "print(classification_report(y_train_, predicted, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = lrm.predict(X_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.51      0.59      0.54      3333\n",
      "   entailment       0.60      0.58      0.59      3333\n",
      "      neutral       0.52      0.45      0.48      3334\n",
      "\n",
      "  avg / total       0.54      0.54      0.54     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = label_encoder.classes_\n",
    "print(classification_report(y_test_, predicted, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrm = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 716,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrm.fit(X_train_, y_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.46      0.50      0.48      3333\n",
      "   entailment       0.52      0.59      0.55      3333\n",
      "      neutral       0.51      0.40      0.45      3334\n",
      "\n",
      "  avg / total       0.50      0.50      0.49     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted = lrm.predict(X_test_)\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = label_encoder.classes_\n",
    "print(classification_report(y_test_, predicted, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "dummy_y = np_utils.to_categorical(y_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#NN\n",
    "# define baseline model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(121, input_dim=121, activation='relu'))\n",
    "    model.add(Dense(242, input_dim=242, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=5, batch_size=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "550146/550146 [==============================] - 103s 187us/step - loss: 1.0129 - acc: 0.4885\n",
      "Epoch 2/5\n",
      "373880/550146 [===================>..........] - ETA: 33s - loss: 0.9942 - acc: 0.5037"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-709-fd7de2ca5ef7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mX_train_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/envs/env_36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_weight'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/env_36/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/env_36/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1000\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda2/envs/env_36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda2/envs/env_36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/env_36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/env_36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/env_36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1120\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/env_36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    412\u001b[0m   \u001b[0;31m# dict instead of doing it in the callers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_handles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m     \"\"\"Creates a fetch handler.\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = estimator.fit( X_train_, dummy_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
