{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Класифікація залежностей із використанням нейронних мереж"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conllu\n",
    "import gzip\n",
    "import bz2\n",
    "from collections import OrderedDict\n",
    "import string\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gensim\n",
    "from sklearn.metrics import classification_report\n",
    "from tokenize_uk import tokenize_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense, Flatten, LSTM, Dropout, Embedding, Bidirectional\n",
    "from keras.layers.merge import Concatenate, concatenate\n",
    "from keras import Input, Model\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я спробував різні векторні представлення, вони дають схожі результати, але LexVec трохи кращі, ніж word2vec та GloVe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for parsing text file with vectors\n",
    "\"\"\"\n",
    "embeddings = {}\n",
    "with bz2.open('/mnt/hdd/Data/NLP/ubercorpus.lowercased.lemmatized.word2vec.300d.bz2', 'rb') as vf:\n",
    "    count = 0\n",
    "    for line in vf:\n",
    "        if count == 0:\n",
    "            count += 1\n",
    "            continue\n",
    "        count += 1\n",
    "        line = line.decode()\n",
    "        values = line.split()\n",
    "        if not values:\n",
    "            continue\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings[word] = coefs\n",
    "\n",
    "pickle.dump(embeddings, open('/mnt/hdd/Data/NLP/glove_lemma_embeddings.pkl', 'wb'))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = pickle.load(open('/mnt/hdd/Data/NLP/lex_lemma_embeddings.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'uk_iu-ud-train.conllu.gz'\n",
    "with gzip.open(fname, 'rb') as f:\n",
    "    raw_train = f.read().decode()\n",
    "\n",
    "fname3 = 'uk_iu-ud-test.conllu.gz'\n",
    "with gzip.open(fname3, 'rb') as f3:\n",
    "    raw_test = f3.read().decode()\n",
    "    \n",
    "train_data = conllu.parse(raw_train)\n",
    "test_data = conllu.parse(raw_test)\n",
    "\n",
    "# fix an error in test set:\n",
    "for sent in test_data:\n",
    "    for w in sent:\n",
    "        if w['deprel'] == 'dep':\n",
    "            w['deprel'] = 'det'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Щоб використати для парсера не тільки слова, але і частини мов (як у https://cs.stanford.edu/~danqi/papers/emnlp2014.pdf), можна натренувати векторні представлення частин мови на основі цього ж самого датасету:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sequences = []\n",
    "for sent in train_data:\n",
    "    seq = [w['upostag'] for w in sent]\n",
    "    pos_sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_model = gensim.models.Word2Vec(pos_sequences, size=300, window=3, min_count=1, iter=100)\n",
    "pos_vectors = {k:pos_model.wv[k] for k in pos_model.wv.vocab}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я спробував два підходи: простіший, при якому ми збираємо з \"золотих\" дерев всю інформацію про залежності між словами, окрім власне типу залежності, і потім визначаємо тип залежності класифікатором; і складніший, у якому ми використовуємо класифікатор у процесі парсингу для визначення наступної дії (SHIFT, REDUCE, LEFT+залежність, RIGHT+залежність). При цьому використовується інформація з 3 слів стеку та 3 слів черги. Функції для першого підходу позначені як \"simple\".\n",
    "\n",
    "У обох випадках я зробив одну модель з використанням тільки представлень слів, і іншу з представленнями слів і частин мови."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У мене був лишився код парсера ще з домашки, тому в деяких місцях я просто адаптував уже готові функції замість писання нових (хоча це зробило б код читабельнішим)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledParser():\n",
    "    \"\"\"\n",
    "    Dependency parser using static oracle,\n",
    "    for labeled dependencies.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, word_vectors, pos_vectors, train_data):\n",
    "        self.ROOT = OrderedDict({'form': 'ROOT', 'id': 0, 'head': -1, \n",
    "                                 'lemma': 'ROOT', 'upostag': 'UNK',\n",
    "                                 'deprel': 'root'})\n",
    "        self.train_data = train_data\n",
    "        self.word_vectors = word_vectors\n",
    "        self.pos_vectors = pos_vectors\n",
    "        # setting unknown word/POS to average vector seems to work a bit better than 0 vector\n",
    "        self.avg_vec = np.average([v for (k,v) in self.word_vectors.items()], axis=0)\n",
    "        self.avg_pos_vec = np.average([v for (k,v) in self.pos_vectors.items()], axis=0)\n",
    "        # initialize indices for word vectors (only for words present in train data)\n",
    "        self.dictionary = self.build_dictionary(self.train_data)\n",
    "        # initialize indices for labels\n",
    "        self.label_dict = self.build_label_dict(self.train_data)\n",
    "        self.label_dict_s = self.build_label_dict_simple(self.train_data)\n",
    "        # initialize indices for POS vectors\n",
    "        self.pos_index = {pos[0]:i for (i, pos) \n",
    "                          in enumerate(self.pos_vectors.items())}\n",
    "        self.pos_vectors.update({'UNK': self.avg_pos_vec})\n",
    "        self.pos_index.update({'UNK': len(self.pos_index)})\n",
    "    \n",
    "    def strip_colon(self, deprel):\n",
    "        \"\"\"\n",
    "        Strip the second part of deprel (after colon).\n",
    "        \"\"\"\n",
    "        if ':' in deprel:\n",
    "            new = deprel.split(':')[0].strip()\n",
    "            return new\n",
    "        else:\n",
    "            return deprel\n",
    "        \n",
    "    def make_action(self, action, stack, queue, relations):\n",
    "        \"\"\"\n",
    "        Applies action to the stack, the queue, and the relations.\n",
    "        \"\"\"\n",
    "        w1 = stack[-1]\n",
    "        w2 = queue[0]\n",
    "        action = action.split('_')[0]\n",
    "        if action == 'SHIFT':\n",
    "            stack.append(queue.pop(0))\n",
    "        elif action == 'REDUCE':\n",
    "            stack.pop()\n",
    "        elif action == 'LEFT':\n",
    "            relations.append((w1['id'], self.strip_colon(w1['deprel']), w2['id']))\n",
    "            stack.pop()\n",
    "        elif action == 'RIGHT':\n",
    "            relations.append((w2['id'], self.strip_colon(w2['deprel']), w1['id']))\n",
    "            stack.append(queue.pop(0))\n",
    "        return stack, queue, relations\n",
    "    \n",
    "    def apply_actions(self, tree, train=False, pos=False):\n",
    "        \"\"\"\n",
    "        Produce dependencies for the tree with known dependencies.\n",
    "        If train=True, also get features and labels in the process.\n",
    "        If pos=True, also get POS indices as features.\n",
    "        \"\"\"\n",
    "        stack = [self.ROOT]\n",
    "        queue = tree[:]\n",
    "        relations = []\n",
    "        label_indices = []\n",
    "        data_indices = []\n",
    "        pos_indices = []\n",
    "        while stack and queue:\n",
    "            top_stack = stack[-1] if stack else None\n",
    "            top_queue = queue[0] if queue else None\n",
    "            action = self.oracle(top_stack, top_queue, relations)\n",
    "            if train:\n",
    "                label_indices.append(self.label_dict[action])\n",
    "                data_indices.append(self.get_indices(stack, queue))\n",
    "                pos_indices.append(self.get_pos(stack, queue))\n",
    "            stack, queue, relations = self.make_action(action, stack, queue, relations)\n",
    "        if train and not pos:\n",
    "            return relations, data_indices, label_indices\n",
    "        if train and pos:\n",
    "            return relations, data_indices, label_indices, pos_indices\n",
    "        return relations\n",
    "    \n",
    "    def get_relations(self, tree):\n",
    "        \"\"\"\n",
    "        For 'simple' model.\n",
    "        \"\"\"\n",
    "        relations = []\n",
    "        for w in tree:\n",
    "            w1 = w['lemma']\n",
    "            deprel = self.strip_colon(w['deprel'])\n",
    "            if w['head'] == 0:\n",
    "                w2 = 'root'\n",
    "            else:\n",
    "                w2 = tree[w['head']-1]['lemma']\n",
    "            relations.append((w1, deprel, w2))\n",
    "        return relations\n",
    "    \n",
    "    def get_relation_data(self, tree):\n",
    "        \"\"\"\n",
    "        For 'simple' model.\n",
    "        \"\"\"\n",
    "        relations = self.get_relations(tree)\n",
    "        label_indices = []\n",
    "        word_indices = []\n",
    "        for r in relations:\n",
    "            w1, deprel, w2 = r\n",
    "            label_indices.append(self.label_dict_s[deprel])\n",
    "            w1i = self.word_to_index(w1)\n",
    "            w2i = self.word_to_index(w2)\n",
    "            word_indices.append((w1i, w2i))\n",
    "        return word_indices, label_indices\n",
    "    \n",
    "    def get_relation_pos_data(self, tree):\n",
    "        \"\"\"\n",
    "        POS data from tree for 'simple' model.\n",
    "        \"\"\"\n",
    "        relations = []\n",
    "        for w in tree:\n",
    "            w1 = w['upostag']\n",
    "            if w['head'] == 0:\n",
    "                w2 = 'UNK'\n",
    "            else:\n",
    "                w2 = tree[w['head']-1]['upostag']\n",
    "            relations.append((w1, w2))\n",
    "        pos_indices = []\n",
    "        for r in relations:\n",
    "            w1, w2 = r\n",
    "            w1i = self.pos_index.get(w1)\n",
    "            w2i = self.pos_index.get(w2)\n",
    "            pos_indices.append((w1i, w2i))\n",
    "        return pos_indices\n",
    "    \n",
    "    def word_to_index(self, word):\n",
    "        try:\n",
    "            wi = self.dictionary[word]\n",
    "        except:\n",
    "            wi = self.dictionary['unk']\n",
    "        return wi\n",
    "    \n",
    "    def pad_stack(self, stack, length=3):\n",
    "        \"\"\"\n",
    "        Make sure stack is always of same length.\n",
    "        \"\"\"\n",
    "        top_stack = stack[-length:]\n",
    "        diff = length-len(top_stack)\n",
    "        unk_w = {'lemma': 'unk', 'upostag': 'UNK'}\n",
    "        padded = diff*[unk_w] + [w for w in top_stack]\n",
    "        return padded\n",
    "    \n",
    "    def append_queue(self, queue, length=3):\n",
    "        \"\"\"\n",
    "        Make sure queue is always of same length.\n",
    "        \"\"\"\n",
    "        top_queue = queue[:length]\n",
    "        diff = length-len(top_queue)\n",
    "        unk_w = {'lemma': 'unk', 'upostag': 'UNK'}\n",
    "        appended = [w for w in top_queue] + diff*[unk_w]\n",
    "        return appended\n",
    "\n",
    "    def get_indices(self, stack, queue):\n",
    "        \"\"\"\n",
    "        Get word indices for stack and queue.\n",
    "        \"\"\"\n",
    "        top_stack = self.pad_stack(stack, 3)\n",
    "        top_queue = self.append_queue(queue, 3)\n",
    "        res = [self.word_to_index(w['lemma'].lower()) for w in top_stack] +\\\n",
    "              [self.word_to_index(w['lemma'].lower()) for w in top_queue]\n",
    "        return np.array(res)\n",
    "    \n",
    "    def get_pos(self, stack, queue):\n",
    "        \"\"\"\n",
    "        Get POS indices for stack and queue.\n",
    "        \"\"\"\n",
    "        top_stack = self.pad_stack(stack, 3)\n",
    "        top_queue = self.append_queue(queue, 3)\n",
    "        res = [self.pos_index.get(w['upostag']) for w in top_stack] +\\\n",
    "              [self.pos_index.get(w['upostag']) for w in top_queue]\n",
    "        return np.array(res)\n",
    "    \n",
    "    def oracle(self, top_stack, top_queue, relations):\n",
    "        \"\"\"\n",
    "        Returns the right action given the state\n",
    "        of the stack, the queue, and the relations.\n",
    "        \"\"\"\n",
    "        if top_stack and not top_queue:\n",
    "            return 'REDUCE'\n",
    "        elif top_queue['head'] == top_stack['id']:\n",
    "            return 'RIGHT' + '_' + self.strip_colon(top_queue['deprel'])\n",
    "        elif top_stack['head'] == top_queue['id']:\n",
    "            return 'LEFT' + '_' + self.strip_colon(top_stack['deprel'])\n",
    "        elif (top_stack['id'] in [i[0] for i in relations] and \n",
    "             top_queue['head'] < top_stack['id']):\n",
    "            return 'REDUCE'\n",
    "        else:\n",
    "            return 'SHIFT'\n",
    "    \n",
    "    def build_dictionary(self, train_data):\n",
    "        \"\"\"\n",
    "        Create word dictionary using keras tools.\n",
    "        \"\"\"\n",
    "        tokenizer = Tokenizer(oov_token='unk', lower=True)\n",
    "        texts = []\n",
    "        for tree in train_data:\n",
    "            text = ' '.join([w['lemma'] for w in tree])\n",
    "            texts.append(text)\n",
    "        tokenizer.fit_on_texts(texts)\n",
    "        sequences = tokenizer.texts_to_sequences(texts)\n",
    "        padded_sequences = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=12)\n",
    "        dictionary = tokenizer.word_index\n",
    "        return dictionary\n",
    "    \n",
    "    def build_label_dict_simple(self, train_data):\n",
    "        \"\"\"\n",
    "        Label dict without parser actions ('LEFT' etc)\n",
    "        \"\"\"\n",
    "        all_deps = []\n",
    "        for t in train_data:\n",
    "            for w in t:\n",
    "                all_deps.append(self.strip_colon(w['deprel']))\n",
    "        all_deps = set(all_deps)\n",
    "        label_index = {k:i for (i,k) in enumerate(list(all_deps))}\n",
    "        return label_index\n",
    "    \n",
    "    def build_label_dict(self, train_data):\n",
    "        \"\"\"\n",
    "        Dictionary of labels + actions ('LEFT_case' etc)\n",
    "        \"\"\"\n",
    "        all_deps = []\n",
    "        for t in train_data:\n",
    "            for w in t:\n",
    "                all_deps.append(self.strip_colon(w['deprel']))\n",
    "        all_deps = set(all_deps)\n",
    "        left = ['LEFT_'+dep for dep in list(all_deps)]\n",
    "        right = ['RIGHT_'+dep for dep in list(all_deps)]\n",
    "        labels = left + right + ['SHIFT', 'REDUCE']\n",
    "        label_index = {k:i for (i,k) in enumerate(labels)}\n",
    "        return label_index\n",
    "        \n",
    "    def word_emb_layer(self, input_len=2):\n",
    "        \"\"\"\n",
    "        Create Embedding layer for word embeddings.\n",
    "        \"\"\"\n",
    "        if not self.dictionary:\n",
    "            return None\n",
    "        embedding_matrix = np.zeros((len(self.dictionary) + 1, 300))\n",
    "        for word, i in self.dictionary.items():\n",
    "            embedding_vector = self.word_vectors.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "            else:\n",
    "                embedding_matrix[i] = self.avg_vec\n",
    "        embedding_layer = Embedding(len(self.dictionary) + 1,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=input_len,\n",
    "                            trainable=False)\n",
    "        return embedding_layer\n",
    "    \n",
    "    def pos_emb_layer(self, input_len=2):\n",
    "        \"\"\"\n",
    "        Create Embedding layer for POS embeddings.\n",
    "        \"\"\"\n",
    "        emb_matrix = np.zeros((len(self.pos_index)+1, 300))\n",
    "        for pos, i in self.pos_index.items():\n",
    "            emb_vector = self.pos_vectors.get(pos)\n",
    "            emb_matrix[i] = emb_vector\n",
    "        emb_layer = Embedding(len(self.pos_index)+1,\n",
    "                              300,\n",
    "                              weights=[emb_matrix],\n",
    "                              input_length=input_len,\n",
    "                              trainable=False)\n",
    "        return emb_layer\n",
    "    \n",
    "    def get_data_indices(self, data):\n",
    "        \"\"\"\n",
    "        Get the input data (word indices).\n",
    "        \"\"\"\n",
    "        x_indices, y_indices = [], []\n",
    "        for tree in data:\n",
    "            rels, data_i, label_i = self.apply_actions(tree, train=True)\n",
    "            x_indices.extend(data_i)\n",
    "            y_indices.extend(label_i)\n",
    "        return np.array(x_indices), np.array(y_indices)\n",
    "    \n",
    "    def get_data_indices_simple(self, data):\n",
    "        \"\"\"\n",
    "        Input data for 'simple' model.\n",
    "        \"\"\"\n",
    "        x_indices, y_indices = [], []\n",
    "        for tree in data:\n",
    "            data_i, label_i = self.get_relation_data(tree)\n",
    "            x_indices.extend(data_i)\n",
    "            y_indices.extend(label_i)\n",
    "        return np.array(x_indices), np.array(y_indices)\n",
    "    \n",
    "    def get_pos_indices(self, data):\n",
    "        \"\"\"\n",
    "        Get the input data (POS indices).\n",
    "        \"\"\"\n",
    "        pos_indices = []\n",
    "        for tree in data:\n",
    "            rels, data_i, label_i, pos_i = self.apply_actions(tree, train=True, pos=True)\n",
    "            pos_indices.extend(pos_i)\n",
    "        return np.array(pos_indices)\n",
    "    \n",
    "    def get_pos_indices_simple(self, data):\n",
    "        \"\"\"\n",
    "        POS inputs for 'simple' model.\n",
    "        \"\"\"\n",
    "        pos_indices = []\n",
    "        for tree in data:\n",
    "            pos_i = self.get_relation_pos_data(tree)\n",
    "            pos_indices.extend(pos_i)\n",
    "        return np.array(pos_indices)\n",
    "    \n",
    "    def NN_simple(self, test_data):\n",
    "        \"\"\"\n",
    "        Model predicting only type of relation, \n",
    "        using word embeddings.\n",
    "        \"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(self.word_emb_layer(2))\n",
    "        model.add(Dense(1024, activation='relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(1024, activation='relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(1024, activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(len(self.label_dict_s), activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['acc'])\n",
    "        x_train, y_train = self.get_data_indices_simple(self.train_data)\n",
    "        y_train = to_categorical(y_train)\n",
    "        x_test, y_test = self.get_data_indices_simple(test_data)\n",
    "        y_test = to_categorical(y_test)\n",
    "        model.fit(x_train, y_train, epochs=5, batch_size=128,\n",
    "                  validation_data=(x_test, y_test))\n",
    "        predicted = model.predict(x_test)\n",
    "        return predicted\n",
    "    \n",
    "    def NN_simple_pos(self, test_data):\n",
    "        \"\"\"\n",
    "        Model predicting only type of relation, using both\n",
    "        word and POS embeddings.\n",
    "        \"\"\"\n",
    "        model_word_in = Input(shape=(2, ))\n",
    "        model_word_emb = self.word_emb_layer(2)(model_word_in)\n",
    "        model_word = Model(model_word_in, model_word_emb)\n",
    "        model_pos_in = Input(shape=(2, ))\n",
    "        model_pos_emb = self.pos_emb_layer(2)(model_pos_in)\n",
    "        model_pos = Model(model_pos_in, model_pos_emb)\n",
    "        concatenated = concatenate([model_word_emb, model_pos_emb])\n",
    "        x = Bidirectional(LSTM(1024, dropout=0.2, recurrent_dropout=0.2,\n",
    "                 return_sequences=True))(concatenated)\n",
    "        x = Dense(1024, activation='relu')(concatenated)\n",
    "        x = Dropout(0.5)(x)\n",
    "        x = Dense(1024, activation='relu')(x)\n",
    "        x = Flatten()(x)\n",
    "        out = Dense(len(self.label_dict_s), activation='softmax', name='output_layer')(x)\n",
    "        \n",
    "        merged_model = Model([model_word_in, model_pos_in], out)\n",
    "        merged_model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "                             metrics=['accuracy'])\n",
    "        x_train, y_train = self.get_data_indices_simple(self.train_data)\n",
    "        pos_train = self.get_pos_indices_simple(self.train_data)\n",
    "        y_train = to_categorical(y_train)\n",
    "        x_test, y_test = self.get_data_indices_simple(test_data)\n",
    "        pos_test = self.get_pos_indices_simple(test_data)\n",
    "        y_test = to_categorical(y_test)\n",
    "        merged_model.fit([x_train, pos_train], y_train, batch_size=128, epochs=5,\n",
    "                         validation_data=([x_test, pos_test], y_test))\n",
    "        predicted = merged_model.predict([x_test, pos_test])\n",
    "        return predicted\n",
    "    \n",
    "    def NN(self, test_data):\n",
    "        \"\"\"\n",
    "        Model predicting next action for transition parser \n",
    "        with LSTM and Dense layers and taking word embeddings as input.\n",
    "        \"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(self.word_emb_layer(6))\n",
    "        model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2,\n",
    "                 return_sequences=True)))\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(len(self.label_dict), activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='rmsprop',\n",
    "                      metrics=['acc'])\n",
    "        x_train, y_train = self.get_data_indices(self.train_data)\n",
    "        y_train = to_categorical(y_train)\n",
    "        x_test, y_test = self.get_data_indices(test_data)\n",
    "        y_test = to_categorical(y_test)\n",
    "        model.fit(x_train, y_train, epochs=5, batch_size=128,\n",
    "                  validation_data=(x_test, y_test))\n",
    "        return model\n",
    "    \n",
    "    def NN_pos(self, test_data):\n",
    "        \"\"\"\n",
    "        Model predicting next action for transition parser \n",
    "        with LSTM and Dense layers and taking both word embeddings \n",
    "        and POS embeddings as input.\n",
    "        \"\"\"\n",
    "        model_word_in = Input(shape=(6, ))\n",
    "        model_word_emb = self.word_emb_layer(6)(model_word_in)\n",
    "        model_word = Model(model_word_in, model_word_emb)\n",
    "        model_pos_in = Input(shape=(6, ))\n",
    "        model_pos_emb = self.pos_emb_layer(6)(model_pos_in)\n",
    "        model_pos = Model(model_pos_in, model_pos_emb)\n",
    "        concatenated = concatenate([model_word_emb, model_pos_emb])\n",
    "        x = Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2,\n",
    "                 return_sequences=True))(concatenated)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        x = Dropout(0.3)(x)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        x = Flatten()(x)\n",
    "        out = Dense(len(self.label_dict), activation='softmax')(x)\n",
    "        model = Model([model_word_in, model_pos_in], out)\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='rmsprop',\n",
    "                      metrics=['acc'])\n",
    "        x_train, y_train = self.get_data_indices(self.train_data)\n",
    "        pos_train = self.get_pos_indices(self.train_data)\n",
    "        y_train = to_categorical(y_train)\n",
    "        x_test, y_test = self.get_data_indices(test_data)\n",
    "        pos_test = self.get_pos_indices(test_data)\n",
    "        y_test = to_categorical(y_test)\n",
    "        model.fit([x_train, pos_train], y_train, epochs=5, batch_size=128,\n",
    "                  validation_data=([x_test, pos_test], y_test))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp = LabeledParser(word_vectors, pos_vectors, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перша проста модель - кілька Dense шарів із дропаутами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 75098 samples, validate on 14939 samples\n",
      "Epoch 1/5\n",
      "75098/75098 [==============================] - 31s 419us/step - loss: 0.9315 - acc: 0.7243 - val_loss: 1.1844 - val_acc: 0.6479\n",
      "Epoch 2/5\n",
      "75098/75098 [==============================] - 31s 418us/step - loss: 0.7539 - acc: 0.7654 - val_loss: 1.1336 - val_acc: 0.6642\n",
      "Epoch 3/5\n",
      "75098/75098 [==============================] - 30s 401us/step - loss: 0.7041 - acc: 0.7792 - val_loss: 1.1386 - val_acc: 0.6660\n",
      "Epoch 4/5\n",
      "75098/75098 [==============================] - 31s 418us/step - loss: 0.6663 - acc: 0.7879 - val_loss: 1.1267 - val_acc: 0.6633\n",
      "Epoch 5/5\n",
      "75098/75098 [==============================] - 35s 461us/step - loss: 0.6334 - acc: 0.7976 - val_loss: 1.1412 - val_acc: 0.6608\n"
     ]
    }
   ],
   "source": [
    "predicted_simple = lp.NN_simple(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Друга проста модель використовує функціональний API бібліотеки keras, тому що в sequential API складніше об'єднати два набори векторних представлень (слова і POS-теги), що покращує точність до 80%. Також тут є шар LSTM - він дав додаткове покращення, незважаючи на відсутність великих послідовностей у даних."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 75098 samples, validate on 14939 samples\n",
      "Epoch 1/5\n",
      "75098/75098 [==============================] - 22s 292us/step - loss: 0.6063 - acc: 0.8004 - val_loss: 0.5200 - val_acc: 0.8206\n",
      "Epoch 2/5\n",
      "75098/75098 [==============================] - 22s 293us/step - loss: 0.4899 - acc: 0.8277 - val_loss: 0.5017 - val_acc: 0.8302\n",
      "Epoch 3/5\n",
      "75098/75098 [==============================] - 23s 306us/step - loss: 0.4633 - acc: 0.8358 - val_loss: 0.4900 - val_acc: 0.8318\n",
      "Epoch 4/5\n",
      "75098/75098 [==============================] - 25s 334us/step - loss: 0.4472 - acc: 0.8408 - val_loss: 0.4882 - val_acc: 0.8289\n",
      "Epoch 5/5\n",
      "75098/75098 [==============================] - 26s 343us/step - loss: 0.4307 - acc: 0.8457 - val_loss: 0.4863 - val_acc: 0.8359\n"
     ]
    }
   ],
   "source": [
    "predicted_simple_pos = lp.NN_simple_pos(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точність на тестовій вибірці понад 83% - найвищий результат, який у мене виходив. Детальніше можна подивитись у таблиці - деякі типи залежностей, які легко передбачити через частину мови, мають практично 100% точність."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        acl       0.77      0.91      0.83       176\n",
      "      advcl       0.31      0.28      0.29       149\n",
      "     advmod       0.93      0.99      0.96       644\n",
      "       amod       0.98      1.00      0.99      1459\n",
      "      appos       0.21      0.04      0.06       105\n",
      "        aux       1.00      0.93      0.96        27\n",
      "       case       1.00      1.00      1.00      1373\n",
      "         cc       0.99      0.99      0.99       554\n",
      "      ccomp       0.60      0.31      0.41        83\n",
      "   compound       0.84      0.46      0.59        81\n",
      "       conj       0.55      0.32      0.40       787\n",
      "        cop       0.98      1.00      0.99        80\n",
      "      csubj       0.78      0.66      0.71        47\n",
      "        det       0.99      0.98      0.98       447\n",
      "  discourse       0.89      0.91      0.90       183\n",
      "       expl       0.45      0.77      0.57        13\n",
      "      fixed       0.82      0.74      0.78        31\n",
      "       flat       0.73      0.59      0.65       367\n",
      "   goeswith       0.00      0.00      0.00         2\n",
      "       iobj       0.20      0.14      0.16        36\n",
      "       list       0.00      0.00      0.00         3\n",
      "       mark       0.99      0.97      0.98       233\n",
      "       nmod       0.72      0.95      0.82      1702\n",
      "      nsubj       0.49      0.64      0.55       796\n",
      "     nummod       0.74      1.00      0.85       117\n",
      "        obj       0.56      0.55      0.55       627\n",
      "        obl       0.68      0.54      0.60      1025\n",
      "     orphan       0.83      0.83      0.83         6\n",
      "  parataxis       0.53      0.14      0.22       191\n",
      "      punct       1.00      1.00      1.00      2677\n",
      "       root       1.00      1.00      1.00       783\n",
      "   vocative       0.00      0.00      0.00         5\n",
      "      xcomp       0.54      0.76      0.63       130\n",
      "\n",
      "avg / total       0.83      0.84      0.82     14939\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/igor/Bin/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "lab_dict = {v:k for (k,v) in lp.label_dict_s.items()}\n",
    "pred_indices = np.argmax(predicted_simple_pos, axis=1)\n",
    "pred_labels = [lab_dict[i] for i in pred_indices]\n",
    "true_labels = [lp.strip_colon(w['deprel']) for tree in test_data for w in tree]\n",
    "print(classification_report(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перша \"складна\" модель також використовує LSTM, але без векторних представлень POS-тегів."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 138889 samples, validate on 27802 samples\n",
      "Epoch 1/5\n",
      "138889/138889 [==============================] - 59s 422us/step - loss: 1.2174 - acc: 0.6456 - val_loss: 1.4094 - val_acc: 0.6081\n",
      "Epoch 2/5\n",
      "138889/138889 [==============================] - 56s 400us/step - loss: 0.8785 - acc: 0.7301 - val_loss: 1.3126 - val_acc: 0.6288\n",
      "Epoch 3/5\n",
      "138889/138889 [==============================] - 54s 389us/step - loss: 0.8092 - acc: 0.7493 - val_loss: 1.2800 - val_acc: 0.6391\n",
      "Epoch 4/5\n",
      "138889/138889 [==============================] - 50s 363us/step - loss: 0.7655 - acc: 0.7614 - val_loss: 1.2808 - val_acc: 0.6388\n",
      "Epoch 5/5\n",
      "138889/138889 [==============================] - 51s 366us/step - loss: 0.7345 - acc: 0.7684 - val_loss: 1.2675 - val_acc: 0.6396\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.models.Sequential at 0x7fc98dc33f28>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lp.NN(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нарешті, остання модель використовує вектори POS-тегів і це дозволяє досягнути 79% точності на тестовій вибірці."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 138889 samples, validate on 27802 samples\n",
      "Epoch 1/5\n",
      "138889/138889 [==============================] - 77s 555us/step - loss: 0.8632 - acc: 0.7263 - val_loss: 0.7250 - val_acc: 0.7379\n",
      "Epoch 2/5\n",
      "138889/138889 [==============================] - 72s 519us/step - loss: 0.6192 - acc: 0.7928 - val_loss: 0.6515 - val_acc: 0.7796\n",
      "Epoch 3/5\n",
      "138889/138889 [==============================] - 70s 505us/step - loss: 0.5730 - acc: 0.8081 - val_loss: 0.6221 - val_acc: 0.7872\n",
      "Epoch 4/5\n",
      "138889/138889 [==============================] - 85s 611us/step - loss: 0.5436 - acc: 0.8176 - val_loss: 0.6161 - val_acc: 0.7868\n",
      "Epoch 5/5\n",
      "138889/138889 [==============================] - 79s 566us/step - loss: 0.5189 - acc: 0.8249 - val_loss: 0.6375 - val_acc: 0.7812\n"
     ]
    }
   ],
   "source": [
    "model_pos = lp.NN_pos(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У цьому випадку є сенс окремо порахувати labeled attachment score, тому що цей парсер неідеальний навіть для золотих дерев, тому накопичення помилок парсера та помилок класифікатора може дати гірший результат:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 14939\n",
      "Correctly defined: 9618\n",
      "LAS: 0.64\n"
     ]
    }
   ],
   "source": [
    "def predict_tree(sentence, model, parser):\n",
    "    stack, queue, relations = [parser.ROOT], sentence[:], []\n",
    "    lab_dict = {v:k for (k,v) in parser.label_dict.items()}\n",
    "    while queue and stack:\n",
    "        word_i = np.array([parser.get_indices(stack, queue)])\n",
    "        pos_i = np.array([parser.get_pos(stack, queue)])\n",
    "        action_i = model.predict([word_i, pos_i])\n",
    "        pred_index = np.argmax(action_i, axis=1)[0]\n",
    "        action_dep = lab_dict[pred_index]\n",
    "        action = action_dep.split('_')[0]\n",
    "        if len(action_dep.split('_')) > 1:\n",
    "            deprel = action_dep.split('_')[1]\n",
    "        if action == 'SHIFT':\n",
    "            stack.append(queue.pop(0))\n",
    "        elif action == 'REDUCE':\n",
    "            stack.pop()\n",
    "        elif action == 'LEFT':\n",
    "            relations.append((stack[-1][\"id\"], deprel, queue[0][\"id\"]))\n",
    "            stack.pop()\n",
    "        elif action == 'RIGHT':\n",
    "            relations.append((queue[0][\"id\"], deprel, stack[-1][\"id\"]))\n",
    "            stack.append(queue.pop(0))\n",
    "        else:\n",
    "            print(\"Unknown action.\")\n",
    "    return sorted(relations)\n",
    "\n",
    "total, tp = 0, 0\n",
    "for tree in test_data:\n",
    "    golden = [(node[\"id\"], lp.strip_colon(node[\"deprel\"]), node[\"head\"]) for node in tree]\n",
    "    predicted = predict_tree(tree, model_pos, lp)\n",
    "    total += len(tree)\n",
    "    tp += len(set(golden).intersection(set(predicted)))\n",
    "\n",
    "print(\"Total:\", total)\n",
    "print(\"Correctly defined:\", tp)\n",
    "print(\"LAS:\", round(tp/total, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
