{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Логічне слідування"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/igor/Bin/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/mnt/hdd/Data/NLP/snli_1.0/'\n",
    "snli_train = pd.read_csv(PATH+'snli_1.0_train.txt', sep='\\t')\n",
    "snli_dev = pd.read_csv(PATH+'snli_1.0_dev.txt', sep='\\t')\n",
    "snli_test = pd.read_csv(PATH+'snli_1.0_test.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Швидкий огляд даних."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gold_label</th>\n",
       "      <th>sentence1_binary_parse</th>\n",
       "      <th>sentence2_binary_parse</th>\n",
       "      <th>sentence1_parse</th>\n",
       "      <th>sentence2_parse</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>captionID</th>\n",
       "      <th>pairID</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "      <th>label3</th>\n",
       "      <th>label4</th>\n",
       "      <th>label5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
       "      <td>( ( A person ) ( ( is ( ( training ( his horse...</td>\n",
       "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
       "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>A person is training his horse for a competition.</td>\n",
       "      <td>3416050480.jpg#4</td>\n",
       "      <td>3416050480.jpg#4r1n</td>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>contradiction</td>\n",
       "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
       "      <td>( ( A person ) ( ( ( ( is ( at ( a diner ) ) )...</td>\n",
       "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
       "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>A person is at a diner, ordering an omelette.</td>\n",
       "      <td>3416050480.jpg#4</td>\n",
       "      <td>3416050480.jpg#4r1c</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      gold_label                             sentence1_binary_parse  \\\n",
       "0        neutral  ( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...   \n",
       "1  contradiction  ( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...   \n",
       "\n",
       "                              sentence2_binary_parse  \\\n",
       "0  ( ( A person ) ( ( is ( ( training ( his horse...   \n",
       "1  ( ( A person ) ( ( ( ( is ( at ( a diner ) ) )...   \n",
       "\n",
       "                                     sentence1_parse  \\\n",
       "0  (ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...   \n",
       "1  (ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...   \n",
       "\n",
       "                                     sentence2_parse  \\\n",
       "0  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...   \n",
       "1  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...   \n",
       "\n",
       "                                           sentence1  \\\n",
       "0  A person on a horse jumps over a broken down a...   \n",
       "1  A person on a horse jumps over a broken down a...   \n",
       "\n",
       "                                           sentence2         captionID  \\\n",
       "0  A person is training his horse for a competition.  3416050480.jpg#4   \n",
       "1      A person is at a diner, ordering an omelette.  3416050480.jpg#4   \n",
       "\n",
       "                pairID         label1 label2 label3 label4 label5  \n",
       "0  3416050480.jpg#4r1n        neutral    NaN    NaN    NaN    NaN  \n",
       "1  3416050480.jpg#4r1c  contradiction    NaN    NaN    NaN    NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snli_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "entailment       183416\n",
       "contradiction    183187\n",
       "neutral          182764\n",
       "-                   785\n",
       "Name: gold_label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snli_train['gold_label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чому деякі речення марковані через \"-\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A fat white boy in a tank top swings at a checkered black and white punching bag at a carnival while people look on and walk by in the background.\n",
      "The boy is playing a game.\n"
     ]
    }
   ],
   "source": [
    "print(snli_train[snli_train['gold_label']=='-'].iloc[145]['sentence1'])\n",
    "print(snli_train[snli_train['gold_label']=='-'].iloc[145]['sentence2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зрозуміло не стало. Краще їх просто ігнорувати."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_train = snli_train[snli_train['gold_label'] != '-'].dropna(subset=['sentence2'])\n",
    "snli_dev = snli_dev[snli_dev['gold_label'] != '-']\n",
    "snli_test = snli_test[snli_test['gold_label'] != '-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>&lt;lambda&gt;</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gold_label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>contradiction</th>\n",
       "      <td>36.832230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entailment</th>\n",
       "      <td>33.510801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>42.095704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                <lambda>\n",
       "gold_label              \n",
       "contradiction  36.832230\n",
       "entailment     33.510801\n",
       "neutral        42.095704"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snli_train.dropna(subset=['sentence2']).groupby('gold_label')['sentence2'].agg([lambda x: x.apply(len).mean()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Бейзлайн\" із вибором класу залежно від довжини гіпотези."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.36      0.23      0.28      3237\n",
      "   entailment       0.42      0.55      0.48      3368\n",
      "      neutral       0.46      0.48      0.47      3219\n",
      "\n",
      "  avg / total       0.42      0.42      0.41      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def len_baseline(sentence):\n",
    "    if len(sentence) < 33:\n",
    "        return 'entailment'\n",
    "    elif len(sentence) < 41:\n",
    "        return 'contradiction'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "    \n",
    "print(classification_report(snli_test['gold_label'], snli_test['sentence2'].apply(len_baseline)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Препроцесинг: negation scope (додати NOT_), стемінг, викинути службові слова (як рекомендують у https://pdfs.semanticscholar.org/2d7d/f0b5ac15cdaa50928031f5bb2fc63a0a1f68.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negation_scope(sent):\n",
    "    doc = nlp(sent, disable=['tagger', 'parser', 'ner'])\n",
    "    prepend = False\n",
    "    new_sent = ''\n",
    "    for token in doc:\n",
    "        if prepend and not token.is_punct:\n",
    "            new_sent += 'NOT_'+token.text_with_ws\n",
    "        elif token.is_punct:\n",
    "            prepend = False\n",
    "            new_sent += token.text_with_ws\n",
    "        elif token.lower in ('not', \"n't\", 'never', 'no', 'nobody', 'none', 'no'):\n",
    "            prepend = True\n",
    "            new_sent += token.text_with_ws\n",
    "        else:\n",
    "            new_sent += token.text_with_ws\n",
    "    return new_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemmas(sent):\n",
    "    doc = nlp(sent, disable=['parser', 'ner'])\n",
    "    lemmas = [(w.lemma_, w.pos_) for w in doc if (not w.pos_ in ('PART', 'CONJ', 'DET', 'ADP', 'INTJ') and not w.is_punct)]\n",
    "    return lemmas\n",
    "\n",
    "def preprocess_and_stem(sent):\n",
    "    new_sent = negation_scope(sent)\n",
    "    doc = nlp(new_sent, disable=['parser', 'ner'])\n",
    "    words = [w.text.lower() for w in doc if (not w.pos_ in ('PART', 'CONJ', 'DET', 'ADP', 'INTJ') and not w.is_punct)]\n",
    "    stems = [snowball.stem(w) for w in words]\n",
    "    return stems\n",
    "\n",
    "def dep_relations(sent):\n",
    "    doc = nlp(sent, disable=['ner'])\n",
    "    triples = [(t.dep_, t.lemma_, t.head.lemma_) for t in doc]\n",
    "    duples = [(t.lemma_, t.head.lemma_) for t in doc]\n",
    "    return triples, duples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_overlap(s, h):\n",
    "    \"\"\"\n",
    "    Here s and h are lemmatized and tokenized lists of (word, tag) tuples.\n",
    "    \"\"\"\n",
    "    pos_map = {'NOUN': wn.NOUN, 'VERB': wn.VERB,\n",
    "               'ADJ': wn.ADJ, 'ADV': wn.ADV}\n",
    "    cross_u = cross_unigrams_pos(s, h)\n",
    "    overlap = 0\n",
    "    for pair in cross_u:\n",
    "        synsets = wn.synsets(pair[0][0], pos = pos_map.get(pair[0][1], None))\n",
    "        if not synsets:\n",
    "            continue\n",
    "        synonyms = []\n",
    "        for sset in synsets:\n",
    "            synonyms.extend(sset.lemma_names())\n",
    "        if pair[1][0] in synonyms:\n",
    "            overlap += 1\n",
    "    return overlap, overlap/len(s)\n",
    "\n",
    "def cross_unigrams_pos(stokens, htokens):\n",
    "    res = []\n",
    "    for sw in stokens:\n",
    "        for hw in htokens:\n",
    "            if sw[1] == hw[1]:\n",
    "                res.append((sw, hw))\n",
    "    return list(set(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для фіч класифікатора ми візьмемо збіг коренів слів між двома реченнями, збіг смислів (функція semantic_overlap), збіг між залежностями, а також довжину різниці між реченнями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(s, h):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    slemmas = get_lemmas(s)\n",
    "    hlemmas = get_lemmas(h)\n",
    "    # \"semantic overlap\" using WordNet\n",
    "    sem_overlap_count, sem_overlap_perc = semantic_overlap(slemmas, hlemmas)\n",
    "    # usual overlap using Snowball stemming and accounting for negation\n",
    "    sstems = preprocess_and_stem(s)\n",
    "    hstems = preprocess_and_stem(h)\n",
    "    overlap_count = len(set(sstems).intersection(set(hstems)))\n",
    "    overlap_perc = overlap_count/len(set(sstems))\n",
    "    # length features\n",
    "    len_diff = len(s) - len(h)\n",
    "    len_token_diff = len(sstems) - len(hstems)\n",
    "    len_diff_perc = len_diff/len(s)\n",
    "    # dependency features\n",
    "    sdep3, sdep2 = dep_relations(s)\n",
    "    hdep3, hdep2 = dep_relations(h)\n",
    "    dep3overlap = set(sdep3).intersection(set(hdep3))\n",
    "    dep2overlap = set(sdep2).intersection(set(hdep2))\n",
    "    dep3count = len(dep3overlap)\n",
    "    dep2count = len(dep2overlap)\n",
    "    dep3perc = dep3count/len(sdep3)\n",
    "    dep2perc = dep2count/len(sdep2)\n",
    "    # initialize features dict\n",
    "    features = {\n",
    "        'sem_overlap_count': sem_overlap_count,\n",
    "        'sem_overlap_perc': sem_overlap_perc,\n",
    "        'overlap_count': overlap_count,\n",
    "        'overlap_perc': overlap_perc,\n",
    "        'len_diff': len_diff,\n",
    "        'len_token_diff': len_token_diff,\n",
    "        'len_diff_perc': len_diff_perc,\n",
    "        'dep3count': dep3count,\n",
    "        'dep2count': dep2count,\n",
    "        'dep3perc': dep3perc,\n",
    "        'dep2perc': dep2perc\n",
    "    }\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Натренуємо на невеликій вибірці із тренувального датасету."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dev row 9842"
     ]
    }
   ],
   "source": [
    "train_sample = snli_train.sample(30000, random_state=505)\n",
    "#dev_sample = snli_dev.sample(2000, random_state=505)\n",
    "\n",
    "train_features, train_labels = [], []\n",
    "j = 1\n",
    "for i, row in train_sample.iterrows():\n",
    "    sys.stdout.write('\\rProcessing train row {}'.format(j))\n",
    "    j += 1\n",
    "    train_features.append(get_features(row['sentence1'], row['sentence2']))\n",
    "    train_labels.append(row['gold_label'])    \n",
    "\n",
    "dev_features, dev_labels = [], []\n",
    "j = 1\n",
    "for i, row in snli_dev.iterrows():\n",
    "    sys.stdout.write('\\rProcessing dev row {}'.format(j))\n",
    "    j += 1\n",
    "    dev_features.append(get_features(row['sentence1'], row['sentence2']))\n",
    "    dev_labels.append(row['gold_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = DictVectorizer()\n",
    "train_vec = vec.fit_transform(train_features)\n",
    "dev_vec = vec.transform(dev_features)\n",
    "clf = LogisticRegression(penalty='l1', random_state=505)\n",
    "clf.fit(train_vec, train_labels)\n",
    "pred_labels = clf.predict(dev_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.48      0.52      0.50      3278\n",
      "   entailment       0.53      0.70      0.61      3329\n",
      "      neutral       0.53      0.32      0.40      3235\n",
      "\n",
      "  avg / total       0.51      0.51      0.50      9842\n",
      "\n",
      "\n",
      "Accuracy is 0.514\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(dev_labels, pred_labels))\n",
    "print('\\nAccuracy is', round(accuracy_score(dev_labels, pred_labels), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для покращеного класифікатора використаємо ознаки з https://nlp.stanford.edu/pubs/snli_paper.pdf, плюс семантичний збіг (за Ворднетом) та збіг за коренями слів із попереднього класифікатора. Збіг за залежностями не використано - він дуже уповільнює тренування, але дає невелике покращення (принаймні в теперішньому вигляді). Зокрема, серед фіч: метрика BLEU для виявлення подібності речень; різниця в довжині; збіг між реченнями, зокрема окремо збіг між словами зі спільними частинами мови; уніграми та біграми слів; \"крос-уніграми\" для слів зі спільною частиною мови, та \"крос-біграми\" для пар слів зі спільною частиною мови у другого слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_by_pos(tokens, pos):\n",
    "    \"\"\"\n",
    "    Take only words of a specified POS.\n",
    "    \"\"\"\n",
    "    if pos == 'NOUN':\n",
    "        return [t[0] for t in tokens if t[1] in ('PROPN', 'NOUN')]\n",
    "    else:\n",
    "        return [t[0] for t in tokens if t[1] == pos]\n",
    "    \n",
    "def cross_unigrams(stokens, htokens):\n",
    "    res = []\n",
    "    for sw in stokens:\n",
    "        for hw in htokens:\n",
    "            if sw[1] == hw[1]:\n",
    "                res.append((sw[0], hw[0]))\n",
    "    return list(set(res))\n",
    "\n",
    "def cross_bigrams(stokens, htokens):\n",
    "    res = []\n",
    "    sbigrams = nltk.bigrams(stokens)\n",
    "    hbigrams = nltk.bigrams(htokens)\n",
    "    for sb in sbigrams:\n",
    "        for hb in hbigrams:\n",
    "            if sb[1][1] == hb[1][1]:\n",
    "                res.append(([s[0] for s in sb], [h[0] for h in hb]))\n",
    "    return res\n",
    "\n",
    "def extract_features(s, h):\n",
    "    \"\"\"\n",
    "    s, h are sentence1 (premise) and hypothesis (sentence2).\n",
    "    Right now using word lemmas, not stems.\n",
    "    \"\"\"\n",
    "    sdoc = nlp(s, disable=['parser', 'ner'])\n",
    "    hdoc = nlp(h, disable=['parser', 'ner'])\n",
    "    s_tokens = [(t.lemma_, t.pos_) for t in sdoc if not t.is_punct]\n",
    "    h_tokens = [(t.lemma_, t.pos_) for t in hdoc if not t.is_punct]\n",
    "    swords = [t[0] for t in s_tokens]\n",
    "    hwords = [t[0] for t in h_tokens]\n",
    "    # initialize a dictionary\n",
    "    features = {}\n",
    "    # using WordNet\n",
    "    sem_overlap_count, sem_overlap_perc = semantic_overlap(s_tokens, h_tokens)\n",
    "    # BLEU scores for 1, 2, 3, 4-grams and then cumulative BLEU for all n\n",
    "    bleu1 = sentence_bleu([swords], hwords, weights=(1, 0, 0, 0))\n",
    "    bleu2 = sentence_bleu([swords], hwords, weights=(0, 1, 0, 0))\n",
    "    bleu3 = sentence_bleu([swords], hwords, weights=(0, 0, 1, 0))\n",
    "    bleu4 = sentence_bleu([swords], hwords, weights=(0, 0, 0, 1))\n",
    "    bleu_cum = sentence_bleu([swords], hwords, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "    features.update({\n",
    "        'bleu1': bleu1,\n",
    "        'bleu2': bleu2,\n",
    "        'bleu3': bleu3,\n",
    "        'bleu4': bleu4,\n",
    "        'sem_overlap': sem_overlap_count,\n",
    "        'sem_overlap_perc': sem_overlap_perc,\n",
    "        'bleu_cum': bleu_cum\n",
    "    })\n",
    "    # length difference\n",
    "    lendiff = len(swords) - len(hwords)\n",
    "    features.update({'lendiff': lendiff})\n",
    "    # overlaps\n",
    "    sstems = preprocess_and_stem(s)\n",
    "    hstems = preprocess_and_stem(h)\n",
    "    stem_overlap_count = len(set(sstems).intersection(set(hstems)))\n",
    "    stem_overlap_perc = stem_overlap_count/len(set(sstems))\n",
    "    overlap = set(swords).intersection(set(hwords))\n",
    "    overlap_count = len(overlap)\n",
    "    overlap_perc = overlap_count/len(set(swords))\n",
    "    snouns = set(choose_by_pos(s_tokens, 'NOUN'))\n",
    "    sverbs = set(choose_by_pos(s_tokens, 'VERB'))\n",
    "    sadjs = set(choose_by_pos(s_tokens, 'ADJ'))\n",
    "    sadvs = set(choose_by_pos(s_tokens, 'ADV'))\n",
    "    noun_overlap = snouns.intersection(set(choose_by_pos(h_tokens, 'NOUN')))\n",
    "    verb_overlap = sverbs.intersection(set(choose_by_pos(h_tokens, 'VERB')))\n",
    "    adj_overlap = sadjs.intersection(set(choose_by_pos(h_tokens, 'ADJ')))\n",
    "    adv_overlap = sadvs.intersection(set(choose_by_pos(h_tokens, 'ADV')))\n",
    "    noun_count = len(noun_overlap)\n",
    "    verb_count = len(verb_overlap)\n",
    "    adj_count = len(adj_overlap)\n",
    "    adv_count = len(adv_overlap)\n",
    "    noun_perc = noun_count/len(snouns) if len(snouns) > 0 else 0\n",
    "    verb_perc = verb_count/len(sverbs) if len(sverbs) > 0 else 0\n",
    "    adj_perc = adj_count/len(sadjs) if len(sadjs) > 0 else 0\n",
    "    adv_perc = adv_count/len(sadvs) if len(sadvs) > 0 else 0\n",
    "    features.update({\n",
    "        'overlap_count': overlap_count,\n",
    "        'overlap_perc': overlap_perc,\n",
    "        'stem_overlap_count': stem_overlap_count,\n",
    "        'stem_overlap_perc': stem_overlap_perc,\n",
    "        'noun_count': noun_count,\n",
    "        'verb_count': verb_count,\n",
    "        'adj_count': adj_count,\n",
    "        'adv_count': adv_count,\n",
    "        'noun_perc': noun_perc,\n",
    "        'verb_perc': verb_perc,\n",
    "        'adj_perc': adj_perc,\n",
    "        'adv_perc': adv_perc\n",
    "    })\n",
    "    # unigrams and bigrams for hypothesis\n",
    "    hbigrams = nltk.bigrams(hwords)\n",
    "    for w in hwords:\n",
    "        features['word={}'.format(w)] = 1\n",
    "    for b in hbigrams:\n",
    "        features['bigram={w1}_{w2}'.format(w1=b[0], w2=b[1])] = 1\n",
    "    # cross-unigrams\n",
    "    cross_u = cross_unigrams(s_tokens, h_tokens)\n",
    "    for u in cross_u:\n",
    "        features['cu={w1}_{w2}'.format(w1=u[0], w2=u[1])] = 1\n",
    "    # cross-bigrams\n",
    "    cross_b = cross_bigrams(s_tokens, h_tokens)\n",
    "    for b in cross_b:\n",
    "        w1, w2 = b[0]\n",
    "        w3, w4 = b[1]\n",
    "        features['cb={w1}_{w2}_{w3}_{w4}'.format(w1=w1, w2=w2, w3=w3, w4=w4)] = 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Натренуємо, так само, на вибірці з 30000 рядків."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train row 15"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/igor/Bin/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/home/igor/Bin/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/home/igor/Bin/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dev row 9842"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = [], []\n",
    "j = 1\n",
    "for i, row in train_sample.iterrows():\n",
    "    sys.stdout.write('\\rProcessing train row {}'.format(j))\n",
    "    j += 1\n",
    "    train_features.append(extract_features(row['sentence1'], row['sentence2']))\n",
    "    train_labels.append(row['gold_label'])    \n",
    "\n",
    "dev_features, dev_labels = [], []\n",
    "j = 1\n",
    "for i, row in snli_dev.iterrows():\n",
    "    sys.stdout.write('\\rProcessing dev row {}'.format(j))\n",
    "    j += 1\n",
    "    dev_features.append(extract_features(row['sentence1'], row['sentence2']))\n",
    "    dev_labels.append(row['gold_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = DictVectorizer()\n",
    "train_vec = vec.fit_transform(train_features)\n",
    "dev_vec = vec.transform(dev_features)\n",
    "clf = LogisticRegression(penalty='l1', random_state=505)\n",
    "clf.fit(train_vec, train_labels)\n",
    "pred_labels = clf.predict(dev_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.74      0.71      0.73      3278\n",
      "   entailment       0.70      0.77      0.74      3329\n",
      "      neutral       0.67      0.62      0.64      3235\n",
      "\n",
      "  avg / total       0.70      0.70      0.70      9842\n",
      "\n",
      "\n",
      "Accuracy is  0.702\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(dev_labels, pred_labels))\n",
    "print('\\nAccuracy is ', round(accuracy_score(dev_labels, pred_labels), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класифікатор не дуже добре працює з нейтральними гіпотезами, але загалом looks good. Можна спробувати на повній вибірці і подивитись, наскільки покращиться якість завдяки більшій кількості тренувальних даних."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 19"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/igor/Bin/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/home/igor/Bin/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/home/igor/Bin/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 550151"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = [], []\n",
    "for i, row in snli_train.iterrows():\n",
    "    sys.stdout.write(\"\\rProcessing row %i\" % i)\n",
    "    train_features.append(extract_features(row['sentence1'], row['sentence2']))\n",
    "    train_labels.append(row['gold_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=505, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = DictVectorizer()\n",
    "train_vectorized = vec.fit_transform(train_features)\n",
    "clf = LogisticRegression(penalty='l1', random_state=505)\n",
    "clf.fit(train_vectorized, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/igor/Bin/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/home/igor/Bin/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/home/igor/Bin/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "dev_features, dev_labels = [], []\n",
    "for i, row in snli_dev.iterrows():\n",
    "    dev_features.append(extract_features(row['sentence1'], row['sentence2']))\n",
    "    dev_labels.append(row['gold_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_vectorized = vec.transform(dev_features)\n",
    "dev_pred_labels = clf.predict(dev_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction      0.804     0.806     0.805      3278\n",
      "   entailment      0.797     0.848     0.822      3329\n",
      "      neutral      0.761     0.709     0.734      3235\n",
      "\n",
      "  avg / total      0.787     0.788     0.787      9842\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(dev_labels, dev_pred_labels, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.788\n"
     ]
    }
   ],
   "source": [
    "print(round(accuracy_score(dev_labels, dev_pred_labels),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тепер на власне тестовій вибірці."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/igor/Bin/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/home/igor/Bin/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/home/igor/Bin/anaconda3/lib/python3.6/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction      0.811     0.806     0.808      3237\n",
      "   entailment      0.792     0.847     0.818      3368\n",
      "      neutral      0.766     0.715     0.739      3219\n",
      "\n",
      "  avg / total      0.790     0.790     0.789      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_features, test_labels = [], []\n",
    "for i, row in snli_test.iterrows():\n",
    "    test_features.append(extract_features(row['sentence1'], row['sentence2']))\n",
    "    test_labels.append(row['gold_label'])\n",
    "\n",
    "test_vectorized = vec.transform(test_features)\n",
    "test_pred_labels = clf.predict(test_vectorized)\n",
    "print(classification_report(test_labels, test_pred_labels, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.79\n"
     ]
    }
   ],
   "source": [
    "print(round(accuracy_score(test_labels, test_pred_labels),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У авторів оригінальної статті було 78.2, у мене вийшло 79 - тобто вдалось повторити і навіть трохи покращити результат, використовуючи тільки методи класичного машинного навчання."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
