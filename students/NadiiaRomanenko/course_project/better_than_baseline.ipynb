{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature ideas\n",
    "\n",
    "+ TF-IDF similiarity\n",
    "+ doc2vec (tf-idf weighted w2v) similiarity\n",
    "+ LDA similiarity\n",
    "\n",
    "    - (further improvement?)~ entities intersection (Polyglot / NER by Chaplinskyi)\n",
    "    - alignment in headlines + word vectors between correspondent words \n",
    "    - mean and std for similiarities between all pairs of words in 2 headlines\n",
    "\n",
    "\n",
    "http://jens-lehmann.org/files/2017/kcap_simdoc.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import re, glob, pandas as pd, numpy as np\n",
    "\n",
    "from cp_utils.vectorizers import *\n",
    "\n",
    "logging.getLogger('polyglot').setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-03 23:42:11,596 : INFO : loading TfidfModel object from ./vectors/tfidf_lemm_nofunctors_unigramms.gensim\n",
      "2018-05-03 23:42:13,563 : INFO : loading id2word recursively from ./vectors/tfidf_lemm_nofunctors_unigramms.gensim.id2word.* with mmap=None\n",
      "2018-05-03 23:42:13,564 : INFO : loaded ./vectors/tfidf_lemm_nofunctors_unigramms.gensim\n",
      "2018-05-03 23:42:13,565 : INFO : loading Dictionary object from ./vectors/pos_lemmatized_nofunctors.dict\n",
      "2018-05-03 23:42:13,765 : INFO : loaded ./vectors/pos_lemmatized_nofunctors.dict\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfIdfGensimVectoriser(tfidf_path='./vectors/tfidf_lemm_nofunctors_unigramms.gensim',\n",
    "                              dictionary='./vectors/pos_lemmatized_nofunctors.dict',\n",
    "                              lemmatize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_vect = LdaVectorizer(lda_path='./vectors/lda050418_1000dim_15pass_100iter_10offset_0.7lr.lda',\n",
    "                         tfidf_path='./vectors/tfidf_lemm_nofunctors_unigramms.gensim',\n",
    "                         dictionary='./vectors/pos_lemmatized_nofunctors.dict',\n",
    "                         lemmatize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-03 23:42:15,927 : INFO : loading Word2VecKeyedVectors object from ./vectors/w2v_sent_5dim_5win_5Mwaxvocab_15Kbatch_10epoch.w2v\n",
      "2018-05-03 23:42:18,753 : INFO : loading wv recursively from ./vectors/w2v_sent_5dim_5win_5Mwaxvocab_15Kbatch_10epoch.w2v.wv.* with mmap=None\n",
      "2018-05-03 23:42:18,754 : INFO : loading vectors from ./vectors/w2v_sent_5dim_5win_5Mwaxvocab_15Kbatch_10epoch.w2v.wv.vectors.npy with mmap=None\n",
      "2018-05-03 23:42:20,486 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-05-03 23:42:20,488 : INFO : loading vocabulary recursively from ./vectors/w2v_sent_5dim_5win_5Mwaxvocab_15Kbatch_10epoch.w2v.vocabulary.* with mmap=None\n",
      "2018-05-03 23:42:20,490 : INFO : loading trainables recursively from ./vectors/w2v_sent_5dim_5win_5Mwaxvocab_15Kbatch_10epoch.w2v.trainables.* with mmap=None\n",
      "2018-05-03 23:42:20,491 : INFO : loading syn1neg from ./vectors/w2v_sent_5dim_5win_5Mwaxvocab_15Kbatch_10epoch.w2v.trainables.syn1neg.npy with mmap=None\n",
      "2018-05-03 23:42:22,195 : INFO : setting ignored attribute cum_table to None\n",
      "2018-05-03 23:42:22,196 : INFO : loaded ./vectors/w2v_sent_5dim_5win_5Mwaxvocab_15Kbatch_10epoch.w2v\n",
      "2018-05-03 23:42:22,196 : INFO : loading TfidfModel object from ./vectors/tfidf_lemm_nofunctors_unigramms.gensim\n",
      "2018-05-03 23:42:22,820 : INFO : loading id2word recursively from ./vectors/tfidf_lemm_nofunctors_unigramms.gensim.id2word.* with mmap=None\n",
      "2018-05-03 23:42:22,821 : INFO : loaded ./vectors/tfidf_lemm_nofunctors_unigramms.gensim\n",
      "2018-05-03 23:42:22,822 : INFO : loading Dictionary object from ./vectors/pos_lemmatized_nofunctors.dict\n",
      "2018-05-03 23:42:22,996 : INFO : loaded ./vectors/pos_lemmatized_nofunctors.dict\n"
     ]
    }
   ],
   "source": [
    "d2v_tfidf = TfIdfD2vVectoriser(vec_path='./vectors/w2v_sent_5dim_5win_5Mwaxvocab_15Kbatch_10epoch.w2v',\n",
    "                               tfidf_path='./vectors/tfidf_lemm_nofunctors_unigramms.gensim',\n",
    "                               dictionary='./vectors/pos_lemmatized_nofunctors.dict',\n",
    "                               lemmatize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize texts and map them to ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_generator():\n",
    "    with ZipFile('aggr_texts.zip') as zf:\n",
    "        names = zf.namelist()\n",
    "        for fn in names:\n",
    "            with zf.open(fn) as f:\n",
    "                yield fn, f.read().decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for fname, doc in text_generator():\n",
    "    d = {}\n",
    "    d['id'] = re.sub('\\D', '', fname)\n",
    "    d['text'] = re.split('\\s+', doc)\n",
    "    texts += [d]\n",
    "    \n",
    "texts_df = pd.DataFrame(texts)\n",
    "id2index = {int(row['id']): i for i, row in texts_df.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nr/prjctr/nlp_env/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/nr/prjctr/nlp_env/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorized D2V\n",
      "CPU times: user 52.1 s, sys: 20 ms, total: 52.1 s\n",
      "Wall time: 52.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lda_feats = lda_vect.transform(texts_df.text.values)\n",
    "print('vectorized LDA')\n",
    "\n",
    "d2v_feats = d2v_tfidf.transform(texts_df.text.values)\n",
    "print('vectorized D2V')\n",
    "\n",
    "tfidf_feats = tfidf.transform(texts_df.text.values)\n",
    "print('vectorized Tf-Idf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as pkl\n",
    "\n",
    "with open('d2v_feats.pkl', 'wb') as f:\n",
    "    pkl.dump(d2v_feats, f)\n",
    "    \n",
    "with open('tfidf_feats.pkl', 'wb') as f:\n",
    "    pkl.dump(tfidf_feats, f)\n",
    "\n",
    "with open('lda_feats.pkl', 'wb') as f:\n",
    "    pkl.dump(lda_feats, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct features for pairs of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs = pd.read_csv('train_pair_ids.tsv', sep='\\t')\n",
    "test_pairs = pd.read_csv('test_pair_ids.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_agg_vector(row):\n",
    "    lda_row_1 = lda_feats[id2index[row.id1]]\n",
    "    lda_row_2 = lda_feats[id2index[row.id2]]\n",
    "    \n",
    "    d2v_row_1 = d2v_feats[id2index[row.id1]]\n",
    "    d2v_row_2 = d2v_feats[id2index[row.id2]]\n",
    "    \n",
    "    tfidf_row_1 = tfidf_feats[id2index[row.id1]]\n",
    "    tfidf_row_2 = tfidf_feats[id2index[row.id2]]\n",
    "    \n",
    "    d2v_cosine = cosine_similarity(d2v_row_1, d2v_row_2)[0, 0]\n",
    "    lda_diff = np.abs(lda_row_1 - lda_row_2)\n",
    "    tfidf_cosine = cosine_similarity(tfidf_row_1, tfidf_row_2)[0, 0]\n",
    "    \n",
    "    feature_vec = sparse.hstack([d2v_row_1,\n",
    "                                 d2v_row_2,\n",
    "                                 d2v_cosine,\n",
    "                                 lda_diff,\n",
    "                                 tfidf_cosine])\n",
    "    return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got train features\n",
      "Got test features\n"
     ]
    }
   ],
   "source": [
    "train_balanced = pd.concat([train_pairs.loc[train_pairs.is_similar, ],\n",
    "                            train_pairs.loc[~train_pairs.is_similar, \n",
    "                                          ].sample(n=16276)])\n",
    "\n",
    "train_feats = sparse.vstack([get_feature_agg_vector(row) for i, row in train_balanced.iterrows()])\n",
    "print('Got train features')\n",
    "\n",
    "test_feats = sparse.vstack([get_feature_agg_vector(row) for i, row in test_pairs.iterrows()])\n",
    "print('Got test features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<70275x1102 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6956648 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 30s, sys: 14min 31s, total: 26min 1s\n",
      "Wall time: 6min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cls = MLPClassifier(hidden_layer_sizes=(100, 100,))\n",
    "cls.fit(train_feats, train_balanced.is_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = cls.predict(test_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.76      0.90      0.82     45449\n",
      "       True       0.72      0.47      0.57     24826\n",
      "\n",
      "avg / total       0.74      0.75      0.73     70275\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[11660, 13166],\n",
       "       [ 4645, 40804]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(classification_report(predicted, test_pairs.is_similar))\n",
    "\n",
    "confusion_matrix(predicted, test_pairs.is_similar,\n",
    "                 labels=[True, False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Baseline 0.56 f1 score for similar pairs - 0.73\n",
    "* Adding weighted aweraged word2vec, lda, and tf-idf features resulted in better f1 for unsimilar documents and much fewer false-positives for similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
